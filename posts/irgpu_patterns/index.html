<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="IRGPU: Patterns for massively parallel programming" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Patterns for massively parallel programming" /><meta property="og:description" content="Patterns for massively parallel programming" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/irgpu_patterns/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/irgpu_patterns/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-05-05T14:00:00+02:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="IRGPU: Patterns for massively parallel programming" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-03T16:45:54+02:00","datePublished":"2021-05-05T14:00:00+02:00","description":"Patterns for massively parallel programming","headline":"IRGPU: Patterns for massively parallel programming","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/irgpu_patterns/"},"url":"https://lemasyma.github.io/cours/posts/irgpu_patterns/"}</script><title>IRGPU: Patterns for massively parallel programming | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>IRGPU: Patterns for massively parallel programming</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>IRGPU: Patterns for massively parallel programming</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-05-05 14:00:00 +0200" data-toggle="tooltip" data-placement="bottom" title="Wed, May 5, 2021, 2:00 PM +0200" >May 5, 2021</em> </span> <span> Updated <em class="timeago" date="2021-10-03 16:45:54 +0200 " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 3, 2021, 4:45 PM +0200" >Oct 3, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2042 words"> <em>11 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Lien de la <a href="https://hackmd.io/@lemasymasa/rkmdFZed_">note Hackmd</a></p><h1 id="irgpu-patterns-for-massively-parallel-programming">IRGPU: Patterns for massively parallel programming</h1><h1 id="programming-patterns--memory-optimizations">Programming patterns &amp; memory optimizations</h1><p><strong>The programming patterns</strong></p><ul><li>Map<li>Map + Local reduction<li>Reduction<li>Scan</ul><p><strong>The IP algorithms</strong></p><ul><li>LUT applications<li>Local features extraction<li>Histogram<li>Integral images</ul><h1 id="map-pattern">Map pattern</h1><h2 id="map-pattern-overview">Map pattern overview<a href="#map-pattern-overview"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Un pixel en entree et sortie<li>La dependance est nulle</ul><div class="alert alert-info" role="alert"><p>Map replicated a function over every element of an index set. The computation of each pixel is independant wrt the others</p></div><p><code class="language-plaintext highlighter-rouge">out(x,y) = f(in(x,y))</code></p><p><img data-proofer-ignore data-src="https://i.imgur.com/fbjNWGZ.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/Mk2oJIk.png" alt="" /></p><p><em>What do you think about <code class="language-plaintext highlighter-rouge">k</code>’s impact of the performance ?</em> Le premier fait <code class="language-plaintext highlighter-rouge">threadIdx.x + k</code> et l’autre fait <code class="language-plaintext highlighter-rouge">threadIdx.x * k</code></p><ul><li>A gauche, le thread numero i est le thread numero i+1: <strong>c’est continue au niveau des addresses memoire</strong> (lineaire)<li>A droite: acces stride (palier)</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/AsmEcfK.png" alt="" /></p><div class="alert alert-success" role="alert"><ul><li>Linear sequential access with offset c’est bien<li>Strided access c’est po bien</ul></div><h3 id="strided-access-pattern">Strided access pattern<a href="#strided-access-pattern"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/UIPbkpm.png" alt="" /></p><p>Sur le kernel numero 2, un grand temps du process est passe pour la gestion memoire (acces memoire mal fait)</p><h1 id="memory-performance">Memory performance</h1><h2 id="memory-bandwith">Memory bandwith<a href="#memory-bandwith"><i class="fas fa-hashtag"></i></a></h2></h2><p>What you think about memory</p><p><img data-proofer-ignore data-src="https://i.imgur.com/B9ci8TU.png" alt="" /></p><p>Reality:</p><p><img data-proofer-ignore data-src="https://i.imgur.com/tv7pZTj.png" alt="" /></p><h2 id="memory-access-hierarchy">Memory access hierarchy<a href="#memory-access-hierarchy"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/26hzlga.png" alt="" /></p><ul><li>Memoire L2: cache intermediaire<li>Memoire sur le chip low-latency<ul><li>Registres<li>Shared memory<li>L1 (meme zone que shared memory)<ul><li>Dire au processeur de gerer la memoire ou la gerer nous-meme<li>Dans ce cas on configure nous meme la shared memory<li>Sers de cache entre le L2 et la shared memory</ul></ul></ul><h2 id="cached-loads-from-l1-low-latency">Cached Loads from L1 low-latency<a href="#cached-loads-from-l1-low-latency"><i class="fas fa-hashtag"></i></a></h2></h2><p>2 choses a prendre en compte:</p><ol><li>acces memoire alignes ou non<li>acces memoire coalesced (stride) ou non</ol><p>2 types of global memory loads: <strong>cached</strong> or <strong>uncached</strong></p><h3 id="aligned-vs-misaligned">Aligned vs Misaligned<a href="#aligned-vs-misaligned"><i class="fas fa-hashtag"></i></a></h3></h3><p>A load is <strong>aligned</strong> if the first address of a memory access in 32 bytes</p><ul><li>Memory addresses must be type-aligned (<code class="language-plaintext highlighter-rouge">typeof(machin)</code>)<li>Otherwise poor perf<li><code class="language-plaintext highlighter-rouge">cudaMalloc</code> = alignement on 256 bits at least</ul><h3 id="coalesced-vs-uncoalesced">Coalesced vs uncoalesced<a href="#coalesced-vs-uncoalesced"><i class="fas fa-hashtag"></i></a></h3></h3><p>A <strong>load</strong> is coalesced if a warp access is non-continuous</p><h2 id="misaligned-cached-loads-from-l1">Misaligned cached loads from L1<a href="#misaligned-cached-loads-from-l1"><i class="fas fa-hashtag"></i></a></h2></h2><p>We need a load strategy:</p><ul><li>32 threads of warp access a 32-bit word = 128 bytes<li>128 bytes = L1 bus (single load - bus utilization = 100%)<li>Access permutation has no (or very low) overheard</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/iwWlZJZ.png" alt="" /></p><ul><li>If data are not 128-bits aligned, 2 loads are required</ul><p>Adresses 96-224 required.. but 0-256 loaded</p><p><img data-proofer-ignore data-src="https://i.imgur.com/Z4BOxMo.png" alt="" /></p><ul><li>If data is accessed strided</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/sBfwvO6.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/oeBdxY5.png" alt="" /></p><p><em>Pas possible d’augmenter la taille du bus ?</em> Le bus est fixe (hardware)</p><h2 id="loads-from-gloabl-uncached-memory">Loads from gloabl (uncached) memory<a href="#loads-from-gloabl-uncached-memory"><i class="fas fa-hashtag"></i></a></h2></h2><p>Same idea but memory is split in segments of 32 bytes</p><p><img data-proofer-ignore data-src="https://i.imgur.com/ri57qfT.png" alt="" /></p><h2 id="coalesced-memory-access-summary">Coalesced Memory Access (summary)<a href="#coalesced-memory-access-summary"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/HzDzK61.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/yofR2mV.png" alt="" /></p><h2 id="how-memory-works-for-real">How memory works for real<a href="#how-memory-works-for-real"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>DRAM is organised in 2D Core array<li>Each DRAM core array has about 16M bits</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/fsGELN3.png" alt="" /></p><h3 id="example">Example<a href="#example"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>A 4x4 memory cell<li>With 4 bits pin interface width</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/H9l2qyo.png" alt="" /></p><h3 id="dram-burst">DRAM Burst<a href="#dram-burst"><i class="fas fa-hashtag"></i></a></h3></h3><p>La memoire est lente</p><ul><li>DDR = 1/2 interface speed<li>DDR2 = 1/4 interface speed<li>DDR3 = 1/8 interface speed</ul><div class="alert alert-success" role="alert"><p>Solution: Bursting</p></div><p>Load N x interface width of <strong>the same row</strong></p><p><img data-proofer-ignore data-src="https://i.imgur.com/keRA3iG.png" alt="" /></p><p>Au lieu d’avoir 1/4, on renvoit 3 x 1/4</p><p>Better, but not enough to saturate the memory bus</p><h2 id="summary">Summary<a href="#summary"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Use <strong>coalesced</strong> (<em>contiguous</em> and <em>aligned</em>) accessed to memory</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/D5Nbfwi.png" alt="" /></p><p><em>How to make coalesced loads with 2D arrays ?</em> <img data-proofer-ignore data-src="https://i.imgur.com/zVRBvKI.png" alt="" /></p><div class="alert alert-danger" role="alert"><p>Ca correspond au <strong>pitch</strong></p></div><p>Pitch: taille des lignes pour que le debut des lignes correspond a un multiple de 32</p><h1 id="using-shared-memory">Using shared memory</h1><h2 id="transposition">Transposition<a href="#transposition"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/TEq18Ug.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/1FkMRoY.png" alt="" /></p><p><em>Where are non-coalesced access ?</em> Sur un warp, les <code class="language-plaintext highlighter-rouge">x</code> sont lineaire (0 - 31). Ici, ce qui est lineaire selon x c’est la lecture dans <code class="language-plaintext highlighter-rouge">in</code>. 32 threads vont ecrire 32 elements non-continus</p><div class="alert alert-success" role="alert"><p><code class="language-plaintext highlighter-rouge">a[x][y]</code></p></div><h2 id="tiling-and-memory-privatization">Tiling and memory privatization<a href="#tiling-and-memory-privatization"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>On decoupe le travaille en sous-block (tuile)</p><p><img data-proofer-ignore data-src="https://i.imgur.com/AaI69bz.png" alt="" /></p></div><blockquote><p>Ca marche bien sur les images !</p></blockquote><p>For each block:</p><ul><li>Read the tile from global to private block memory<li>process the block (used shared memory)<li>write the tile from the private (shared) block memory to global memory</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/MdMGfx4.png" alt="" /></p><h3 id="collaborative-loading-and-writing-then-blockdim--tiledim">Collaborative loading and writing then BLOCKDIM = TILEDIM<a href="#collaborative-loading-and-writing-then-blockdim--tiledim"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>All threads load one or more data<li>Access must be <strong>coalesced</strong><li>Use barrier synch to make sure that all threads are ready to start the phase</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/CV8GUbH.png" alt="" /></p><p><em>Est-ce que ecrire dans la tile c’est lineaire ?</em> Pour chaque ligne <code class="language-plaintext highlighter-rouge">y</code>, <code class="language-plaintext highlighter-rouge">x</code> va varier le plus rapidement possible: c’est lineaire</p><h3 id="tiled-transposition-in-shared-memory">Tiled transposition in shared memory<a href="#tiled-transposition-in-shared-memory"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/3kENRo7.png" alt="" /></p><p>L’algorithme pour transposer en utilisant la shared memory commence par copier la taille en shared memory, transpose en shared memory et transpose les acces coalesced en memoire globale</p><p><img data-proofer-ignore data-src="https://i.imgur.com/KscPzBJ.png" alt="" /></p><p><em>A quel moment on fait des acces aligned dans la shared memory ?</em> On lit de maniere alignee en global, on ecrit en aligne partout sauf la derniere ligne ou c’est un acces non-aligne</p><p><img data-proofer-ignore data-src="https://i.imgur.com/gqT5i0W.png" alt="" /></p><p>Performance (GB/s on TESLA K40)</p><p><img data-proofer-ignore data-src="https://i.imgur.com/bcKFDsm.png" alt="" /></p><p>Speed up de 2 entre la version qui utilise la shared memory et celle qui ne l’utilise pas</p><h1 id="about-shared-memory">About shared memory</h1><p><em>Comment on peut combler le gap (encore) entre les GB/s de la TESLA ?</em></p><h2 id="dram-banks">DRAM banks<a href="#dram-banks"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Bursting: access multiple locations of a line in the DRAM core array (horizontal parallelism)</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/j7rOC0e.png" alt="" /></p><div class="alert alert-success" role="alert"><p>Permet d’utiliser d’avantage la memoire a sa capacite totale</p></div><h2 id="bank-conflits-in-shared-memory">Bank conflits in shared memory<a href="#bank-conflits-in-shared-memory"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>If 2 threads try to perform 2 <strong>different</strong> loads in the same bank $\to$ <strong>Bank conflict</strong><li>Evert bank can provide 64 bits every cycle<li>Only 2 modes:<ol><li>Change after 32 bits<li>Change after 64 bits</ol></ul><p><img data-proofer-ignore data-src="https://i.imgur.com/3lRZhel.png" alt="" /></p><div class="alert alert-success" role="alert"><p>Demander la meme adresse par 2 threads differents ne pose pas de problemes</p></div><p>Les addresses consecutives ne sont pas dans les memes banques.</p><h3 id="2-way-conflicts">2-way conflicts:<a href="#2-way-conflicts"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.comXkxAjy.png" alt="" /></p><p>On fait 2 loads en shared memory</p><div class="alert alert-warning" role="alert"><p>Conflict = serialized access po bien</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/YmfxB50.png" alt="" /></p><h2 id="concrete-example-for-shared-memory">Concrete example for shared memory<a href="#concrete-example-for-shared-memory"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Bank size: 4B = 4 uint8<li>32 Banks - Many channels<li>Warp size = 32 threads</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/oQ66wiH.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/O7xreLP.png" alt="" /></p><p><strong>Pour le premier cas du tableau:</strong> <em>Est-ce qu’il a des threads qui demandent des addresses differentes dans une meme banque ?</em> Non</p><p><em>Quel est le nombre de loads (requetes memoire) qu’on va effectuer ?</em> Une seule requete memoire</p><div class="table-wrapper"><table><thead><tr><th>op<th>Items<th>Bank used<th>Conflict Free<th>#Loads<tbody><tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x]</code><td>$[0,1,…,31]$<td>$[0,1,…,7]$<td>Oui<td>1<tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x % 4]</code><td>$[0,1,2,3,0,1,2,3…]$<td>$[0]$<td>Oui<td>1<tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x + 1]</code><td>$[1,2,3,…32]$<td>$[0,1,…,8]$<td>Oui<td>1<tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x * 2]</code><td>$[0,1,2,3,…62]$<td>$[0,1,…,15]$<td>Oui<td>1<tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x * 8]</code><td>$[0,8,…248]$<td>$[0,2,…,30]$<td>Non (conflits sur toutes les banques)<td>2<tr><td><code class="language-plaintext highlighter-rouge">load M[tid.x * 12]</code><td>$[0,8,…248]$<td>$[0,1,…,31]$<td>Oui<td>1</table></div><p><code class="language-plaintext highlighter-rouge">load M[tid.x * 8]</code>: <img data-proofer-ignore data-src="https://i.imgur.com/jAku2dw.png" alt="" /></p><p><code class="language-plaintext highlighter-rouge">load M[tid.x * 12]</code>: <img data-proofer-ignore data-src="https://i.imgur.com/etNDHqA.png" alt="" /></p><h1 id="bank-conflicts-in-transpose">Bank conflicts in Transpose</h1><p><img data-proofer-ignore data-src="https://i.imgur.com/J0S9MsY.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/dQwmZid.png" alt="" /></p><p>Si on a 32 banques, on utilisent que 2 banques sur 32 et on a plein de conflits.</p><div class="alert alert-warning" role="alert"><p>Reading a column may cause bank conflict</p></div><h2 id="solution-to-bank-conflicts">Solution to bank conflicts<a href="#solution-to-bank-conflicts"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="with-padding-to-wrap_size--1">With padding (to WRAP_SIZE + 1)<a href="#with-padding-to-wrap_size--1"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/ERSDmSC.png" alt="" /></p><p><em>Comment se passe la lecture des columns ?</em> Avec le padding, on decale la lecture de 1 (on se decale d’une banaque). En lisant la 1ere column, on tombe toujours dans une banque differente, evitant ainsi les conflits.</p><h3 id="index-mapping-function">Index mapping function<a href="#index-mapping-function"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/FajHwoH.png" alt="" /></p><h2 id="performances-gbs-on-tesla-k40">Performances (GB/s on TESLA K40)<a href="#performances-gbs-on-tesla-k40"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/NlafuWa.png" alt="" /></p><h2 id="shared-memory-summary">Shared memory (summary)<a href="#shared-memory-summary"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Super fast access (almost as fast as registers)<li>But limited resources</ul><h3 id="occupancy">Occupancy<a href="#occupancy"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/H0OfGMx.png" alt="" /></p><h1 id="stencil-pattern">Stencil Pattern</h1><p>Use case:</p><ul><li>Dilation/erosion<li>Box (Mean) / Convolution Filters<li>Bilateral Filter<li>Gaussian Filter<li>Sobel Filter</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/k1qCOk7.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>Il y a une dependance entre les pixels</p></div><h2 id="naive-stencil-implem">Naive Stencil Implem<a href="#naive-stencil-implem"><i class="fas fa-hashtag"></i></a></h2></h2><p>Local average with a rectangle of radius $r$ (Ignoring border problems for now)</p><p><img data-proofer-ignore data-src="https://i.imgur.com/CDF9LBE.png" alt="" /></p><h2 id="naive-stencil-performance">Naive Stencil Performance<a href="#naive-stencil-performance"><i class="fas fa-hashtag"></i></a></h2></h2><p>Say we have this GPU:</p><ul><li>Peak power: 1 500 GFlops and Memory Bandwith: 200 GB/s</ul><p>All threads access global memory</p><ul><li>1 memory access for 1 FP addition<li>Requires 1500 x <code class="language-plaintext highlighter-rouge">sizeof(float)</code> = 6 TB/s of data<li>But only 200 GB/x mem bandwith $\to$ 50 GLFOPS (3% the peak)</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/WaaSHtg.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>Problem: too many access to global memory</p></div><div class="alert alert-success" role="alert"><p>Solution: tiling, copy data in shared memory to global memory</p></div><h2 id="handling-border">Handling Border<a href="#handling-border"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/7Ke7pfY.png" alt="" /></p><ol><li>Add border to the image to have in-memory access<li>Copy tile + border to shared memory</ol><h3 id="the-bad-way">The bad way<a href="#the-bad-way"><i class="fas fa-hashtag"></i></a></h3></h3><p>Each thread copies one value and border threads are then idle</p><p><img data-proofer-ignore data-src="https://i.imgur.com/i8tbBEO.png" alt="" /></p><h3 id="the-good-way">The good way<a href="#the-good-way"><i class="fas fa-hashtag"></i></a></h3></h3><p>A thread may copy several pixels</p><p><img data-proofer-ignore data-src="https://i.imgur.com/pvXUUJC.png" alt="" /></p><h2 id="stencil-pattern-with-tiling-performance">Stencil pattern with tiling performance<a href="#stencil-pattern-with-tiling-performance"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/wN0CVgX.png" alt="" /></p><h1 id="reduction-pattern">Reduction Pattern</h1><h2 id="intuition-for-reduction-pattern">Intuition for reduction pattern<a href="#intuition-for-reduction-pattern"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Reduction combines every elemet in a collection into one element using an associative operator</p></blockquote><h2 id="reduction-pattern-solution-1">Reduction pattern: solution 1<a href="#reduction-pattern-solution-1"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/WeQUfQW.png" alt="" /></p><p><em>Est-ce que c’est correct ?</em> Non, on va avoir des acces concurentiels (data race)</p><h3 id="data-race">Data race<a href="#data-race"><i class="fas fa-hashtag"></i></a></h3></h3><div class="alert alert-info" role="alert"><p>Plusieurs parties d’un programme qui essaie d’acceder sans ordre predefini a la meme donnee</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/R1q5wFy.png" alt="" /></p><p>We need to ensure that each of those read-compute-write sequences are <strong>atomic</strong>.</p><h2 id="atomics-reminder">Atomics reminder<a href="#atomics-reminder"><i class="fas fa-hashtag"></i></a></h2></h2><p>Atomics</p><ul><li>Read, modify, write in 1 operation<li>Cannot be mixed with accesses from other threads<li>On global memory and shared memory<li>Atomic operations to the same address are serialized</ul><p>Operations <img data-proofer-ignore data-src="https://i.imgur.com/J0MTpux.png" alt="" /></p><h2 id="reduction-pattern-corrected">Reduction Pattern Corrected<a href="#reduction-pattern-corrected"><i class="fas fa-hashtag"></i></a></h2></h2><p><strong>Accumulation in global memory</strong></p><p><img data-proofer-ignore data-src="https://i.imgur.com/8QdeZd4.png" alt="" /></p><p><strong>Analysis</strong></p><p><img data-proofer-ignore data-src="https://i.imgur.com/EcRZIrW.png" alt="" /></p><p>Time: 5.619 ms Correct result but <strong>high contention</strong> on the global atomic variable</p><div class="alert alert-success" role="alert"><p>The execution is actually <strong>sequential</strong> !</p></div><h2 id="global-atomics-is-this-really-parallel-">Global atomics: is this really parallel ?<a href="#global-atomics-is-this-really-parallel-"><i class="fas fa-hashtag"></i></a></h2></h2><p>This version will produce the right result. However, <strong>is it really parallel ?</strong></p><p>How our global atomic instruction is executed:</p><ol><li>lock memory cell<li>read old value<li>compute new value<li>write new value<li>release the memory cell</ol><p>Memory cell = cache line burst</p><p>Our kernel generates a lot of collisions on global memory</p><h2 id="leverage-shared-memory">Leverage Shared Memory<a href="#leverage-shared-memory"><i class="fas fa-hashtag"></i></a></h2></h2><p>Atomic operations are <strong>much</strong></p><h2 id="motivation-for-output-privatization">Motivation for output privatization<a href="#motivation-for-output-privatization"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/v8beAV1.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/mCcY84B.png" alt="" /></p><h2 id="using-shared-memory-1">Using shared memory<a href="#using-shared-memory-1"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/xGWVBNv.png" alt="" /></p><h2 id="reduction-pattern-v2-output-privatization">Reduction pattern V2: Output privatization<a href="#reduction-pattern-v2-output-privatization"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/R98fyGN.png" alt="" /></p><h3 id="with-sync">With sync<a href="#with-sync"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/hT3muek.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/qHcQ2Xc.png" alt="" /></p><h2 id="reduction-functions-and-trees">Reduction functions and trees<a href="#reduction-functions-and-trees"><i class="fas fa-hashtag"></i></a></h2></h2><p>On peut reduire en parallele plusieurs fragments</p><p><img data-proofer-ignore data-src="https://i.imgur.com/Ps454nh.png" alt="" /></p><h2 id="complexity-in-steps-and-operations">Complexity in steps and operations<a href="#complexity-in-steps-and-operations"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/SDnMiRA.png" alt="" /></p><p>The tree parallel version is:</p><ul><li>work efficient<li>not resource efficient<ul><li>Average number of thread $((N-1/\log_2(N))$ « peak requirement ($N/2$)</ul></ul><h3 id="proof-of-number-of-operations">Proof of number of operations<a href="#proof-of-number-of-operations"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/dpfV5lj.png" alt="" /></p><h2 id="reduction-pattern-tree-reduction-without-atomics">Reduction pattern: tree reduction without atomics<a href="#reduction-pattern-tree-reduction-without-atomics"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/jJNk6qz.png" alt="" /></p><ul><li>Use a local sum without atomics<ul><li>Map reduction tree to compute units (threads)</ul><li>Add to a global atomic once for each block</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/Dv7RNfJ.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/WEGvBcP.png" alt="" /></p><p><strong>What is happening ?</strong> The (naive) tree version is slower than the locally sequential version</p><h2 id="sp-starvation">Sp starvation<a href="#sp-starvation"><i class="fas fa-hashtag"></i></a></h2></h2><p>In each iteration, 2 control flow paths will be sequentiall traversed for each warp</p><ul><li>Threads that perform addition and threads that do not<li>Threads that do not perform addition <strong>still consume execution resources</strong></ul><h2 id="resource-efficient-version">Resource efficient version<a href="#resource-efficient-version"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/vA8Y2QC.png" alt="" /></p><p>Tous les threads vont etre utilise sauf a la fin. On va avoir que des warps actifs, a chaque iterations on libere la moitie des warps.</p><p><img data-proofer-ignore data-src="https://i.imgur.com/hKwmj9M.png" alt="" /></p><h2 id="a-quick-analysis">A quick analysis<a href="#a-quick-analysis"><i class="fas fa-hashtag"></i></a></h2></h2><p>For a 1024 thread block</p><ul><li>No divergence on the first 5 steps<ul><li>1024, 512, 256, 128, 64, 32 consecutive threads are active in each step<li>All threads in each warp either all active or all inactive</ul><li>The final 5 steps will still have divergence<ul><li>Can use warp-level optimization then (warp suffle)</ul></ul><h2 id="limit-global-collision">Limit global collision<a href="#limit-global-collision"><i class="fas fa-hashtag"></i></a></h2></h2><p><em>What happens with very large input arrays ?</em> <strong>Lot of global atomics</strong></p><p><em>How to avoid this ?</em> <strong>Global array, one cell for each block</strong></p><ul><li>No more locks<li>But requires a second level of reduction</ul><p><strong>More work per thread</strong></p><ul><li>Just fire enough blocks to hide latency<li>Sequential reduction, then tree reduction<li>“algorithm cascading”</ul><h2 id="algorithm-cascading">Algorithm cascading<a href="#algorithm-cascading"><i class="fas fa-hashtag"></i></a></h2></h2><p>Perform first reduction during the collaborative loading</p><div class="alert alert-warning" role="alert"><p>Warning: kernel launch parameters must be scaled accordingly !</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/LsGjS5w.png" alt="" /></p><h2 id="last-optimizations">Last optimizations<a href="#last-optimizations"><i class="fas fa-hashtag"></i></a></h2></h2><p>Loop unrolling</p><ul><li>Unroll tree reduction loop for the last warp (less sync needed)<li>Unroll all tree reduction loops (need to know block size)<li>Unroll the sequential reduction loop (knowing the work per thread)</ul><h1 id="histogram-computation">Histogram computation</h1><h2 id="mandelbrot-practice-session">Mandelbrot practice session<a href="#mandelbrot-practice-session"><i class="fas fa-hashtag"></i></a></h2></h2><p>During the practice session, you will have to compute the <strong>cumulated histogram</strong> of the image.</p><ol><li>Compute the histogram $H$<ul><li>Count the number of occurences of each value</ul><li>Computed the cumulated histogram $C$</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/cGXIbYP.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>Inefficient, <em>non-coalesced</em> memory access</p></div><h2 id="first-sample-code">First sample code<a href="#first-sample-code"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/eMOR4VG.png" alt="" /></p><p><em>What is the issue ?</em> Ajouter un data array</p><p><img data-proofer-ignore data-src="https://i.imgur.com/rnUgffz.png" alt="" /></p><h2 id="parallel-algorithm-using-output-privatization">Parallel algorithm using output privatization<a href="#parallel-algorithm-using-output-privatization"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="local-histogram">Local histogram<a href="#local-histogram"><i class="fas fa-hashtag"></i></a></h3></h3><h4 id="initialization">Initialization<a href="#initialization"><i class="fas fa-hashtag"></i></a></h4></h4><p>Shared memory must be initialized This can be done with the “comb-like” pattern</p><p><img data-proofer-ignore data-src="https://i.imgur.com/unIYdXO.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>We need synchronization after this stage</p></div><h4 id="computation">Computation<a href="#computation"><i class="fas fa-hashtag"></i></a></h4></h4><p>Like previous code, but with local atomics</p><p><img data-proofer-ignore data-src="https://i.imgur.com/ke6xZ3o.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>We need synchronization after this stage</p></div><h4 id="commit-to-global-memory">Commit to global memory<a href="#commit-to-global-memory"><i class="fas fa-hashtag"></i></a></h4></h4><p><img data-proofer-ignore data-src="https://i.imgur.com/je24ZHQ.png" alt="" /></p><p><code class="language-plaintext highlighter-rouge">int n = blockDim.x * threadIdx.y + threadIdx.x</code></p><h2 id="summary-1">Summary<a href="#summary-1"><i class="fas fa-hashtag"></i></a></h2></h2><p>Performance boosters:</p><ul><li>Coalesced accesses<li>Output privatization</ul><p>Requirements:</p><ul><li>atomics<li>synchronization</ul><h1 id="scan-pattern">Scan pattern</h1><h2 id="what-is-a-scan-">What is a scan ?<a href="#what-is-a-scan-"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>Scan computes all partial reductions of a collection</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/yvFGSq8.png" alt="" /></p><p>Usage:</p><ul><li>Integration (cumulated histogram)<li>Resource allocation (memory to parallel threads, camping spots…)<li>Base building block for many algorithms (sorts, strings comparisons)</ul><h2 id="performance-baselines">Performance baselines<a href="#performance-baselines"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/Cj1a8dh.png" alt="" /></p><p><strong>Sequential version</strong></p><h3 id="naive-parallel-version">Naive parallel version<a href="#naive-parallel-version"><i class="fas fa-hashtag"></i></a></h3></h3><p>Have every thread to add up all x elements needed for the y element</p><h2 id="scan-pattern-at-the-warp-or-block-level">Scan pattern at the warp or block level<a href="#scan-pattern-at-the-warp-or-block-level"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="kogge-stone">Kogge-Stone<a href="#kogge-stone"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/zh1JFSK.png" alt="" /></p><ul><li>Number of steps: $\log N$ (bien)<li>Ressource efficiency (bien)<li>Work efficiency $\sim N\log N$ (pas bien)</ul><h3 id="brent-kung">Brent-Kung<a href="#brent-kung"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/0JJb8j7.png" alt="" /></p><ul><li>Number of steps: $2\log N$<li>Ressource efficiency: all warps remain active till the end (pas bien)<li>Work efficiency: $2N$ (bien)</ul><h3 id="sklansky">Sklansky<a href="#sklansky"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/Skxx9gb.png" alt="" /></p><ul><li>Number of steps: $\log N$<li>Ressource efficiency: bien<li>Work efficiency: $\frac{N}{2}\log N$ (bien)</ul><h2 id="scan-pattern-at-the-block-or-grid-level">Scan Pattern at the Block or Grid Level<a href="#scan-pattern-at-the-block-or-grid-level"><i class="fas fa-hashtag"></i></a></h2></h2><p>The patterns before can be applied:</p><ul><li>At the warp level (no sync until Volta)<li>At the block level (thread sync)</ul><p>At the global level: multi-level kernel app in global memory</p><ul><li>Scan then propagate<li>Reduce then scan</ul><h3 id="scan-then-propagate">Scan then propagate<a href="#scan-then-propagate"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/tEcHvf6.png" alt="" /></p><h3 id="reduce-then-scan">Reduce then scan<a href="#reduce-then-scan"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/qzKo8RB.png" alt="" /></p><h2 id="summary-2">Summary<a href="#summary-2"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/sfbYbdL.png" alt="" /></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s8/'>Image S8</a>, <a href='/cours/categories/irgpu/'>IRGPU</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/s8/" class="post-tag no-text-decoration" >S8</a> <a href="/cours/tags/irgpu/" class="post-tag no-text-decoration" >IRGPU</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=IRGPU: Patterns for massively parallel programming - Cours&url=https://lemasyma.github.io/cours/posts/irgpu_patterns/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=IRGPU: Patterns for massively parallel programming - Cours&u=https://lemasyma.github.io/cours/posts/irgpu_patterns/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=IRGPU: Patterns for massively parallel programming - Cours&url=https://lemasyma.github.io/cours/posts/irgpu_patterns/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/irgpu_introduction/"><div class="card-body"> <em class="timeago small" date="2021-05-03 10:00:00 +0200" >May 3, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IRGPU: Introduction</h3><div class="text-muted small"><p> Lien de la note Hackmd Agenda GPU and architectures (2h) Programming GPUs with CUDA (2h) TP 00 CUDA (3h) Efficient programming with GPU (2h) TP 01 CUDA GPU and architectures Why using...</p></div></div></a></div><div class="card"> <a href="/cours/posts/irgpu_getting-started/"><div class="card-body"> <em class="timeago small" date="2021-05-03 14:00:00 +0200" >May 3, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IRGPU: Getting started with CUDA</h3><div class="text-muted small"><p> Lien de la note Hackmd CUDA overview What is CUDA ? A product It enables to use NVidia GPUs for computation A C/C++ variant Mostly C++ 15 compatible, with extensions and also some restri...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_hyperplan/"><div class="card-body"> <em class="timeago small" date="2021-05-07 10:00:00 +0200" >May 7, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Hyperplan d'appui</h3><div class="text-muted small"><p> Lien de la note Hackmd Rappels Hyperplan d’appui a une partie $A$ de $\mathbb R^n$ en un point $p\in A$, est un hyperplan affine de $\mathbb R^n$ qui laisse $A$ dans un des deux demi-espaces de...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/rse_1/" class="btn btn-outline-primary" prompt="Older"><p>RSE: Premier cours</p></a> <a href="/cours/posts/ocvx_hyperplan/" class="btn btn-outline-primary" prompt="Newer"><p>OCVX: Hyperplan d'appui</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
