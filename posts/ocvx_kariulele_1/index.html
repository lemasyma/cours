<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="OCVX: Optimisation Convexe 1, suite" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Optimisation Convexe 1, suite" /><meta property="og:description" content="Optimisation Convexe 1, suite" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-03-22T21:10:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="OCVX: Optimisation Convexe 1, suite" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-03T16:45:54+02:00","datePublished":"2021-03-22T21:10:00+01:00","description":"Optimisation Convexe 1, suite","headline":"OCVX: Optimisation Convexe 1, suite","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/"},"url":"https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/"}</script><title>OCVX: Optimisation Convexe 1, suite | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>OCVX: Optimisation Convexe 1, suite</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>OCVX: Optimisation Convexe 1, suite</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-03-22 21:10:00 +0100" data-toggle="tooltip" data-placement="bottom" title="Mon, Mar 22, 2021, 9:10 PM +0100" >Mar 22, 2021</em> </span> <span> Updated <em class="timeago" date="2021-10-03 16:45:54 +0200 " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 3, 2021, 4:45 PM +0200" >Oct 3, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3822 words"> <em>21 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Notes de ce cours par <a href="https://github.com/kariulele">Kariulele</a> et Bjorn (Un grand merci a eux!)</p><blockquote><h4 id="trouver-lextremum-dune-parabole">Trouver l’extremum d’une parabole<a href="#trouver-lextremum-dune-parabole"><i class="fas fa-hashtag"></i></a></h4></h4><p>$f(x) = ax^2 + bx +c \qquad a &gt; 0$ $f’(x) = 2ax +b$ $x^{*} \qquad tq f’(x^{*}) = 0$ $2ax^{*} + b = 0$ $x^{*} = -\frac {-b}{2a}$</p>\[\begin{aligned} f^{*} &amp;= f(x^{*})\\ &amp;= a \left(-\frac{b} {2a}\right)^2 + b\left(-\frac{b}{2a}\right) + c\\ &amp;= \frac{b^2}{4a}- \frac{b^2}{2a} +c\\ &amp;= -\frac{b^2}{4a} + c \end{aligned}\]</blockquote><p>$f: \mathbb R \longrightarrow \mathbb R$ f est dérivable en $x_0$ : $\underset{h \rightarrow 0}{lim} \frac{f(x+h) - f(x)}{h}$ est finie.</p><p>Et $\underset{h \rightarrow 0}{lim} \frac{f(x_0 + h) - f(x_0)}{h} = f’(x_0)$ $\underset{h \rightarrow x_0}{lim} \frac{f(x)-f(x_0)}{x - x_0}$ $f = o_{x_0}(g)$ $f$ est négligeable par rapport à $g$ en $x_0$. $\Leftrightarrow$ Il existe une fonction $\varepsilon : \mathbb R \rightarrow R$ avec $\varepsilon(x) \underset{x \rightarrow x_0}{\longrightarrow} 0$ Et $f(x) = \varepsilon(x)g(x)$ au voisinage de $x_0$.</p><p>Si $g$ ne s’annule pas au voisinage de $x_0$ $f=o_{x_0}(g) \Leftrightarrow \underset{x \rightarrow x_0}{lim} \frac{f(x)}{g(x)} = 0$</p><p>$\underset{h \rightarrow 0}{lim} \frac{f(x_0 + h) - f(x_0)}{h} = f’(x_0)$ $\underset{h \rightarrow 0}{lim} \frac{f(x_0 + h) - f(x_0)}{h} - \frac{hf’(x_0)}{h} = 0$ soit $\varepsilon : \mathbb R \longrightarrow \mathbb R$ tq $\underset{h \rightarrow 0}{\varepsilon (h) \rightarrow 0}$ $\underset{h \rightarrow 0}{lim} \frac{f(x_0 + h) - f(x_0) - hf’(x_0)}{h} = \underset{h \rightarrow 0}{lim} \space\varepsilon (h) = 0$</p><p>$\frac {f(x_0 + h) -f(x_0) - hf’(x_0)}{h} = \varepsilon(h)$ $f(x_0 + h) -f(x_0) - hf’(x_0) = h\varepsilon(h)$</p><p>$f(x_0 + h) = f(x_0) + hf’(x_0) + h\varepsilon(h) = f(x_0) + hf’(x_0) + o_0(h)$ $f(x) = f(x) + (x - x_0)f’(x_0) + (x - x_0) \underbrace{\varepsilon(x - x_0)}_{o_0(x- x_0)}$</p><p>$f: \mathbb R^n \longrightarrow \mathbb R$ $x = \begin{pmatrix}x_1 \ \vdots \ x_n \end{pmatrix} \longmapsto f(x_1, \dots, x_n)$</p><p>$f(x_1, … , x_n) = x_1 + x_2 + … + x_n$ La $k^{ième}$ dérivée partielle de f existe en $x_0 \in \mathbb R^n$ $\Leftrightarrow$ la fonction $\underset{t \rightarrow f(x_0,…,x_n)}{\varphi :\ \mathbb R \rightarrow \mathbb R}$ est dérivable en 0</p><p>et $\varphi’(0) = \frac{\partial f}{\partial x_k}(x_0) \Leftrightarrow \partial k f(x_0)$</p><p>$f(x,y) = \begin{cases}\frac{xy}{x^2 + y^2} \space \text{ si } (x,y) \ne (0,0) <br /> 0 \qquad \text{ si } (x,y) = (0,0)\end{cases}$</p><p>\begin{aligned}\frac{\partial f}{\partial x}(x,y) &amp;= \frac{\partial}{\partial x}\left(\frac{xy}{x^2 + y^2}\right) <br /> &amp;= y \frac{\partial}{\partial x} \left( \frac{x}{x^2 + y^2} \right) <br /> &amp;= y \frac{x^2 + y^2 -x(2x)}{(x^2 + y^2)^2} <br /> &amp;= y \frac{y^2 - x^2}{(x^2 + y ^2)^2}\end{aligned}</p><p>$\frac{\partial f}{\partial x} (t,0) = 0 \qquad \frac{\partial f}{\partial y}(0,t) = 0$</p><p>$\frac{\partial f}{\partial x}(x,y) \Leftrightarrow$ on dérive selon l’axe $(o_x)$ $\Leftrightarrow$ on dérive selon le vecteur $e_x = (1,0)$</p><p>$\frac{\partial f}{\partial y}(x,y) \Leftrightarrow$ on dérive selon l’axe $(o_y)$</p><h3 id="la-derivee-directionnelle">La derivee directionnelle<a href="#la-derivee-directionnelle"><i class="fas fa-hashtag"></i></a></h3></h3><p>Dans le cas de $n$ variables : $f:\mathbb R^n \longrightarrow \mathbb R$ $\frac{\partial f}{\partial x_k}(x)$ = on dérive par rapport à la $k^{ième}$ variable $\Leftrightarrow$ on dérive selon la $k^{ième}$ variable $\Leftrightarrow$ on dérive selon le vecteur $ek = (0, \dots, o, \underbrace{1}_{k^{ième}}, 0, \dots, 0)$</p><div class="alert alert-info" role="alert"><p><strong>Définition</strong> : On appelle dérivée directionnelle de $f$ en $x_0$ suivant le vecteur $h \in \mathbb R^2$ et on note $D_hf(x_0)$ la dérivée en 0 de la fonction \(\varphi : \begin{aligned}\mathbb R &amp; \longrightarrow \mathbb R\\ t &amp;\longmapsto f(x_0 + th) \end{aligned}\)</p></div><p>$\frac{\partial f}{\partial x}(x_0) \equiv$ derivee de</p>\[\varphi :\begin{aligned}\mathbb R &amp; \rightarrow \mathbb R\\ t &amp;\longmapsto \underbrace{f(x_{01} +, \dots, x_{0k+t}, \dots, x_{0n})}_{\begin{pmatrix}x_{01} \\ \vdots\\ x_{0n}\end{pmatrix} + t\begin{pmatrix}0 \\ \vdots \\ 1 \rightarrow k^e\\ \vdots \\ 0\end{pmatrix}} \end{aligned}\]<p>$f: \mathbb R^2 \longrightarrow R \qquad \space \qquad x_0=(1,2)$ $(x,y) \longmapsto x^2 - y^2 \qquad h=(3,5)$</p><p>$\varphi : \mathbb R \longrightarrow \mathbb R$ $t \longmapsto f(x_0 + th)$</p><p>$\varphi(t) = f\left(\begin{pmatrix}1 \ 2 \end{pmatrix} + t \begin{pmatrix}3 \ 5\end{pmatrix}\right)$ $= f(1+3t, 2+ 5t)$ $= (1 +3t)^2 - (2 + 5t)^2$ $= 1 + 6t + 9t^2 -(4 - 20t + 25t^2)$ $= -3 - 14t - 16t^2$</p><p>$\varphi ‘(t) = -14 -32t$ $\varphi ‘(0) = -14 = D_h(x)$</p><p>$h \leftrightarrow \alpha h$ $D_{\alpha h}f(x_0) = \alpha D_hf(x_0)$ On parle de dérivée directionnelle selon la direction de $h \in \mathbb R^n \verb++ {0}$ uniquement quand $h$ est unitaire (par opposition à la dérivée directionnelle selon le vecteur $h$).</p><p>Malheureusement, l’existence de derivees directionnelles en $Vn$ point selon tout vecteur n’implique pas la continuité en ce point.</p><p>$f(x,y) = \begin{cases}\frac {y^2}{x} \qquad x \ne 0<br /> y \space\space\qquad x = 0\end{cases}$</p><p>En $(0,0)$ soit $h = \begin{pmatrix} h_1 \ h_2\end{pmatrix} \ne (0,0)$</p><p>\begin{aligned}\varphi(t) = f(th) = f(th_1, th_2) &amp;= \begin{cases} \frac{(th_2)^2}{th_1} \qquad\space\space h \neq 0<br /> th_2 \qquad\quad\ h = 0\end{cases}<br /> &amp;= \begin{cases} t\frac{h_2^2}{h_1} \qquad\space\space h \neq 0<br /> th_2 \qquad\quad\ h = 0\end{cases}\end{aligned}</p><p>$\varphi ‘(t) = \begin{cases} \frac{h_2^2}{h_1} \qquad h_1 \ne 0 <br /> h_2 \qquad h_1 = 0\end{cases}$ = $\varphi ‘(0)$</p><p>si $g$ est continue en $0$ et $f$ continue en $g(0)$ alors $f \circ g$ est continue en $0$ $g: \mathbb R \longrightarrow \mathbb R^2$ $t \longmapsto (t^2, t)$</p><p>$f \circ g : \mathbb R \longrightarrow \mathbb R^2$ $t \longmapsto f \circ g(t) = f(g(t))$</p><p>$f(g(t)) = f(t^2, t) = \begin{cases}\frac{t^2}{t^2} \qquad t \neq 0<br /> 0 \qquad\space t = 0\end{cases}$</p><p>$f \circ g(t) = \begin{cases}1 \qquad\space t \neq 0<br /> 0 \qquad\space t = 0\end{cases}$</p><p>Donc $f \circ g$ pas continue en $0$ $\Rightarrow f$ pas continue en $g(0) = (0,0)$</p><p>$f(x_0 +h) = f(x_0) + hf’(x_0) + \begin{cases}o_0(h)\ h \varepsilon(h) \text{ avec } \varepsilon(h) \qquad \varepsilon (h) \underset{h \rightarrow 0}{ \longrightarrow} 0\end{cases}$</p><div class="alert alert-info" role="alert"><p><strong>Définition</strong> : On dit que $f: \mathbb R^n \longrightarrow \mathbb R$ est differentiable de $x_0$ ssi il existe une application linéaire $d_{x_0}f$ (aussi noté $df_{x_0}$) tq $f(x_0 + h) = f(x_0) + d_{x_0}f(h) + \underset{||H|| \varepsilon (h)}{o_0(h)}$</p></div><p>$h \mapsto h f’(x_0)$ est linéraire $\varepsilon : \mathbb R^n \longrightarrow \mathbb R$ $d_{x_0} f:h \mapsto d_{x_0}f(h) = h \times f’(x_0)$</p><div class="alert alert-info" role="alert"><p><strong>Propriété</strong> : Si $f$ est différentiable en $x_0$ alors $f$ est continue en $x_0$ <strong>Propriété</strong> : Si $f$ est différentiable en $x_0$ alors $f$ admet des dérivées directionnelles selon tout vecteur $h \in \mathbb R^n \verb++ {0}$, et la dérivée directionnelle vaut $D_hf(x_0) = d_{x_0}f(h)$</p></div><p>Soit $f$ différentiable en $x_0$. Donc les dérivées partielles $\frac{\partial f}{\partial x_k}$ existent en $x_0$</p><p>Soit $h \in \mathbb R^n \verb++ {0}$ et $(e_1, \dots , e_n)$ la base $h = \begin{pmatrix} h_1 \ \vdots \ h_n\end{pmatrix} = \sum_{i = 1}^{n} h_i e_i$</p><p>$D_hf(x_0) = d_{x_0}f(h) = d_{x_0}f(\overset{n}{\underset{i=1}{\sum}} h_ie_i) = \overset{n}{\underset{i=1}{\sum}} h_i d_{x_0}f(e_i)=\overset{n}{\underset{i=1}{\sum}} h_i \frac{\partial f}{\partial x_i}x_0$</p><p>$d_{x_0}f(h) = \langle \nabla f(x_0), h \rangle$</p><p>Soit $f:\mathbb R^n \rightarrow \mathbb R$ on définit le vecteur gradient de $f$ en $x_0$ par $\nabla f(x_0) = \begin{pmatrix}\frac{\partial f}{\partial x_1}(x_0) <br /> \vdots <br /> \frac{\partial f}{\partial x_n}(x_0)\end{pmatrix}$</p><p>Si $f$ différentiable en $x_0$, alors $d_{x_0} f:h \longmapsto \langle \nabla f(x), h \rangle$ $d_{x_0} :h \longmapsto h f(x_0)$</p><p>Soit \(f: \begin{aligned}\mathbb R^n &amp;\longmapsto \mathbb R^p\\ x = (x_1, \ldots, x_n) &amp; \longmapsto f(x) = (f_1(x), \ldots, f_p(x))\end{aligned}\)</p><p>Soit $x_0 \in \mathbb R^n$ et $f$ différentiable en $x_0$ Les $f_1,\dots, f_p$ sont différentiables en $x_0$</p><p>Soit $h \in \mathbb R^n \qquad \overbrace{f(x + h)}^{\in \mathbb R^p} = \overbrace{f(x_0)}^{\in \mathbb R^p} + \overbrace{d_{x_0}f(h)}^{\in \mathbb R^P} + o_0(h)$</p>\[\begin{pmatrix} f_1(x_0 + h) \\ \vdots \\ f_p(x_0 + h)\end{pmatrix} =\begin{pmatrix} f_1(x_0) \\ \vdots \\ f_p(x_0)\end{pmatrix} + \begin{pmatrix}d x_0 f_1( h) \\ \vdots \\d x_0 f_p(h)\end{pmatrix} + o_0(h)\] \[f(x_0 +h) = f(x_0) + \begin{pmatrix} \langle \nabla f_1(x_0), h \rangle\\ \vdots \\ \langle \nabla f_p(x_0), h \rangle \end{pmatrix} + o_0(h)\] \[\langle \nabla f_i(x_0),h \rangle = \nabla f_i(x_0)^T h = \left(\frac{\partial f_i}{\partial x_1}(x_0), \ldots, \frac{\partial f_i}{\partial x_n}(x_0)\right)\begin{pmatrix}h_1 \\ \vdots \\ h_n\end{pmatrix} \qquad \frac{\partial f_i}{\partial x_j}(x_0) = \partial_j f_i(x_0)\] \[f(x_0+h) = f(x_0) + \begin{pmatrix} (\partial_1 f_1(x_0) \dots \dots \partial_n f_1(x_0))h \\ \vdots \\ (\partial_1 f_p(x_0) \dots \dots \partial_n f_p(x_0))h \end{pmatrix} + o_0(h)\] \[\text{les p composantes de f :} \begin{pmatrix} \frac{\partial f_1}{\partial x_1}(x_0)&amp; \dots&amp; \frac{\partial f_1}{\partial x_n}(x_0)\\ \vdots &amp; &amp; \vdots \\ \frac{\partial f_p}{\partial x_1}(x_0)&amp; \dots&amp; \frac{\partial f_p}{\partial x_n}(x_0) \end{pmatrix} \begin{pmatrix} h_1 \\ \vdots \\ h_n\end{pmatrix}\]<p>On appelle jacobienne de $f$ en $x_0 = (u_1, \ldots, u_n)\begin{pmatrix}v_1 \ \vdots \ v_n\end{pmatrix}$ la matrice : \(\mathcal J_{x_0}f = \left[\frac{\partial f_i}{\partial x_j}(x_0)\right]_{\begin{aligned}i &amp;= 1, \ldots, p\\ j &amp;= 1, \ldots, n\end{aligned}}\) Telle que \(\underbrace{f(x_0 + h)}_{\in \mathbb R^p} = \underbrace{(x_0)}_{\in \mathbb R^p} + \underbrace{\underbrace{\mathcal J_ {x_0}f}_ {\in \mathbb M_{p,n}(\mathbb R)} \times \underbrace{h}_ {\in \mathbb R^p}}_{\in \mathbb R^p} + o_0(h)\)</p><p>$d_{x_0}f: h \longmapsto \mathcal J_{x_0}f \times h$ est bien linéaire</p><p>Soit $f:\mathbb R^n \rightarrow \mathbb R^p$ differentiable en $x_0 \in \mathbb R^n$ Soit $g:\mathbb R^p \rightarrow \mathbb R^n$ differentiable en $f(x_0) \in \mathbb R^p$</p><p>Alors la composee $g \circ f = d_{f(x_0)} g \circ d_{x_0} f$ Avec les jacobiennes $\mathcal J_{x_0} g \circ f = \mathcal J_{f(x_0)} g <br /> {\times}^{\text{produit matriciel}} \mathcal J_{x_0}f$</p><p>$(g \circ f)’ = f’ \times (g’ \circ f)$ $(g \circ f)(x) = g(f(x))$ $(g \circ f’)(x) = f’(x) \times g’(f(x))$</p><h3 id="interprétation-géométrique-du-gradient">Interprétation géométrique du gradient<a href="#interprétation-géométrique-du-gradient"><i class="fas fa-hashtag"></i></a></h3></h3><p>On se limite désormais au cas des fonctions convexes. Quand les resultats énoncés s’appliquent à un cadre plus général que l’on spécifiera. Les questions auxquelles on n’a pas encore de réponses générales :</p><ul><li>“Direction” de minimisation d’une fonction objectif<li>Trouver des hyperplans d’appui au lieu admissible d’un problème d’optimisation</ul><blockquote><p>C’est la proposition suivante qui permet d’apport une réponse à ces 2 questions :</p></blockquote><div class="alert alert-info" role="alert"><p><strong>Proposition :</strong> Soit $f:U \subset \mathbb R^N \longrightarrow \mathbb R$ fonction convexe et différentiable en $a \in U$. $\nabla f(a)$ définit un hyperplan d’appui à $\mathcal C_{\le r}(f) (r = f(a))$</p><p><img data-proofer-ignore data-src="https://i.imgur.com/CPpicHe.png" alt="" /></p></div><p>[Q 4-33] On cherche à résoudre le problème de minimisation : $\min f_0(x,y)= 2x + y$ sujet à : $3x^2 + y^2 \leqslant 4$</p><p><img data-proofer-ignore data-src="https://i.imgur.com/SQDB0Cg.png" alt="" /></p><p>Pour minimiser $f_0$ on part vers la “gauche” du dessin ; vers la direction opposée au gradient de $f_0$. La position “limite” de ces courbes de niveaux (la courbe de niveau qui réalise la valeur optimale) correspond à un hyperplan d’appui.</p><p>$\mathbb{A}$ est le sous-niveau de niveau 4 de $f_1(x, y) = 3x^2+y^2$ \(\nabla f_1(x, y) = \begin{pmatrix}6x \\ 2y\end{pmatrix}\)</p><p><img data-proofer-ignore data-src="https://i.imgur.com/g7Nr8NR.png" alt="" /></p><p>On cherche donc un point (x, y) tel que : \(\nabla f_1(x, y) + \lambda \nabla f_0(x, y) = 0 \qquad \text{avec } \lambda \geqslant 0\)</p><p>Pour trouver (x,y) on cherche a resoudre: $\begin{cases}\begin{pmatrix} 6x\ 2y\end{pmatrix} = -\lambda \begin{pmatrix} 2\ 1\end{pmatrix}<br /> 3x^2 +y^2 =4 \end{cases}$</p><p>Des deux premières équations on obtient: \(x=-\frac{\lambda}{3}, y=-\frac{\lambda}{2}\)</p><p>En réinjectant dans la deuxième équation : \(x=-\frac{1}{\sqrt{21}}, y=-2\sqrt{\frac{3}{7}}\)</p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Avec les notations de la proposition on appelle espace tangeant à $\mathcal C_{r}(f)$ en a l’espace affine. \begin{aligned} T_a(f) &amp;= a + \nabla f(a)^\bot<br /> &amp;= a+ {x | \nabla f(a)^\top x = 0}<br /> \end{aligned}</p></div><p>La proposition donne, telle quelle, la réponse à la question 2. posée ci-dessus. Elle suggère également une direction vers laquelle minimiser la valeur objectif de f. On suppose que $\nabla f(a)\neq 0$, on regarde $-t\nabla f(a)$ avec $t &gt; 0$.</p><p>Pour $t$ proche de 0, \(f(u | \nabla f(a))-f(a)=\nabla f(a)^\top(-t\nabla f(a)) + ||t\nabla f(a)||\epsilon(\nabla f(a))\) le $O_o(t)$ est négligable devant $-t\nabla f(a)^\top \nabla f(a)=-t||\nabla f(a)||^2_2$ L’expression $f(a-t\nabla f(a))=f(a)$ est du signe de $-t||\nabla f(a)||^2_2$. Pour $t$ assez pertit \(f(a -t\nabla f(a)) \leqslant f(a)\)</p><div class="alert alert-info" role="alert"><p><strong>Remarque :</strong></p><ol><li>L’étude précédente est contrainte par le fait “$t$ assez petit”. Ca donne une idée de direction du min, pas une garantie.<li>L’étude ci-dessus ne nécessite pas de convexité.</ol></div><h4 id="caracteristique-du-premier-ordre-de-la-convexite-">Caracteristique du premier ordre de la convexite :<a href="#caracteristique-du-premier-ordre-de-la-convexite-"><i class="fas fa-hashtag"></i></a></h4></h4><p>$f: T \subset \mathbb R^b \longmapsto \mathbb R$ est convexe si:</p><ul><li>$U$ est convexe<li>$\forall x,y \in U, f(y) - f(x) \geqslant \nabla f(x)^T(y - x)$</ul><p>En supposant cette caracterisation VRAIE :</p><p>Soit $y \in \mathcal C_{\le r}(f);$ on veut montrer $\nabla f(x)^T(y -x) \le 0$</p><p>Or comme f est convexe on a : \(\nabla f(x)^T(y-x) \leqslant \underbrace{f(y)}_{\le r}\underbrace{(fx)}_{=r}\) d’ ou $\nabla f(x)^T (y-x) \leqslant 0$</p><h4 id="preuve-de-la-caractérisation-de-convexité">Preuve de la caractérisation de convexité<a href="#preuve-de-la-caractérisation-de-convexité"><i class="fas fa-hashtag"></i></a></h4></h4><p>Convexe $\Leftrightarrow$ ($\nabla$ convexe)</p><p>Soient $x,y \in U, t \in [0,1]$ On regarde la fonction \(g(t) = f((1-t)x + ty)\)</p><p>La definition de convexite de f :</p>\[\begin{matrix} &amp; f((1-t) x + t(y)) &amp; \leqslant &amp; (1-t)f(x) + tf(y)\\ \Leftrightarrow &amp; g(t) &amp; \leqslant &amp; (1-t)g(0) + g(1)\\ \Leftrightarrow &amp; g(t) - g(0) &amp; \leqslant &amp; t(g(1) - g(0))\\ \Leftrightarrow &amp; \frac{g(t) - g(0)}{t} &amp; \leqslant &amp; g(1) - g(0)\\ \Rightarrow &amp; g(0) &amp; \leqslant &amp; f(y) - f(x) \end{matrix}\]<p>Or $g(0) \nabla f(x)^T(y -x)$ D’ou $\color{green}{\boxed{\nabla f(x)’(y-x) \leqslant f(y) - f(x)}}$</p><p>($\nabla$ convexe) et $U$ convexe $\Rightarrow$ convexe</p><hr /><p>Soient $x,y \in U \qquad z_t = (1 - t)x + ty$</p>\[\begin{aligned} t \times [f(y)- f(z_t)] &amp;\geqslant&amp; \nabla f(z_t)^\top(y -z_t)\\ (1-t) \times [f(x) - f(z_t) &amp;\geqslant&amp; \nabla f(z_t)(x-z_t)]\\ tf(y) + (1-t)f(x) + tf(z_t) - (1 -t)f(z_t) &amp;\geqslant&amp; \nabla f(z_t)(t(y -z_t) + (1-t)(x - z_t)) \color{orange}{(D)} \end{aligned}\]<p>$\color{orange}{(D)}$ : $\nabla f(z_t)(ty + (Lt)x -z_t) = 0$</p><p>$(D) = 0$</p><p>$(G):$ \begin{aligned} &amp;tf(y) + (1-t)f(x) + f(z_t)<br /> &amp;= tf(y) + (1-t)f(x) + f(ty + (1-t)x) \end{aligned}</p><blockquote><p><strong>Exercice :</strong> Trouver les points sur le paraboloïde $z = 4x^2 + y^2$ où le plan tangent est parallèle au plan $x + 2y + z = 6$. De même pour le plan $3x + 5y - 2z = 5$</p></blockquote><h2 id="problèmes-doptimisation">Problèmes d’optimisation<a href="#problèmes-doptimisation"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="catégorie-des-problèmes-convexes">Catégorie des problèmes convexes<a href="#catégorie-des-problèmes-convexes"><i class="fas fa-hashtag"></i></a></h3></h3><div class="alert alert-info" role="alert"><p><strong>Convention :</strong> pour simplifier la notation on note $f : \mathbb R^n \longrightarrow \mathbb R$ une fonction qui n’est pas nécessairement, définie sur $\mathbb R^n$</p></div><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Un problème d’optimisation convexe est un problème qui s’exprime sous la forme $\min f_0(x)$, sujet à: \(f_i(x) \leqslant 0 \: \forall i \in \{1,\dots,m\}\\ f_j(x) = 0 \: \forall j \in \{1,\dots, p\}\)</p><p>Où $f_0, f_i, h_j : \mathbb R^n \longrightarrow \mathbb R$ sont convexes et de plus les $h_j$ sont affines.</p></div><p>On peut en particulier réécrire $(P)$ sous la forme: $\min f_0(x)$ sujet à : \(\begin{aligned} f_i(x) &amp;\leqslant 0 \: \forall i \in \{1,...,m\}\\ A x &amp;= b \end{aligned}\) où $\mathcal A \in M_{p,n}(\mathbb R); b \in M_{p,1}(\mathbb R)$</p><p>On dit qu’un point $x$ est admissible s’il satisfait les contraintes définies par $(P)$. Le lieu admissible $\mathcal A$ de $(P)$ correspond aux points admissibles de $(P)$. On fait remarquer que sous nos hypotheses, $\mathcal A$ est convexe. On note $p^{*}$ la valeur optimale de $(P)$: \(p^{*} = \underset{x \in \mathcal A}{inf}\{f_0(x)\}\)</p><p>Par convention si $\mathcal A = \emptyset; p^{*} = +\infty$. Dans le cas sur $p^{*} = - \infty$ on dit que ($P$) est non borné. On appelle enfin point optimal $x^{*}$ de $(P)$ tout point tq $f_0(x^{*}) = p^{*}$. Un tel point n’existe pas toujours; par exemple c’est le cas $\underset{x \in \mathbb{R}_+^{*}}{min} \frac{1}{x}$. De plus, il n’existe pas en général qu’un seul point optimal (quand il y en a); prendre par exemple le problème: \(\underset{x \in \mathbb{R}}{min} 10\)</p><p>L’écriture de ($P$) dans la définition est appelée standard d’un problème d’optimisation. Il existe une notion théorique d’équivalence de problème d’optimisation, On ne rentrera pas dans le détail, sachez qu’elle consiste à réexprimer un problème d’optimisation de façon à le résoudre plus facilement.</p><blockquote><p><strong>Exemple :</strong> $\min |x|$ sujet à: \(\begin{aligned} x - 2 &amp;\leqslant 0 \qquad (P_1)\\ -x -2 &amp;\leqslant 0 \end{aligned}\) $P_1$ est équivalent à:</p><p>$\min -x^2$ sujet à: \(\begin{aligned} x - 2 &amp;\leqslant 0\\ -x -2 &amp;\leqslant 0 \end{aligned}\)</p></blockquote><p><strong>Pourquoi la convexité ?</strong></p><h4 id="unicité-du-minimum">unicité du minimum<a href="#unicité-du-minimum"><i class="fas fa-hashtag"></i></a></h4></h4><p>Soit $f: \mathbb R^n \longrightarrow \mathbb R$ une fonction convexe, alors $f$ :</p><ul><li>n’admet pas de maximum locaux strictes.<li>Admet au plus un minimum local stricte.</ul><p>Essayons de justifier le premier point. Supposons qu’il existe un voisinage $\mathcal{B}(x, \varepsilon)$ pour $\varepsilon &gt;0$ tq : \(\forall y \in \mathcal{B}(x, \varepsilon), y \neq x f(y) &lt; f(x)\)</p><p>Soient $y_1,y_2 \in \mathcal{B}(x, \varepsilon) \backslash {x}$ $\forall t \in [0,1]$ (conv):$f(ty_1 + (1-t)y_2) \leqslant tf(y_1) + (1-t) f(y_2)$ Donc $tf(y_1) + (1 -t)f(y_2) \leqslant f(x)$ car $f(y_1) \leqslant f(x)$ et $f(y_2) \leqslant f(x)$</p><p>La condition précédente exprime le fait que la sécante au graphe de f sur $\mathcal{B}(x, \epsilon)$ est en dessous de celui ci, donc pas dans l’épigraphe de f. Dans ce cas f n’est pas convexe.</p><p>Pour le second point:</p><p>si $y_1, y_2$ sont 2 minimaux locaux et différents, on retrouve la situation qui contredit la convexité.</p><h4 id="condition-dexistance-dun-minimum-sous-contraintes">Condition d’existance d’un minimum sous contraintes<a href="#condition-dexistance-dun-minimum-sous-contraintes"><i class="fas fa-hashtag"></i></a></h4></h4><p>Si on est dans la situation suivante</p><p><img data-proofer-ignore data-src="https://i.imgur.com/Vysz8sR.jpg" alt="" /></p><div class="alert alert-success" role="alert"><p><strong>Propriété :</strong> Un point $x \in \mathcal A$ est optimal si:</p>\[\nabla f(x^{*} )^\top(y.x) \geqslant 0\]<blockquote><p>$-\nabla f_0(x^{*})^\top(y-x) \leqslant 0$ $-\nabla f_0(x^{*})$ définit un hyperplan d’appui en $x^{*}$ à $\mathcal A$.</p></blockquote></div><blockquote><p><strong>Preuve :</strong> Supposons $x^{*}$ satisfait $(op)$. D’après les inégalités de convexité sur $f_0$ on a: \(\forall y \in \mathcal A; \nabla f_0(x^{*})^\top(y-x^{*}) \leqslant f(y) - f(x)\) D’après $(op)$: \(f(y) - f(x^{*}) \geqslant 0 \Leftrightarrow f(y) \geqslant f(x^{*})\)</p></blockquote><p>La réciproque se fait par contraposition. On la laisse de côté pour cette fois.</p><p>Est-ce que l’hypothèse de convexité de $(P)$ sur tout son domaine de définition est important ?</p><blockquote><p>$\rightarrow$ Matheux dans sa tête : OUI $\rightarrow$ Informaticien (matheux qui fait calculer) : BAH … CON vexe ?</p></blockquote><h3 id="cas-sans-contrainte">Cas sans contrainte<a href="#cas-sans-contrainte"><i class="fas fa-hashtag"></i></a></h3></h3><p>On s’intéresse en un premier temps au problème d’optimisation de la forme: \(\underset{x \in \mathbb R^n}{\min} f_0(x)\) avec $f_0$ différentiable</p><div class="alert alert-info" role="alert"><p><strong>Propriété :</strong> Si $x^{*}$ est un point optimal de $f_0$ alors: \(\nabla f_0(x^{*}) = \underline{0}\)</p></div><blockquote><p><strong>Preuve :</strong> On se place sur un voisinage $\mathcal B(x^{*}, \varepsilon)$ pour $\varepsilon \gt 0$ où $\forall y \in \mathcal B(x^{*}, \varepsilon); f_0(y) \geqslant f_0 (x^{*})$</p><p>En particulier pour le h assez proche de 0:</p><p>\(\begin{aligned} &amp;f_0(x^{*} + h) -f_0(x^{*}) &amp;\geqslant 0\\ \Rightarrow &amp;\nabla f_0(x^{*})^T h + \theta_0 &amp;\geqslant 0 \end{aligned}\) Ainsi $\forall h \in \mathcal{B}(\underline{0}, \eta)$ pour $\eta &gt; 0$</p><p>\(\nabla f_0(x^{*})^T \geqslant 0\) La seule application linéaire qui est possible sur un voisinage $\mathcal B(\underline{0}, \eta)$ est l’application nulle.</p></blockquote><p>Dans le cas $f_0$ convexe, l’annulation du gradient en un point va nous limiter à un sous-lieu de points optimaux à étudier. En réalité, on a en général la situation suivante:</p><blockquote><p>Les points critiques d’une fonction $f_0$ quelconque sont de l’une des trois formes suivantes:</p><ol><li>Minimum locaux.<li>Maximums locaux.<li>Points selles.</ol></blockquote><p>Dans le cas convexe on a que des points du premier type. Dans ce cas l’étude des points critiques se confond avec celle des points minimaux.</p><h3 id="problème-du-dual">Problème du dual<a href="#problème-du-dual"><i class="fas fa-hashtag"></i></a></h3></h3><p>Soit P le problème d’optimisation:</p><blockquote><p>\(\min f_0(x)\) sujet à \(f_i(x) \leqslant 0\\ h_j(x) = 0\)</p></blockquote><p>Si on voulait ramener l’étude de $(P)$ à la minimisaion d’une seule fonction on pourrait étudier: \(\Phi(x) = f_0(x) + \sum_{i=1}^n I_+(f_i(x)) + \sum_{j=0}^pI_0(f_j(x))\)</p><p>où \(\begin{aligned} I_+ (x) &amp;= \begin{cases}0 \text{ si } x \leqslant 0\\ +\infty \text{ sinon}\end{cases}\\ I_0 (x) &amp;= \begin{cases}0 \text{ si } x = 0\\ +\infty \text{ sinon}\end{cases} \end{aligned}\)</p><p>Problème d’optimisation équivalent à $(P)$ mais inutilisable</p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> On appelle Lagrangien du problème ($P$) la fonction:</p><p>\(\mathcal L_P (\underset{\in \mathbb R^n}{x}, \underset{\in \mathbb R^m}{\lambda},\underset{\in \mathbb R^p}{\nu}) = f_0(x) + \sum_{i=1}^n \lambda_i f_i(x) + \sum_{j=0}^p \nu_j f_j(x)\)</p></div><p>On définit le problème dual $(\check{P})$ de $(P)$ comme suit: on note:</p>\[g(\lambda, \nu ) = \underset{x \in \mathbb R^n}{inf} \mathcal L(x,\lambda, \nu)\]<p>avec cette notation:</p>\[\underset{\lambda, \nu}{max} g(\lambda, \nu) \qquad (\check P)\\ \text{sujet à} \quad \lambda \geqslant 0\]<p>Remarque: $g(\lambda, \nu)$ pour $\lambda \geqslant 0$ est l’$inf$ d’une fonction concave. (affines en les $\lambda$ et $\nu$) c’est donc concave (exo bribes de géometries) Donc $(\check{P})$ est toujours un problème convexe.</p><div class="alert alert-success" role="alert"><p><strong>Propriété :</strong> \(\forall \lambda \geqslant 0 \text{; on a : } g(\lambda, \nu) \leqslant p^{*}\)</p></div><blockquote><p><strong>Preuve :</strong> Soit $x$ un point admissible de ($P$). on a donc $f_i \leqslant 0$ et $h_j(x) = 0$. Donc \(\sum_{i= 1}^m d_if_i(x) + \sum_{j = 1}^P \nu_jh_j(x) \leqslant 0 \: \forall \qquad \lambda \geqslant 0\) D’où: \(\begin{aligned} &amp;\mathcal L_p(x, \lambda_1 \nu) &amp;=&amp; f_0(x) + \sum_{i=1}^n \lambda_i f_i(x) + \sum_{j=0}^p \nu_j f_j(x) &amp;\leqslant&amp; f_0(x)\\ \Rightarrow &amp;\underset{x}{inf} \mathcal{L}(x, \lambda, \nu) &amp;=&amp; g(\lambda, \nu) &amp;\leqslant&amp; f_0(x)\\ \Rightarrow &amp;g(\lambda, \nu) &amp;\leqslant&amp; p^{*} \end{aligned}\)</p></blockquote><p><strong>Corollaire :</strong> Si on note $d^{*}$ la valeur optimale du dual on a : $d^{*} \leqslant p^{*}$</p><p><strong>Question :</strong> Est ce qu’on a l’égalité ? Dans la situation d’égalité on dit qu’on a une dualité forte entre (P) et $(\check{P})$</p><p><strong>Condition de Slater :</strong> Si $(P)$ est convexe et il existe un pt dans l’intèrieur relatif du domaine de définition de $(P)$ tq : $f_i(x)&lt;0$ $A x=b$ alors $(P)$ et $(\check{P})$ sont en dualité forte:</p><p><img data-proofer-ignore data-src="https://i.imgur.com/jtN4O7T.jpg" alt="" /></p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> On dit qu’un couple $(\lambda, \nu)$ est de $t$ dual admissible si $\lambda \geqslant 0$ et $g(\lambda, \nu) \gt -\infty$. Les points $(\lambda^{*},\nu^{*})$ optimaux pour $\check{\mathcal P}$ sont parfois appelés multiplicateurs de Lagrange.</p></div><h3 id="les-conditions-kkt-karush-kuhn-tucker">Les conditions KKT <em>(Karush-Kuhn-Tucker)</em><a href="#les-conditions-kkt-karush-kuhn-tucker"><i class="fas fa-hashtag"></i></a></h3></h3><p>Supposons que les valeurs optimales, primale et duale, soient atteintes et egales, en particulier on a une dualite forte. On designe par $x^{*}$ (respectivement $(\lambda^{*},\nu^{*})$) un point optimal de $\mathcal P$ (respectivement $\check{\mathcal P}$)</p><p>On a : \(\begin{aligned} f_0(x^{*})&amp;=g(\lambda^{*}, \nu^{*})\\ &amp;=\inf(\mathcal L_p (x, \lambda^{*}, \nu^{*}))\\ &amp;\leq \mathcal L_p(x^{*}, \lambda^{*}, \nu^{*})\\ &amp;=f_0(x^{*}) + \sum_{i=1}^m \lambda_i^{*} f_i(x^{*}) + \sum_{j=1}^p \nu_j^{*} h_j(x^{*})\\ &amp;\leq f_0(x^{*}) \end{aligned}\)</p><p>Toutes les inégalités qui apparaissent précédemment sont donc des égalités. On en déduit : 1) $x^{*}$ minimise $\mathcal L_p(x, \lambda^{*},\nu^{*})$ 2) $\displaystyle \sum_{i=1}^m \underbrace{ \lambda_i^{*} f_i(x^{*})}_{\le 0} = 0$ $\Rightarrow \forall i \in {1,…,m}; \lambda_i^{*}f_i(x^{*}) = 0$</p><p>La fonction $x \longmapsto \mathcal L_p(x, \lambda^{*}, \nu^{*})$ est convexe des que $(P)$ l’est. Dire que $x^{*}$ minimise $x \longmapsto \mathcal L_p(x, \lambda^{*}, \nu^{*})$ est equivalent a dire que</p><p>$\nabla_x \mathcal L_p (x^{*},\lambda^{*},\nu^{*}) = 0$ $\Leftrightarrow \nabla f_0(x^{*}) + \displaystyle \sum_{i=1}^m \lambda_i^{*} \nabla f_i(x^{*}) + \displaystyle \sum_{j = 1}^{p} \nabla j^{*} \nabla hj(x^{*}) = 0$</p><p>Pour resumer $(x^{*},\lambda^{*}, \nu^{*})$ verifient les contraintes : $f_i(x^{*}) \leqslant 0 \qquad \forall i \in {1,…,m}$ $h_j(x^{*}) = 0 \qquad \forall j \in {1,…,p}$ $\lambda_i^{*} \geqslant 0 \qquad \forall i \in {1,…,m}$ (KKT) $\lambda_i^{*} f_i(x^{*}) = 0 \qquad \forall i \in {1,…,m}$</p><p>$\nabla f_0(x^{*}) + \displaystyle \sum_{i =1}^m \lambda_i^{*} \nabla f_i(x^{*}) + \displaystyle \sum_{j = 1}^{p} \nu_j^{*} \nabla h_j(x^{*}) = 0$</p><div class="alert alert-success" role="alert"><p>Propriété : Quand $(P)$ est un problème convexe et dans le cas de forte dualité, (condition de Slater satisfaite, par exemple) les conditions KKT sont nécessaires et suffisantes pour avoir une pair primal-dual optimale.</p></div><p><strong>Exercices :</strong> Résoudre en utilisant les conditions KKT</p><p><em>1</em> \(min_{x \in \mathbb R^2} \quad \frac{1}{2}({x_1}^2 + {x_2}^2) \qquad tq \quad x_1 - 2x_2 \leqslant -2\)</p><blockquote><p><strong>Correction :</strong> \(f_0(x_1, x_2) = \frac{1}{2}(x_1^2 + x_2^2)\) \(\begin{aligned} \mathcal L(x_1,x_2, \lambda) &amp;= f_0(x_1, x_2) + \lambda f_1(x_1, x_2)\\ &amp;= \frac{1}{2}\left(x_1^2 + x_2^2\right) + \lambda (x_1 - 2x_2 + 2) \end{aligned}\)</p><p>Pour que: $(x_1^{*}, x_2^{*})$ soit optimal, il faut que: \(\begin{aligned} &amp;\nabla_x \mathcal L(x^{*}, \lambda) = 0 \\ &amp;\nabla_x \mathcal L(x, \lambda) = 0 \Leftrightarrow \begin{cases} \frac{\partial \mathcal L}{\partial x_1} = 0 \\ \frac{\partial \mathcal L}{\partial x_2} = 0 \end{cases} \\ &amp;\begin{cases} \frac{\partial \mathcal L}{\partial x_1} = x_1 + \lambda = 0\\ \frac{\partial \mathcal L}{\partial x_1} = x_2 - 2\lambda = 0 \end{cases} \Leftrightarrow \begin{cases} x_1 = -\lambda\\ x_2 = 2 \lambda \end{cases} \end{aligned}\) La fonction objective duale est: \(\begin{aligned} g(\lambda, \nu) &amp;= \underset{x \in \mathbb R^n}{inf} \mathcal L(x, \lambda, \nu)\\ g(\lambda) &amp;= \frac{1}{2}((-\lambda)^2 + (2\lambda)^2) + \lambda(-\lambda - 4 \lambda + 2)\\ &amp;= \frac{1}{2}(\lambda^2 + 4\lambda^2) - \lambda^2 - 4\lambda^2 + 2\lambda\\ &amp;= \frac{5}{2}\lambda^2 -5\lambda^2 + 2\lambda\\ &amp;= -\frac{5}{2}\lambda^2 + 2\lambda \end{aligned}\) <strong>Problème dual $p^2$</strong> \(\underset{\lambda \geqslant 0}{max} \qquad g(\lambda, \nu)\) On cherche $\underset{\lambda \geqslant 0}{max}(\underbrace{-\frac{5}{2}\lambda^2 + 2\lambda}_{g(\lambda)})$ On cherche \(\lambda^{*} \quad tq \quad \begin{cases} \nabla g(\lambda^{*}) &amp;= 0\\ \lambda^{*} &amp;\geqslant 0 \end{cases}\) 2) \(\begin{aligned} &amp;\nabla g(\lambda) &amp;=&amp; -5\lambda + 2\\ &amp;\nabla g(\lambda^{*}) &amp;=&amp; 0 \\ \Leftrightarrow&amp; -5\lambda^{*} + 2 &amp;=&amp; 0\\ \Leftrightarrow&amp; \lambda^{*} &amp;=&amp; \frac{2}{5} \geqslant 0 \end{aligned}\) et $\displaystyle x^{<em>} = (x_1^{</em>}, x_2^{*}) = \left(-\frac{2}{5}, \frac{4}{5}\right)$</p></blockquote><p><em>2 … Apres avoir mis sous forme matricielle</em> \(min_{x \in \mathbb R^3} \quad \frac{1}{2}(x_1^2 + x_2^2 + x_3^2) \qquad tq \quad \begin{aligned} x_1 + x_2 + 2x_3 = 1 \\ x_1 + 4x_2 + 2x_3 = 3 \end{aligned}\)</p><blockquote><p><strong>Correction :</strong> //FIXME</p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s8/'>Image S8</a>, <a href='/cours/categories/ocvx/'>OCVX</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/scia/" class="post-tag no-text-decoration" >SCIA</a> <a href="/cours/tags/ocvx/" class="post-tag no-text-decoration" >OCVX</a> <a href="/cours/tags/s8/" class="post-tag no-text-decoration" >S8</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=OCVX: Optimisation Convexe 1, suite - Cours&url=https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=OCVX: Optimisation Convexe 1, suite - Cours&u=https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=OCVX: Optimisation Convexe 1, suite - Cours&url=https://lemasyma.github.io/cours/posts/ocvx_kariulele_1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/ocvx_hyperplan/"><div class="card-body"> <em class="timeago small" date="2021-05-07 10:00:00 +0200" >May 7, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Hyperplan d'appui</h3><div class="text-muted small"><p> Lien de la note Hackmd Rappels Hyperplan d’appui a une partie $A$ de $\mathbb R^n$ en un point $p\in A$, est un hyperplan affine de $\mathbb R^n$ qui laisse $A$ dans un des deux demi-espaces de...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_espaces-tangents/"><div class="card-body"> <em class="timeago small" date="2021-05-11 09:00:00 +0200" >May 11, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Espaces tangents</h3><div class="text-muted small"><p> Lien de la note Hackmd Dedramatiser les espaces tangents Pour une fonction \(\begin{aligned}f:\mathbb R&amp;amp;\to\mathbb R \\ x&amp;amp;\mapsto f(x)\end{aligned}\) \[Gr(f) = \{(x,f(x)), x\in\mathbb ...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_introduction/"><div class="card-body"> <em class="timeago small" date="2021-03-11 13:00:00 +0100" >Mar 11, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Introduction</h3><div class="text-muted small"><p> Lien de la note Hackmd Introduction L’optimisation fait partie des missions historiques de l’ingénierie. Elle naît avec l’ère industrielle: une fois un concept élaboré il s’agit de réduire les cou...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/ocvx_verjus_1/" class="btn btn-outline-primary" prompt="Older"><p>OCVX: Optimisation Convexe 1</p></a> <a href="/cours/posts/ocvx_kariulele_2/" class="btn btn-outline-primary" prompt="Newer"><p>OCVX: Optimisation Convexe 2</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
