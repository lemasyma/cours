<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="OCVX: Optimisation Convexe 1" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Optimisation Convexe 1" /><meta property="og:description" content="Optimisation Convexe 1" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/ocvx_verjus_1/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/ocvx_verjus_1/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-03-22T21:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="OCVX: Optimisation Convexe 1" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-03T16:45:54+02:00","datePublished":"2021-03-22T21:00:00+01:00","description":"Optimisation Convexe 1","headline":"OCVX: Optimisation Convexe 1","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/ocvx_verjus_1/"},"url":"https://lemasyma.github.io/cours/posts/ocvx_verjus_1/"}</script><title>OCVX: Optimisation Convexe 1 | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>OCVX: Optimisation Convexe 1</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>OCVX: Optimisation Convexe 1</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-03-22 21:00:00 +0100" data-toggle="tooltip" data-placement="bottom" title="Mon, Mar 22, 2021, 9:00 PM +0100" >Mar 22, 2021</em> </span> <span> Updated <em class="timeago" date="2021-10-03 16:45:54 +0200 " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 3, 2021, 4:45 PM +0200" >Oct 3, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5295 words"> <em>29 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Notes de ce cours par <a href="https://github.com/kariulele">Kariulele</a> et Bjorn (Un grand merci a eux!)</p><p><a href="https://github.com/bashardudin/Epita-SCIA-OCVX">Bashar’s Github</a></p><div class="alert alert-info" role="alert"><p>bashar.dudin@epita.fr guillaume.tochon@lrde.epita.fr</p></div><p>À voir :</p><ul><li><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">Descente de gradient</a></ul><h1 id="introduction">Introduction</h1><p>En rentrant dans l’ère industrielle, il a fallu optimiser les coûts, minimiser les risques, etc. Des mathematiciens ont commencé à se poser des questions. ex: gestion d’un stock, optimiser la construction de produits en usine.</p><ul><li>Algebre linéaire<li>Calcul différentiel<li>Géometrie</ul><blockquote><p>Examples:</p><ol><li>Chercher le plus cours chemnin entre deux coordonnées GPS.<li>Décider des meilleurs routes aériennes qui minimisent le prix d’approvisionnement en kérosène.<li>Identifier des images d’IRM qui correspondent à des malformations du cerveau.<li>Chercher des patterns dans la population d’étudiants intégrants EPITA.</ol></blockquote><h2 id="problèmes-doptimisation">Problèmes d’optimisation<a href="#problèmes-doptimisation"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="définition-formelle">Définition formelle<a href="#définition-formelle"><i class="fas fa-hashtag"></i></a></h3></h3><p>Minimiser $f_0(x)$ sujet à :</p><ul><li>$f_i(x) \leq 0, \forall i \in {1,\dots,p}$<li>$h_j(x) \leq 0, \forall j \in {1,\dots,m}$</ul><p>où $f_0$, les $f_i$ et les $h_i$ sont des applications de $\mathbb{R}^n$ vers $\mathbb{R}$. La fonction $f_0$ est dite <strong>fonction objectif</strong>; suivant le contexte ce sera une fonction de <strong>coût</strong> ou d’<strong>erreur</strong>.</p><p>Un problème d’optimisation du type de $(P)$ :</p><ul><li><strong>différentiable</strong> si toutes les fonctions en jeu le sont.<li><strong>non-contraint</strong> s’il n’a aucune contrainte d’inégalités ou d’égalités.<li><strong>convexe</strong> si l’ensemble des fonctions en jeu sont convexes, les contraintes d’égalités étant de plus affines.</ul><h3 id="lexique">Lexique<a href="#lexique"><i class="fas fa-hashtag"></i></a></h3></h3><p>Étant donnée un problème d’optimisation $(P)$ on appelle :</p><ul><li><strong>point admissible</strong> de $(P)$ tout point de $\mathbb{R}^n$ satisfaisant toutes les contraintes. L’ensemble de tous les points admissibles est appelé <strong>lieu admissible</strong> de $(P)$.<li><strong>valeur objectif</strong> d’un point admissible la valeur que prend la fonction objectif en celui-ci.<li><strong>valeur optimale</strong> de $(P)$ la meilleure borne inférieure sur la fonction objectif.<li><strong>point optimale</strong> de $(P)$ tout point admissible dont la valeur objectif est la valeur optimale.</ul><h3 id="premières-remarques-qualitatives">Premières remarques qualitatives<a href="#premières-remarques-qualitatives"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>Y a-t-il au moins une solution ?<li>S’il y a au moins une solution, combien ?<li>Peut-on toujours décrire l’ensemble des solutions?<li>Y a-t-il moyen d’approcher des solutions?</ul><h2 id="ml">ML<a href="#ml"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="map-fitting">Map fitting<a href="#map-fitting"><i class="fas fa-hashtag"></i></a></h3></h3><p>Problème d’optimisation dit de <em>map fitting</em></p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Une famille différentiable d’applications $f_\alpha:\mathbb{R}^n\longmapsto\mathbb{R}$ indéxeś par $\alpha \in \mathbb{R}^k$ est une famille de fonctions pour laquelle l’application $\varphi:\mathbb{R}^k\times\mathbb{R}^n\longmapsto\mathbb{R}$ qui envoie $(\alpha,x)$ sur $f_\alpha(x)$ est différentiable.</p></div><div class="alert alert-info" role="alert"><p><strong>Map Fitting :</strong> On considère un ensemble de couples $(X_i, y_i)\in\mathbb{R}^n\times\mathbb{R}$ pour $i \in { 1,…,p }$ et une famille différentiable d’applications \(\{ f_\alpha \}_{\alpha\in\mathbb{R}^k}\). Le problème de <em>map fitting</em> relatif aux données précédentes consiste à trouver les meilleurs paramètres $\alpha^{*}$ tels que $f_{\alpha^{*}}$ approche au mieux les $(X_i, y_i)$.</p></div><h3 id="régression-linéaire">Régression linéaire<a href="#régression-linéaire"><i class="fas fa-hashtag"></i></a></h3></h3><p>Le plus simple des problèmes de <em>map fitting</em> est celui de la régression linéaire.</p><ul><li>La famille différentiable à laquelle on s’intéresse est indexés par $\mathbb{R}^2$: $f_\alpha(x)=\alpha_1x+\alpha_0$ pour $\alpha=(\alpha_0,\alpha_1)$<li>La métrique standard utilisée est le <strong>MSE</strong> (Mean Square Error) donnée pour un $f_\alpha$ par \(\mathcal{E}(\alpha)=\sum_{i=1}^p\frac{1}{p}(f_\alpha (X_j)-y_i)^2\)</ul><p>Le but est de trouver un paramètre $\alpha = (\alpha_0,\alpha_1)$ tel que $\mathscr{E}(\alpha)$ est minimal, autrement dit de réoudre le problème d’optimisation sans contraintes</p><h2 id="contour-du-cours">Contour du cours<a href="#contour-du-cours"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>La première partie est noté par un TD et un partiel<li>La seconde partie est noté par une analyse à faire (projet?)</ul><h2 id="classification">Classification<a href="#classification"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Comment séparer la classe1 de la classe2 ?</p><p><img data-proofer-ignore data-src="https://www.researchgate.net/profile/Ammar_Chouchane/publication/303899174/figure/fig18/AS:614128192323590@1523430979027/Classification-SVM-a-SVM-lineaire-separation-par-une-ligne-droite-b-SVM-non.png" alt="Classification Schéma" /></p><p>On fait un trait…</p></blockquote><h2 id="produit-scalaire">Produit scalaire<a href="#produit-scalaire"><i class="fas fa-hashtag"></i></a></h2></h2><p>$x = \begin{pmatrix}x_1 \ \vdots \ x_n\end{pmatrix} \qquad y = \begin{pmatrix}y_1 \ \vdots \ y_n\end{pmatrix}$</p>\[\begin{aligned} \langle:\rangle : \mathbb{R}^n &amp;\longrightarrow \mathbb{R}\\ (x,y) &amp;\longmapsto \langle x,y \rangle \end{aligned}\]<p>$\langle x,y \rangle =\displaystyle\sum_{i=1}^{n}x_iy_i$</p><p>$\Vert x\Vert =\sqrt{\langle x,x \rangle}$</p><p>On veut :</p><ul><li>$x_1, x_2$ en fonction de $\Vert x\Vert $ et $\varphi$<li><p>$y_1, y_2$ en fonction de $\Vert y\Vert $ et $\psi$</p><li>$x_1 = \Vert x\Vert \cos{\varphi} \qquad x_2 = \Vert x\Vert \sin{\varphi}$<li><p>$y_1 = \Vert y\Vert \cos{\psi} \qquad y_2 = \Vert y\Vert \sin{\psi}$</p><li>$\langle \vec{x}, \vec{y}\rangle = x_1 y_1 + x_2 y_2 = \Vert x\Vert .\Vert y\Vert .(\underbrace{\cos{\varphi}\cos{\psi} + \sin{\varphi}\sin{\psi}}_{\cos{(\psi - \varphi)} = \cos{\theta}}) =\Vert x\Vert .\Vert y\Vert .\cos{\theta}$</ul><div class="alert alert-info" role="alert"><p><strong>En dimension n :</strong> \(\theta(x,y)=arccos\left(\frac{\langle x,y \rangle}{\Vert x\Vert .\Vert y\Vert }\right)\)</p></div><div class="alert alert-info" role="alert"><p><strong>Formules usuelles trigonometriques :</strong> \(\sin \left(s+t\right)=\sin \left(s\right)\cos \left(t\right)+\cos \left(s\right)\sin \left(t\right)\\ \sin \left(s-t\right)=\sin \left(s\right)\cos \left(t\right)-\cos \left(s\right)\sin \left(t\right)\\ \cos \left(s+t\right)=\cos \left(s\right)\cos \left(t\right)-\sin \left(s\right)\sin \left(t\right)\\ \cos \left(s-t\right)=\cos \left(s\right)\cos \left(t\right)+\sin \left(s\right)\sin \left(t\right)\\ \cos \left(s\right)\cos \left(t\right)=\frac{\cos \left(s-t\right)+\cos \left(s+t\right)}{2}\\ \sin \left(s\right)\sin \left(t\right)=\frac{\cos \left(s-t\right)-\cos \left(s+t\right)}{2}\\ \sin \left(s\right)\cos \left(t\right)=\frac{\sin \left(s+t\right)+\sin \left(s-t\right)}{2}\\ \cos \left(s\right)\sin \left(t\right)=\frac{\sin \left(s+t\right)-\sin \left(s-t\right)}{2}\\\)</p></div><p>On peut représenter une droite avec :</p><ul><li>2 points<li>1 point et un vecteur directeur ou normal.</ul><p>$x=\begin{pmatrix} x_1 \ x_2 \end{pmatrix} \in D \Leftrightarrow \langle \vec{Ox},\vec{n} \rangle = 0 \Leftrightarrow \underbrace{x^{\top}n=0}_{\langle x,n \rangle}$ $\rightarrow$ Equation d’un hyperplan de vecteur normal $\vec{n}$</p><p>Soit une droite $ax_1 +bx_2 + c = 0$ :</p><ul><li>Son <strong>vecteur normal</strong> : $\vec{n} = \begin{pmatrix}a\ b\end{pmatrix}$<li>Son <strong>vecteur directeur</strong> : $\vec{u} = \begin{pmatrix}-b\ a\end{pmatrix}$</ul><blockquote><p><strong>Exercice :</strong> Dessiner le lieu de $\mathbb{R}^2$ donné par la relation \(\begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 1 \end{pmatrix} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} \leq \begin{pmatrix}0\\0\end{pmatrix}\)</p><p>$x_1 + 2x_2 \leq 0$ $-x_1 + x_2 \leq 0$</p><p>On remplace l’inégalité par une égalité : $x_1 + 2x_2 = 0 \qquad \vec{n_1} = \begin{pmatrix}1\ 2\end{pmatrix} \qquad \vec{u_1} = \begin{pmatrix}-2\ 1\end{pmatrix}$ $-x_1 + x_2 = 0 \qquad \vec{n_2} = \begin{pmatrix}-1\ 1\end{pmatrix} \qquad \vec{u_2} = \begin{pmatrix}-1\ -1\end{pmatrix}$</p><p>On a les vecteurs normaux on peut donc représenter graphiquement le lieu. <img data-proofer-ignore data-src="https://i.imgur.com/M8Y0xkz.png" alt="" /></p></blockquote><blockquote><p><strong>Exercice :</strong> Trouver le lieu de $\mathbb{R}^3$ tq $\underbrace{x_1 + x_2 + x_3}_{(1 1 1)\begin{pmatrix}x1\ x_2\ x_3\end{pmatrix}} \geq 0$</p><p>$\vec{n} = \begin{pmatrix}1\ 1\ 1\end{pmatrix}$ $\langle n, x \rangle \geq 0$</p><p><img data-proofer-ignore data-src="https://i.imgur.com/kcXCXJE.png" alt="" /></p></blockquote><h2 id="espace-affine">Espace affine<a href="#espace-affine"><i class="fas fa-hashtag"></i></a></h2></h2><p>$A = {(1,t) \in \mathbb{R}^2 \vert t \in \mathbb{R}} = (1, 0) + \underbrace{{(0,t) \in \mathbb{R}^2 \vert t \in \mathbb{R}}}_{F}$</p><div class="alert alert-info" role="alert"><p>On note qu’on pouvait utiliser un autre point $(1,42)$ au lieu de $(1,0)$ donne le même résultat.</p></div><p>$P={(t, 3t + u, -u) \setminus (t, u) \in \mathbb{R}^2}$</p><ul><li>$O \in P$<li>$A = \begin{pmatrix}1\ 3\ 0\end{pmatrix} \in P$<li>$B = \begin{pmatrix}0\ 1\ -1\end{pmatrix} \in P$ $\vec{n} = \vec{OA} \times \vec{OB}$</ul><div class="alert alert-info" role="alert"><p><strong>Produit vectoriel :</strong> <a href="https://fr.wikiversity.org/wiki/Produit_vectoriel/Avanc%C3%A9">Wikipédia</a></p></div><blockquote><p><strong>Exercice :</strong> Ecrire <strong>paramétriquement</strong> la droite $D$ de $\mathbb{R}^2$ de vecteur directeur $\vec{u}=\begin{pmatrix}1\ -1\end{pmatrix}$ et passant par $(2,3)$</p><p>Soit $M \in D \Leftrightarrow \exists ~\alpha \in \mathbb{R}$ tq \(\vec{AM} = \alpha \vec{u} \\ \Leftrightarrow \begin{pmatrix}x_1 - 2\\ x_2 - 3\end{pmatrix} = \alpha \begin{pmatrix}1\\ -1\end{pmatrix}\\ \Leftrightarrow \begin{cases} x_1 -2 = \alpha \\ x_2 - 3 = -\alpha\end{cases} \\ \Leftrightarrow \begin{cases}x_1 = \alpha + 2\\ x_2 = 3 - \alpha\end{cases}\)</p><p>Donc $(D) = {(\alpha +2, 3-\alpha) \vert \alpha \in \mathbb{R}}$</p></blockquote><blockquote><p><strong>Exercice :</strong> Dessiner le lien de $\mathbb{R}^2$ decrit par les contraintes : $\begin{pmatrix}-1 &amp; 2 <br /> 1 &amp; 1\ \end{pmatrix} = \begin{pmatrix}x \ y \end{pmatrix} \le \begin{pmatrix} -1 \ 1\end{pmatrix}$ $ax + by = 0$ $ax + by +c = 0$ $\overrightarrow{n} \begin{pmatrix}a \ b \end{pmatrix} \overrightarrow{u} \begin{pmatrix}-b \ a \end{pmatrix}$ ${ (x,y) \in \mathbb{R}^2 , -x + 2y \le -1 \text{ et } x+y \le 1 }$ $(D_1) = -x + 2y + 1 = 0$ $(0, \frac{-1}{2} \in D_1)$ $\overrightarrow{n_1} = \begin{pmatrix} -1 \ 2\end{pmatrix}$ $\overrightarrow{u_1} = \begin{pmatrix} -2 \ -1\end{pmatrix}$</p></blockquote><hr /> \[\underbrace{Ax = r}_{\text{écriture implicite}} \qquad \text{ avec } \begin{cases}\text{A matrice } m \times n\\A = [a_{i,j}]_{mn}\\x \in \mathbb R^n ~ r \in \mathbb R^m \end{cases}\] \[\begin{cases} a_{11} x_1 + a_{12} x_2 + \dots + a_{1n}xn = r_1\\ \vdots \\ a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = r_m \end{cases}\]<div class="alert alert-info" role="alert"><p><strong>Hyperplan</strong>: Un plan de dimension $n-1$. Exemples: En 2D, c’est une droite. En 3D, c’est un plan…</p></div><p>Notre description affine ne suffit pas dans le cas général.</p><div class="alert alert-info" role="alert"><p><strong>Description d’un cercle :</strong> $x^2 + y^2 = r^2<br /> x^2+y^2-r^2=0$ $\boxed{f(x,y) = x^2 + y^2 - r^2}$</p></div><div class="alert alert-info" role="alert"><p><strong>Ligne / Courbe de niveau d’une fonction f :</strong> \(\mathscr{C}_r=\{x\in \mathbb{R}^n | f(x)=r\}\)</p><p><strong>Lieu de sous niveau d’une fonction f :</strong> \(\mathscr{C}_{\leqslant r}=\{ x\ \in \mathbb{R}^n \ f(x) \leqslant r \}\)</p></div><div class="alert alert-info" role="alert"><p>Les <strong>coniques :</strong> Ce sont les paraboles, hyperboles, elipses…</p></div><blockquote><p><strong>Exercice :</strong> Courbes de niveau 0,1,2</p><ol><li>$f(x,y)=x^2+y^2$<blockquote><p>$\mathscr{C}_0$: seul $(0,0)$ $\mathscr{C}_1$: cercle de rayon 1 centré en 0 $\mathscr{C}_2$: cercle de rayon 2 centré en 0</p><ol><li>$g(x,y)=x^2+4y^2$ $\mathscr{C}_0$: seul $(0,0)$ <br /> $\mathscr{C}_1$: ellipse demi-grand axe 1, demi-petit axe $\frac{1}{2}$, centré en 0 $\mathscr{C}_2$: ellipse demi-grand axe $\sqrt{2}$, demi-petit axe $\frac{\sqrt{2}}{2}$, centré en 0</ol></blockquote></ol><p><img data-proofer-ignore data-src="https://i.imgur.com/pJCjNKt.gif" alt="schema cercle" /></p></blockquote><div class="alert alert-info" role="alert"><p><strong>Équation d’une éllipse</strong> \(\bigg(\frac xa\bigg)^2 + \bigg(\frac yb\bigg)^2=1\) <img data-proofer-ignore data-src="https://i.imgur.com/4I9Oq2K.gif" alt="" /> <strong>a :</strong> demi-grand axe <strong>b :</strong> demi-petit axe</p></div><div class="alert alert-info" role="alert"><p>Epigraphe d’une fonction $f.\mathbb R^n \rightarrow \mathbb R$</p><p>$Epi(f) = {(x,t) ~\vert~ f(x) \leq t}$</p><p><img data-proofer-ignore data-src="https://i.stack.imgur.com/y98sx.png" alt="Représentation épigraphe" /></p></div><blockquote><p><strong>Exercice :</strong> Dessiner l’intersection de l’épigraphe de $f(x)=-\sqrt{x}$ (sur $\mathbb R^+$) avec la partie ${(x, y) | y \leq \sqrt x}$ <img data-proofer-ignore data-src="https://i.imgur.com/0R5DgRj.png" alt="" /></p></blockquote><div class="alert alert-success" role="alert"><p>Une partie de $A\subset \mathbb{R}^n$ est <strong>convexe</strong> ssi $\forall x,y\in A, \forall t\in [0,1]$ alors \(tx+(1-t)y \in A\)</p></div><div class="alert alert-info" role="alert"><p><strong>L’adhérence</strong> d’une partie est sa frontiere. $A \cup \partial{A} = \underbrace{\bar{A}}_{adhérence}$</p></div><blockquote><p><strong>Exemple :</strong> $A = [1,2[$ $\partial A = {{1} {2}}$ $\bar A = [1,2]$</p></blockquote><blockquote><p>Quoi ? Hyper parapluie ??? [name=multun]</p></blockquote><div class="alert alert-info" role="alert"><p>Un <strong>hyperplan d’appui</strong> d’une partie $A$ est un hyperplan de $A$ qui posséde un élément du bord de $A$ <img data-proofer-ignore data-src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSLLg3Yp48CLVcKGuwVXIXZ8GxKMd8DJ58WHYSagPY4qjq5SX9Sag" alt="Schema hyperplan d'appui" /></p></div><div class="alert alert-info" role="alert"><p>Une <strong>fonction convexe</strong> admet des hyperplans d’appui en chacun des points de sa frontière.</p></div><div class="alert alert-info" role="alert"><p>$f$ <strong>convexe</strong> $\Leftrightarrow f(tx+(1-t)y) \leq tf(x)+ (1-t)f(y)$</p></div><h3 id="exemple-de-fonctions-convexes-">Exemple de fonctions convexes :<a href="#exemple-de-fonctions-convexes-"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>$f(x) = ax^2 +bx +c , a \ge 0$<li>$f(x) = bx+ c$<li>$e^{ax} \quad \forall a$<li>$f(x) = -\log(x)$<li>$f(x) = \sqrt(x)$<li>$f(x) = \vert x\vert$<li>$f(x) = x^{2p}, p \in \mathbb{N}^*$</ul><blockquote><p><strong>Exercice :</strong> Est ce que la somme de fonctions convexes est convexe ?</p><p>$f = \sum_{i=1}^{N}w_if_i$ la somme de fonctions convexes</p><p>\(\begin{aligned} f(tx + (1-tg)) &amp;= \sum_{i=1}^N f_i (tx+(1-t)y) \\ &amp;\leq tf_i(x)+(1-t)f_i(y) \\ &amp;\leq \sum_{i=1}^N w_i(tf_i(x)+(1-t)f_i(y)) \\ &amp;\leq \sum_{i=1}^N tw_if_i(x) + \sum_{i=1}^N(1-t)w_if_i(y) \\ &amp;\leq t\underbrace{\sum_{i=1}^N w_if_i(x)}_{f(x)} + (1-t) \underbrace{\sum_{i=1}^N w_if_i(y)}_{f(y)} \end{aligned}\) Donc c’est bien convexe !</p></blockquote><p>Soit $f(x,y) = x^2 + y^2$ La fonction <a href="https://fr.wikipedia.org/wiki/Matrice_hessienne">Hessienne</a> de $f$: \(H(x,y) = \begin{pmatrix}\frac{\partial^2f(x,y)}{\partial x^2} &amp;\frac{\partial^2f(x,y)}{\partial y \, \partial x} \\ \frac{\partial^2f(x,y)}{\partial y \, \partial x} &amp; \frac{\partial^2f(x,y)}{\partial y^2} \end{pmatrix}\)</p><h1 id="programme-linéaire">Programme linéaire</h1><blockquote><p><strong>Exercice 1:</strong></p><p>$\mathcal A_u : \begin{cases} -x+2y \leq 1<br /> x + y \leq 1 \end{cases}$</p><p>$\mathcal A_b = \mathcal A_u \cup {(x,y) \in \mathbb R^2, x-3y \leq 6}$</p><ul><li><p>$(D_1)$: $-x + 2y + 1 = 0 \qquad (0, -\frac 12) \in (D_1) \qquad \vec{n_1}\begin{pmatrix}-1\ 2\end{pmatrix} \qquad \vec{u_1}\begin{pmatrix}-2\ -1\end{pmatrix}$</p><li><p>$(D_2)$: $x + y - 1 = 0 \qquad (0,1) \in (D_2) \qquad \overrightarrow{n_2}\begin{pmatrix} 1 \ 1\end{pmatrix}\qquad \overrightarrow{u_2}\begin{pmatrix} -1 \ 1\end{pmatrix}$</p><li><p>$(D_3)$: $x - 3y - 6 = 0 \qquad (0,-2) \in (D_3) \qquad \overrightarrow{n_3}\begin{pmatrix} 1 \ -3\end{pmatrix}\qquad \overrightarrow{u_3}\begin{pmatrix} 3 \ 1\end{pmatrix}$</p></ul><p>\(\underbrace{\min f_0(x,y) = y = -\infty}_{(x,y) \in \mathcal{A}_u}\) \(\underbrace{\min f_0(x,y) = -y = 0}_{(x,y) \in \mathcal{A}_u}\)</p></blockquote><blockquote><p><strong>Exercice 2:</strong></p><p>$f(x,y) = 3x^2 + y^2$ \(\mathcal{C}_2(f)\mathcal{C}_4(f)\)? \(\mathcal{C}_{\le 4}(f)\)? \(\min f_0(x,y) = 2x + y\) sujet à $3x^2 +y^2 \le 4$</p>\[\begin{aligned} \mathcal{C}_2(f) : &amp; 3x^2 + y^2 = 2 \\ \Leftrightarrow &amp;\frac{3}{2}x^2 \frac{1}{2}y^2 = 1\\ \Leftrightarrow &amp;\begin{pmatrix}\frac{x}{\frac{\sqrt{2}}{\sqrt{3}}}\end{pmatrix}^2 + \begin{pmatrix}\frac{y}{\sqrt{2}}\end{pmatrix}^2\\ \end{aligned} \begin{aligned} \mathcal{C}_4(f) : &amp; 3x^2 + y^2 = 4 \\ \Leftrightarrow &amp;\frac{3}{4}x^2 \frac{1}{4}y^2 = 1\\ \Leftrightarrow &amp;\begin{pmatrix}\frac{x}{\frac{2}{\sqrt{3}}}\end{pmatrix}^2 + \begin{pmatrix}\frac{y}{2}\end{pmatrix}^2\\ \end{aligned} \begin{aligned} \mathcal{C}_0(f_0) : &amp; 2x + y = 0 \\ &amp; (0,0) \in \mathcal{C}_0(f_0)\\ &amp; \overrightarrow{n}\begin{pmatrix} 2\\ 1\end{pmatrix} \overrightarrow{u}\begin{pmatrix} -1\\ 2\end{pmatrix} \end{aligned}\]<p>\(\min(f_0(xy)) = f_0^*$ pour $(x,y) = (x^*,y^*)\) \(3x^{*2} +y^{*2} = 4$ $(x^*,y^*) \in \mathcal{C}_4(f)\) \(2x^{*} + y^* = f_0^*$ $(x^*,y^*) \in \mathcal{C}_{f_0^*}(f_0)\)</p><div class="alert alert-info" role="alert"><p>\(\begin{aligned} \Delta f(x^*, y^*) = \begin{pmatrix} 6x^* \\ 2y^*\end{pmatrix}\\ \langle \Delta f(x^*,y^*), \overrightarrow{u} \rangle = 0\\ \begin{pmatrix} 6x^* &amp; 2y^* \end{pmatrix} \begin{pmatrix} -1 \\ 2 \end{pmatrix} = 0 \end{aligned}\)</p></div><p>\(\begin{aligned} 3x^{*2} + y^{*2} &amp;= 4\\ -6x^* + 4y^* &amp;= 0 \rightarrow y^* &amp;= \frac{6}{4} x^*\\ &amp; &amp; = \frac{3}{2}x^*\\ &amp; &amp;= \frac{6}{\sqrt{21}} \end{aligned}\) $y^* = \frac{6}{\sqrt{21}}$</p><p>\(3x^{*2} + (\frac{3}{2}x^{\*})^2 = 4\) \(3x^{*2} + \frac{9}{4}x^{\*2} = 4\) \(\frac{21}{4}x^{\*2} = 4\) \(x^{*2} = \frac{16}{21}\) \(x^* = \frac{4}{\sqrt{21}}$ ou $-\frac{4}{\sqrt{21}}\) <img data-proofer-ignore data-src="https://i.imgur.com/MfE89nI.jpg" alt="" /></p></blockquote><h1 id="géometrie-différentielle-pour-les-petits">Géometrie différentielle pour les petits</h1><p>Avec ce qu’on a vu à ce jour on peut chercher à résoudre un problème d’optimisation de la forme suivante \(\begin{aligned} \text{min}\qquad &amp; \overbrace{-x_1-2x_2}^{f_o(x_1,x_2)} \\ \text{sujet à} \qquad &amp;x_1+x_2 \leqslant 5 \\ &amp; -2x_1+x_2 \leqslant 3 \\ &amp; x_1, x_2 \geqslant 0 \end{aligned}\)</p><p><img data-proofer-ignore data-src="https://i.imgur.com/y2TapSp.png" alt="" /></p><blockquote><p>Sur la rouge non plus, cf (0.5, 1) Tu dépasse a gauche</p></blockquote><p>$\color{red}{\text{Le lieu admissible}}$ $\color{green}{\mathscr{C}_0}:$ Courbe de niveau de $f_0$ passant par $(0,0)$</p><p>Afin d’améliorer la valeur objectif du point courant, on cherche un point à la fois dans le lieu admissible et dans le demi-espace, qu’on determine à partir de l’équation de la fonction objectif.</p><p>La courbe de niveau de la fonction objectif au point optimal isole le lieu admissible dans la partie + des demi-espaces defini par la courbe de niveau de la fonction objectif en ce point. le demi-espace est un hyperplan d’appui au lieu admissible.</p><hr /><blockquote><p><strong>Exercice :</strong></p><p>\(\begin{aligned} \text{min}\qquad &amp; x+y \\ \text{sujet à} \qquad &amp;x^2+y^2 \leqslant 1 \end{aligned}\) Comment trouver les hyperplans d’appui au lieu admissible?</p><p><img data-proofer-ignore data-src="https://i.imgur.com/MbGpHe5.png" alt="" /> point optimal en $(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})$ je propose <img data-proofer-ignore data-src="https://i.imgur.com/AIZviNO.png" alt="" /> $\color{blue}{\text{Point optimal B}}$</p></blockquote><p>Quand on cherche à minimiser une fonction objectif affine contrainte par un lieu admissible qui est une ellipse de $\mathbb R^2$, on se retrouve à rechercher des hyperplans d’appui de celui-ci . Cette notion nous ramene à l’étude des dérivées de fonction numériques dont les graphes décrivent des morceaux du bord du lieu admissible. Pour généraliser cette approche, on a besoin de généraliser la notion de dérivée sur plusieurs variables.</p><h2 id="normes-sur-mathbb-r2">Normes sur $\mathbb R^2$<a href="#normes-sur-mathbb-r2"><i class="fas fa-hashtag"></i></a></h2></h2><p>Une norme sur $\mathbb R-ev$ est une manière de mesurer la longueur d’un vecteur, tout en préservant un minimum la structure d’ev. Elle permet en particulier de mesurer la distance entre deux points pour la longueur du vecteur qui les relie.</p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Une norme sur $\mathbb R^n$ est une application $\Vert \cdot\Vert : \mathbb{R}^n \longmapsto \mathbb{R}$ telle que</p><p>1) $\Vert x\Vert = 0 \Longleftrightarrow x = 0$ 2) $\forall \lambda \in \mathbb R , \forall x \in \mathbb R^n: \Vert \lambda x\Vert = |\lambda|\cdot\Vert x\Vert \qquad \quad$ (relation d’homogéinité) 3) $\forall x, y \in \mathbb{R}^n$, $\Vert x+y\Vert \leqslant \Vert x\Vert + \Vert y\Vert \qquad \qquad$ (inégalité triangulaire)</p></div><blockquote><p><strong>Exercice :</strong> sur $\mathbb R^n$</p><ol><li> \[\Vert x\Vert _1 = \displaystyle\sum_{i=1}^{n}\vert x_i\vert\]<li> \[\Vert x\Vert _2 = \bigg(\displaystyle\sum_{i=1}^{n}(x_i)^2\bigg)^{\frac 1 2}=\sqrt{x^Tx}\]<li>$\Vert x\Vert _\infty = \underset{i\in {1,\dots,n}}{\max}{\vert x_i\vert }$<li>Pour $p \geqslant 1$,<blockquote><p>\(\Vert x\Vert _p = \bigg(\displaystyle\sum_{i=1}^{n}\vert x_i\vert ^p\bigg)^{\frac 1 p} \qquad \qquad\) (la norme p) ($p \ge 1$)</p></blockquote></ol></blockquote><p>À partir d’une norme sur $\mathbb R^n$, on va pouvoir définir :</p><ol><li>Une distance : $\forall x, y \in \mathbb R^n, d(x, y) = \Vert x-y\Vert $</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/IqsTRki.png" alt="" /></p><ol><li><p>Une notion de voisinage d’un point</p><p>Quand on fait de l’analyse, on s’intéresse à ce qui se passe autour d’un point donné $\varepsilon$-près. Par example, pour montrer qu’une suite numérique $(u_n)_{n\in \mathbb{N}}$ converge vers $l\in \mathbb{R}$, on vérifie: \(\forall \varepsilon &gt; 0, \exists N \in \mathbb{N}, n \ge N \Longrightarrow \underbrace{|u_n - l| &lt; \varepsilon}_{u_n\in ] l-\varepsilon, l+\varepsilon [}\)</p></ol><p>La notion de norme sur $\mathbb R^n$ permet de généraliser cette notion à toute dimension. Par exemple si $(u_n)_{n \in \mathbb{N}}$ une suite à valeurs dans $\mathbb R^n$ et $l = (l_1, .., l_n) \in \mathbb{R}^n$. On dit que (u_n) converge vers l au sens de la norme $\Vert .\Vert $ si : \((E) \qquad \forall \varepsilon &gt; 0, \exists N \in \mathbb{N}, n \ge N \Longrightarrow \Vert u_n - l\Vert &lt; \varepsilon\)</p><p>On note $\begin{aligned}B_{\Vert \cdot\Vert }(l,\varepsilon)={x\in \mathbb{R}^n | \Vert x-l\Vert &lt;\varepsilon} \ \bar{B}_{\Vert \cdot\Vert }(l,\varepsilon)={x\in \mathbb{R}^n | \Vert x-\Vert |&lt;\varepsilon} \end{aligned}$</p><p>Dans ce cas, $(E)$ s’écrit : \(\forall \varepsilon &gt; 0, \exists N \in \mathbb N , b \ge N \Rightarrow \mathcal U_n \in B_{\Vert .\Vert }(l,\varepsilon)\)</p><p>Dans le cas de $\mathbb R^2$ on represente $\overline{B_{\Vert \cdot\Vert }}(\underbrace{\underline{0}}_{\text{l’origine}},1)$</p><p><img data-proofer-ignore data-src="https://i.imgur.com/vxTQU9I.png" alt="" /> <img data-proofer-ignore data-src="https://i.imgur.com/WB5lwF8.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>Remarque: les boules d’une norme sont convexes.</p><blockquote><p><em>du coup, les boules de Noel aussi #loul</em></p></blockquote></div><p>Comme on a pu le voir pour la cas de la convergence d’une suite se donner une norme sur $\mathbb{R}^n$ va nous permettre de transposer les notons de continuité d’une fonction ou de comparaison de fonctions en un point $(o, \theta, \text{~} )$</p><blockquote><p><strong>Exercice :</strong> On se donne une norme $\Vert .\Vert $ sur$\mathbb R^n$. continuite): Soit $f:(E, \Vert .\Vert _E)\rightarrow (F, \Vert .\Vert _F)$ on dit que f est continue en $a \in E$ si</p><ul><li>$f$ est défini au voisinage de $a$ \(\forall \varepsilon &gt; 0, \exists \mu &gt; 0 tq \Vert x-a\Vert &lt; \mu \Rightarrow \Vert f(x) - f(a)\Vert &lt; \varepsilon\)</ul><p>$(\theta)$ Une fonction f est un $\theta_1(g)$ en a $\in$ E s’il existe $\varepsilon$ :(E,$\Vert \cdot\Vert _E$) $\rightarrow \mathbb R$ telle que</p><ul><li>$f=\varepsilon g$<li>$\varepsilon \xrightarrow[a]{} 0$</ul><p>Quand g n’est pas identiquement nulle au voisinage de a, la condition précédente est équivalente à $\frac{\Vert f\Vert _F}{\Vert g\Vert _F} \xrightarrow[a]{} 0$</p></blockquote><p>Il semble à ce stade que la définition de continuité ou celle de convergence dépende de la norme choisie.</p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Les normes $\Vert \cdot\Vert_\alpha$ et $\Vert \cdot\Vert_\beta$ sur $\mathbb{R}^n$ sont dites équivalentes s’il existe $c, C \in \mathbb{R}_+^{*}$ telle que \(\forall x \in \mathbb{R}^n, c\Vert x\Vert _\alpha \leqslant \Vert x\Vert _\beta \leqslant C\Vert x\Vert _\alpha\) Si 2 normes sont équivalentes alors elles définissent les mêmes fonctions continues, les mêmes o, $\theta$, ~ ou encore les mêmes suites convergentes.</p></div><div class="alert alert-info" role="alert"><p><strong>Théorème :</strong> Sur $\mathbb R^n$ toutes les normes sont équivalentes.</p></div><h2 id="normes-sur-mathbb-rn">Normes sur $\mathbb R^n$<a href="#normes-sur-mathbb-rn"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>Les normes usuelles:</p><ol><li> \[\Vert .\Vert _2 : \forall x \in \mathbb R^n ; \Vert x\Vert _2 = (\sum_{x=1}^n x_i^2)^{\frac 12}=\sqrt{x^Tx}\]<li> \[\Vert .\Vert _1: \forall x \in \mathbb R^n ; \Vert x\Vert _1 = \sum_{i=1}^n\vert x_i\vert\]<li> \[\Vert .\Vert _\infty: \forall x \in \mathbb R^n ; \Vert x\Vert _\infty = \underset{i \in \{1,\dots,n\}}{max} \vert X_i\vert\]</ol><p><strong>Propriété</strong>: \(\forall p \geq 1:\\ \forall x \in \mathbb R^n ; \Vert x\Vert _p = (\sum(x_i)^p)^{\frac 1p}\)</p></div><p>À partir d’une norme, on définit :</p><ul><li>Une distance : $d_{\Vert .\Vert }$(x,y) = \Vert x-y\Vert $<li>Des boules :<ul><li>Ouvertes : $B_{\Vert .\Vert }(x, \varepsilon) = {y\vert d_{\Vert .\Vert }(x,y) &lt; \varepsilon}$<li>Fermées : \(\bar{B}_{\Vert .\Vert }(x, \varepsilon) = \{y\vert d_{\Vert .\Vert }(x,y) \leq \varepsilon\}\)</ul></ul><p><strong>Objectif</strong> : Montrer que les boules ouvertes pour une norme $\Vert .\Vert $ sur $\mathbb R^n$ sont convexes.</p><p><img data-proofer-ignore data-src="https://i.imgur.comcskBRG.png" alt="" /></p><div class="alert alert-success" role="alert"><p><strong>Définition :</strong> Une fonction $f : \mathbb{R}^n \rightarrow \mathbb{R}$ est dite convexe si \(\forall x, y \in \mathbb{R}^n, \forall t \in [0, 1]; \\f(tx + (1 - t)y) \le tf(x)+(1-t)f(y)\) <img data-proofer-ignore data-src="https://i.imgur.com/s55OJWR.png" alt="" /></p><hr /><p>L’inégalité de convexité se traduit géométriquement par le fait que les secantes entre deux points du graphe de $f$ sont au-dessus de salves prises par $f$ entre les abscisses de ces points.</p></div><div class="alert alert-info" role="alert"><p>Soient $f: (\mathbb R^2, \Vert .\Vert ) \rightarrow (\mathbb R, \Vert .\Vert )$ et $(0,0) \in \mathbb R^2$</p><ul><li>$f$ est continue en $(0,0) \in \mathbb R^2$ si $\forall \varepsilon &gt; 0, \exists ~ q &gt; 0, \quad \Vert (x,y)\Vert &lt; q \implies \Vert f(x,y)-f(0,0)\Vert &lt; \varepsilon$</ul></div><h1 id="différentiabilité-et-différentielle">Différentiabilité et différentielle</h1><p>Pour rappel on avait conclu à la séance précédente qu’il nous fallait étendre la notion de dérivée d’une fonction numérique au cas des fonctions à plusieurs variables.</p><p>On se donne une $f: \mathbb R \rightarrow \mathbb R$ au voisinage $a \in \mathbb R$</p><p>Dire que $f$ est derivable en $a \in \mathbb R$ c’est a dire que la limite: $lim_{h \rightarrow 0, h \ne 0} \frac{f(a +h) - f(a)}{h} \quad (D)$ existe ; càd est un nombre réél $l\in\mathbb{R}$</p><p>L’objectif est détendre la notion de dérivabilié au cas d’une fonction : $g: \mathbb R^n \mapsto \mathbb R^m (n&gt;1)$ en $a\in\mathbb{R}^n$</p><div class="alert alert-danger" role="alert"><p>On <strong>ne peut pas</strong> faire : $\frac{g(a+h)-g(a)}{\underbrace{h}_{\text{Un vecteur}}}$ Diviser par un vecteur n’a pas de sens…</p></div><p>L’approche $(D)$ n’est pas celle qui s’étend le plus facilement vers le cas multivarié. On doit voir les choses autrement.</p><p>Si $f$ est derivable en $a$: $f(a +h) = \boxed{f(a)} + \underbrace{f’(a)h}_{*} + \boxed{o_0(h)}$</p><p>$*$ Est la partie linéaire de l’approximation affine de $f$ en a; \(\begin{aligned}&amp;h \mapsto f'(a).h\\ &amp; \mathbb R \rightarrow \mathbb R \end{aligned}\)</p><div class="alert alert-info" role="alert"><p><strong>Remarque</strong>: Caractériser les applications linéaires de $\mathbb R$ dans lui-même.</p><p>Soit $\mathcal L: \mathbb R \rightarrow \mathbb R$</p><p>une application linéaire $\forall x \in \mathbb R; \mathcal L(x) = \mathcal L(x.1) = x.\mathcal L(1)$ Si $\mathcal L$ est un <a href="https://fr.wikipedia.org/wiki/Endomorphisme"><strong>endomorphisme</strong></a> de $\mathbb R$ alors $\mathcal L$ est de la forme $x \mapsto \lambda x; \lambda \in \mathbb R$.</p></div><p><strong>Propriété</strong>: Soit $f: \mathbb R \rightarrow \mathbb R$ une application définie au voisinage de $a\in\mathbb R$, $f$ est dérivable en $a$ ssi il existe $\lambda_a\in\mathcal{L}(\mathbb{R,R})$ telle que $\forall h \text{ proche de } 0,\;f(a +h) = f(a) + f’(a)h + o_0(h)$ $(**)$</p><p><strong>Preuve</strong>:</p><ul><li>$(\Rightarrow)$: c’est ce que l’on vient de dire<li>$(\Leftarrow)$: On suppose qu’on a une écriture du type $(**)$</ul><p>On regarde pour h assez proche de 0, $h \neq 0$</p>\[\begin{aligned} \frac {f(a+h) - f(a)}{h} &amp;= \frac{\lambda_n(h) + o_0(h)}{h}\\ &amp;= \frac 1h \lambda_n(h) + \frac 1h o_0(h)\\ \frac {f(a+h) - f(a)}{h} &amp;= \lambda_a(1) + o_0 (1)\\ \Rightarrow \underset{h \rightarrow 0\\ h \neq 0}{lim}\frac {f(a+h) - f(a)}{h} &amp;= \lambda_a(1) \in \mathbb R \end{aligned}\]<p>Donc $f$ est derivable en a et $f’(a)=\lambda_a(1)$</p><div class="alert alert-success" role="alert"><p><strong>Définition de la différentiabilité</strong>:</p><p>On suppose $\mathbb R^n, \mathbb R^m$ munies de normes qu’on note indifférentiablement $\Vert \cdot\Vert $. Dans la suite les énoncés qu’on fait ne dépendent pas des normes choisies.</p></div><p>On appelle ouvert de $\mathbb{R}^n$ pour $\Vert \cdot\Vert $ toute partie de $\mathbb{R}^n$ qui contient des voisinages de chacun de ses points.</p><p>Ex:</p><ul><li>$]a,b[ \subset \mathbb R$<li>$B_{\Vert \cdot\Vert }(x,R); R &gt; 0$</ul><p><strong>Définition:</strong> Soit $f : u \subset \mathbb R^n \rightarrow \mathbb R^n$ une fonction définie en $a \in u$, $f$ est différentiable en $a$ s’il existe $\lambda_u \in \mathcal L(\mathbb R^n, \mathbb R^n)$ telle que $\forall h$ assez proche de $\underline{0}$:</p><p>$f(u+h)=f(a) + \lambda_a(h) + o_\underline{0}(h)$</p><div class="alert alert-info" role="alert"><p>$\underline{0} = (0, …, 0) \in \mathbb R^n$</p></div><p><strong>Propriété</strong>: Quand elle existe l’application linéaire $\lambda a$ est unique.</p><p><strong>Preuve</strong>: Supposons qu’il existe pour h assez proche de 0,2 $\lambda_n,\mu_n \in \mathcal(\mathbb R^n, \mathbb R^m)$ telles que:</p><p>$f(a+h) = f(a) + \lambda_n(h) + o_\underline{0}(h)$ $= f(a) + \mu_a(h) + o_\underline{0}(h)$</p><p>$\forall h$ assez proche de $\underline{0}$:</p><p>$(\lambda_a - \mu_a)(h) = o_\underline{0}(h)$</p><p>Soit $v_1, \ldots ,v_n$ une base de $\mathbb R^n$. Pour $t$ au voisinage de $0 \in \mathbb R; t \ne 0$ $\forall i \in {1,\ldots, n}$ $(\lambda_a - \mu_a)(tv_1) = o_\underline{0}(t\overbrace{n}^{\in \mathbb R^n})$ D’ou $(\frac{\lambda_a - \mu_a)(tv_1)}{t} = o_\underline{0}(v)$ $\Leftrightarrow (\lambda_a -\mu_a)(v_i) = o_\underline{0}(v)$ $\Rightarrow (\lambda_a -\mu_a)(v_i) = 0 \qquad t \rightarrow 0$</p><p>Comme $\lambda_a - \mu_a$ est une application linéaire nulle sur tout élément de la base $v_1,\ldots,v_n$ elle est nulle. Donc $\lambda_a = \mu_a$.</p><p><strong>Definition</strong>: Soit $f: u \rightarrow \mathbb R^n$ definie sur un ouvert $u \subset \mathbb R^n$ Si $f$ est différentiable, l’unique application lineaire $Df(u) \in \mathcal L (\mathbb R^n, \mathbb R^n)$, telle que pour h assez proche de \underline{0}. $f(a+h) = f(a) + Df(a)(h) + o_\underline{0}(h)$</p><p>$Df(a)$ est appelée dans le ce cas la diffééérentielle de $f$ en $a$.</p><div class="alert alert-info" role="alert"><p><strong>Propriété</strong>: Si $f$ est différentiable en un point $a \in u$ alors elle est continue en $a \in u$</p></div><div class="alert alert-info" role="alert"><p><strong>Propriétés</strong>: Soient $f,g$ 2 fonctions \(\begin{aligned}&amp;u \\ &amp;\mathbb R^n \rightarrow \mathbb R\end{aligned}\) différentiables en $a \in u$ alors</p><ol><li><p>$\forall \lambda \in \mathbb R, \lambda f$ est différentiables en a et $D(\lambda f(a)) = \lambda Df(a)$</p><li>$f+g$ est différentiable en $a$ et $D(f+g)(a)=Df(a) + Dg(a)$<li>$\langle f,g \rangle$ est différentiable en a et $D(\langle f, g\rangle)(a) = \langle f(a), Dg(a)\rangle + \langle Df(a), g(a) \rangle$</ol></div><p>Prop: Soient f, g des fonctions différentiables respectivement en $a$ et $b = f(a)$ Alors $g \circ f$ est différentiable en a et $D(g \circ f)(a) = Dg(f(n))\circ Df(a)$</p><p>$\mathbb R^n \overset{f}{\longmapsto}\mathbb R^m \overset{g}{\longmapsto}\mathbb R^k$ $a \rightarrow f(a)$</p><p>$\mathbb R^n \overset{Df(a)}{\longmapsto}\mathbb R^m \overset{dg(f(a))}{\longmapsto}\mathbb R^k$</p><blockquote><p>Ex: 1) $\mathbb R^n \overset{f}{\rightarrow} \mathbb R$ $X \rightarrow \langle X,X \rangle = X^TX$</p><p>2) $\mathbb R^n \overset{g}{\rightarrow} \mathbb R$ $X \rightarrow e^{X^TX}$</p><p>f(X + h) = (X + h)^T(X+h) $= (X^T +h^T)(X +h)$ $f(X) = X^TX + h^TX + X^Th + h^Th$ $\mu_q h^Th = \theta_\underline{0}(h)$ $h \ne 0, \frac{|h^Th|}{\Vert h\Vert _2} = \frac{|h^Th|}{\sqrt{h^Th}}$ $= \sqrt{hTh} = \Vert h\Vert _2 \rightarrow 0; h \rightarrow \underline{0}$</p></blockquote><hr /><div class="alert alert-info" role="alert"><p>Rappel normes, voir plus haut. $\Vert x\Vert _0 =$ # d’elements non nuls de x. (ce n’est pas une norme) Inégalité triangulaire inversée: $|: \Vert x\Vert -\Vert y\Vert : | \le \Vert x - y\Vert $</p></div><p>A partir d’une norme \Vert .\Vert on définit la notion distance: $d: E \times E \rightarrow \mathbb R^+$ $(x,y) \rightarrow \langle x,y \rangle$ alors $\sqrt{ \langle x,x \rangle}$ est une norme pour x.</p><h3 id="voisinage-de-a">Voisinage de a<a href="#voisinage-de-a"><i class="fas fa-hashtag"></i></a></h3></h3><p>Boule centrée en a et de rayon r. $\rightarrow$”Tout ce qui se passe à une $\underbrace{\text{distance}}_{\text{besoin d’une norme}} r$ de $a \in \mathbb R^n$</p><p>$\rightarrow B_{\Vert .\Vert }(a,r) = {x \in \mathbb R, d(x,a) \lt r}$ Ceci est une boule ouverte</p><p>$B_{\Vert .\Vert }(a,r) = {x \in \mathbb R^n, d(x,a) \le r}$ Ceci est une boule fermée</p><p>$\overline{B_{\Vert .\Vert }} = B_{\Vert .\Vert } \cup B_{\Vert .\Vert }$</p><p>Les fonctions norme sont des fonctions convexes.</p><p>\(p = \frac 12 \qquad x = \begin{pmatrix} x_1 \\ x_2\end{pmatrix} \qquad \Vert x\Vert _{\frac12} = (\sqrt{\vert x_1\vert } + \sqrt{\vert x_2\vert })^2\) \(B_{\Vert .\Vert \frac12}(0,1) = \{(x_1,x_2) \in \mathbb R^2, (\sqrt{\vert x_1\vert } + \sqrt{\vert x_2\vert } \le 1\}\)</p><p>Dans le cadre ou $x_1 \ge 0, x_2 \ge 0$</p><p>$(\sqrt{x_1} + \sqrt{x_2})^2 &lt; 1$ $\sqrt{x_1} + \sqrt{x_2} &lt;1$ $x_2 &lt; (1 - \sqrt{x_1})^2$</p><p>$f:(\mathbb R^n, \Vert .\Vert _{\alpha} \rightarrow (\mathbb R^p, \Vert .\Vert _p)$ $x = (x_1, …, x_n) \rightarrow (f_1(x_1,…,x_n),…f_p(x_1,…,x_n))$ $a \in \mathbb R^n f$ est continue en $a \in \mathbb R^n$ $\forall \varepsilon &gt; 0, \exists \mu &gt; 0, \forall x \in \mathbb R^n, \Vert x - a\Vert _{\alpha} &lt; \mu \Rightarrow \Vert f(x) - f(a)\Vert _{\beta} &lt; \varepsilon$</p><p>Deux normes $\Vert .\Vert _\alpha$ et $\Vert .\Vert _{\beta}$ sont equivalentes ssi<br /> $\exists c \ge 0, C \ge 0, \forall x \in E, c\Vert x\Vert _\beta \le \Vert x\Vert _\alpha \le C\Vert x\Vert _\beta$ dans un espace vectoriel de dimension finie, toutes les normes sont équivalentes.</p><p>$x \in \mathbb R^n, x = \begin{pmatrix} x_1 \ .\ .\ .\ x_n\end{pmatrix}$ \(\Vert x\Vert_1 = \sum_{i = 1}^n \vert x_i\vert \qquad \Vert x\Vert _\infty = max_{i = 1,...,n}\vert x_i\vert\)</p><p>…</p><h1 id="fonction-lipschitzienne">Fonction Lipschitzienne</h1><p>Une fonction est dite Lipschitzienne ssi: $\exists K &gt; 0$ tq $\Vert f(x) - f(y)\Vert \le K \Vert x - y\Vert $ $\forall x,y \in D_y$ Si f est Lipschitzienne, f est continue.</p><blockquote><p>$f: \mathbb R^2 \rightarrow \mathbb R$ $(x_1,x_2) \rightarrow x_1 + x_2$ $x = \begin{pmatrix}x_1\ x_2\end{pmatrix} y = \begin{pmatrix}y_1\ y_2\end{pmatrix}$ $\Vert x - y\Vert = \Vert \begin{pmatrix}x_1 - y_1\ x_2 - y_2\end{pmatrix}\Vert _1 = |x_1-y_1| + |x_2 - y_2|$ $\Vert f(x) - f(y)\Vert _1 = \Vert x_1 + x_2 - (y_1 +y_2)\Vert _1$ $= \Vert (x_1 - y_1)+ (x_2 -y_2)\Vert _1$ $= |(x_1 - y_1) + (x_2 - y_2)|$ $\le|x_1 - y_1| + |x_2 - y_2|$</p></blockquote><p>L’ensemble des fonctions continues est un espace vectoriel.</p><ul><li>Si f,g sont continues $\lambda f + \mu g$ est continue $\forall(\lambda,\mu) \in \mathbb R^2 \rightarrow$ structure d’espace vectoriel<li>Si p =1, f*g est continue, $\frac fg$ est continu partout ou g ne s’annule pas.<li>Si $h:\mathbb R^p \rightarrow \mathbb R^n$ qui est continue, alors $h\circ f: \mathbb R^n \rightarrow \mathbb R^n, x\rightarrow h(f(x))$ est continue.</ul><p>Toutes les fonctions de type polynome sont continues. et $\frac{f(x,y)}{g(x,y)}$ avec f et g polynomiales, est continue partout ou g ne s’annule pas. (fonction rationelle)</p><p>$\begin{cases} \frac{xy}{x^2 +y^2} \text{ si } (x,y) \ne (0,0)<br /> 0 \text{ sinon}\end{cases}$ $f(x,0) = 0 \rightarrow$ continu en 0 $f(0,y) = 0 \rightarrow$ continu en 0</p><blockquote><p>exemple Soient $g:t \rightarrow (t,t)$ $f \circ g(t) = f(g(t)) =f(t,t) = \frac{t^2}{t^2 + t^2} = \frac{t^2}{2t^2} = \frac12$ si $t \ne 0$</p><p>Donc $f \circ g$ n’est pas continue.</p></blockquote><blockquote><p><strong>Exercice</strong>: 1) $\mathbb R \overset{f}\longmapsto \mathbb R\ X \longmapsto X^T X$ On cherche à determiner la diffierenciablité de $f$ en tout point $a \in \mathbb{R}^n$ \(\begin{aligned}f(a+h)&amp;=(a+h)^T(a+h) \\ &amp;=a^Ta+h^Ta+a^Th+h^Th \\ &amp;=f(a)+\underbrace{2a^Th}_{h \rightarrow 2a^Th \\\text{ est linéaire}}+h^Th\end{aligned}\) $\Vert h\Vert _2^2=\Vert h\Vert _2\Vert h\Vert _2=\Vert h\Vert \varepsilon(h)$ On a $f(a+h)=f(a)+\text{ lim en h }+o_0(h)$. $f$ est différentiable en a et $\Delta f(a)h=2a^Th$ 2) $\mathbb R^n$ $\overset{g}{\rightarrow} \mathbb R$ $X \rightarrow e^{X^TX}$ $g: \mathbb R^n \overset{f}{\rightarrow} \mathbb R \overset{exp}{\rightarrow} \mathbb R$ $X \rightarrow X^TX \rightarrow e^{X^TX}$ On s’intéresse à la différenciabilité de g en $a\in\mathbb{R}^n$. La fonction g est composée de fonctions différentiables, elle est donc différentiable en tout point. En $a \in \mathbb{R}^n, \Delta g(a’)h=\Delta \exp (a^Ta)(\Delta f(a)f(h))$ Dans le cours: $\Delta g(a)=\Delta(exp\circ f)(a)=\Delta \exp (f(a))\circ \Delta f(a)$ \(\forall h \in \mathbb{R}^n, \Delta g(a)(h)=\Delta \exp (a^Ta)(\underbrace{\Delta f(a)f(h)}_{\in \mathbb{R}})\)</p><ul><li>$\Delta f(a)(h)=2aTh \qquad h \in \mathbb R$<li>$\Delta exp(y)(k) = exp’(y)\cdot k = e^yk$ $\implies \Delta g(a)(h) = e^{a^Ta} \times 2a^Th \qquad h \in \mathbb R^n\space a^T \in \mathbb R^n, e^{a^Ta} \in \mathbb R$</ul></blockquote><h1 id="gradient-et-dérivées-partielles">Gradient et dérivées partielles</h1><p>Soit $f:U \underline{\subset} \mathbb R^n \rightarrow \mathbb{R}^m$ une application différentiable en $a \in U$. On peut donc écrire, pour h assez proche de 0: $f(a+ h) = f(a) + Df(a)(h) + \underset{l \rightarrow 0}{o_0(h)}$ $(f(a+h) = f(a) + \Delta f(a)f(h) + \Vert h\Vert \varepsilon(h))$ L’application $\Delta f(a)\in \mathscr{L}(\mathbb{R}^n, \mathbb{R}^m)$ où $\mathscr{L}(\mathbb{R}^n,\mathbb{R}^m)$ est l’ensemble des applications linéaires de $\mathbb{R}^n$ dans $\mathbb{R}^m$, est caractérisée par sa matrice dans des bases données.</p><div class="alert alert-success" role="alert"><p><strong>Définition:</strong> On appelle jacobienne de $f$ en $a \in U$ la matrice de $Df(a)$ dans les bases canoniques de $\mathbb{R}^n$ et $\mathbb{R}^m$ Dans cette section,on étudie comment trouver les coefficients de la matrice jacobienne de $f$ en $a$ <strong>Notation:</strong> La jacobienne de $f$ en $a$ s’écrit $\mathcal J_f(a) \in M_{m,n}(\mathbb{R})$ On a $f(a+ h) = f(a) + \mathcal J_f(a) \cdot h + o_0(h)$</p></div><p>On se pose en premier temps la question de savoir comment déterminer les lignes, puis en un second temps, les colonnes.</p><h2 id="colorpurpletextpour-les-lignes">$\color{purple}{\text{Pour les lignes}}$<a href="#colorpurpletextpour-les-lignes"><i class="fas fa-hashtag"></i></a></h2></h2><p>On écrit $f = (f_1, -, f_m)$ où $f_i:u \rightarrow \mathbb R$ est la composante de $f$ suivant la $i_{eme}$ coordonnée.</p><p><strong>Exemple</strong>: $\mathbb R^3 \overset{f}{\rightarrow} \mathbb R^4$ $\begin{pmatrix}x \ y \ z \end{pmatrix} \rightarrow \begin{pmatrix}xy^2 \ x+y+z \ xyz \ z\end{pmatrix}$</p><p>En notant, $f_1\bigg(\overset{x}{\underset{z}{y}}\bigg) = xy^2$ $f_2\bigg(\overset{x}{\underset{z}{y}}\bigg) = x+y+z$ $f_3\bigg(\overset{x}{\underset{z}{y}}\bigg) = xyz$ $f_4\bigg(\overset{x}{\underset{z}{y}}\bigg) = z$</p><p>on a $f = (f_1, f_2, f_3, f_4)$ En $a \in U$, on a pour chaque $f$: $f_1(a+h) = f_i(a) + \mathcal J_{f_i}(a)h + \Vert h\Vert \varepsilon(h)$ On peut donc écrire:</p>\[\begin{aligned}f(a+h) &amp;= \begin{pmatrix} f_1(a+h) \\ \vdots \\ f_m(a+h) \end{pmatrix} \\ &amp;=\begin{pmatrix} f_1(a) + \mathcal J_{f_1}(a)h + \Vert h\Vert \varepsilon (h)\\ \vdots \\ f_m(a) + \mathcal J_{f_m}(a)h + \Vert h\Vert \varepsilon (h) \end{pmatrix} \\ &amp;= \begin{pmatrix} f_1(a) \\ \vdots \\f_m(a)\end{pmatrix} + \begin{pmatrix} \mathcal J_{f_1}(a)h \\ \vdots \\\mathcal J_{f_m}(a)h\end{pmatrix} + \Vert |\varepsilon(h) \\ &amp;= f(a) + \begin{pmatrix} \mathcal J_{f_1}(n) \\ \vdots \\ \mathcal J_{f_m}(n)\end{pmatrix}h + \Vert h\Vert \varepsilon(h)\end{aligned}\]<p>$(\mathcal J_{f_i}(n) \in \mathbb M_{m,n}(\mathbb R))$</p><p><strong>”$\varepsilon(h)$” est une quantité négligeable sous forme de vecteur</strong></p><p>Par unicité de la différentielle, on a : $\mathcal J_f(a) = \begin{pmatrix} \mathcal J_{f_1}(a) \ \vdots \ \mathcal J_{f_m}(a)\end{pmatrix}$ Autrement dit, $\mathcal{J}<em>f(a)$ est la concaténation des $\mathcal{J}</em>{f_i}(a)$ verticalement (en colonnes).</p><h2 id="colorpurpletextpour-les-colonnes">$\color{purple}{\text{Pour les colonnes}}$<a href="#colorpurpletextpour-les-colonnes"><i class="fas fa-hashtag"></i></a></h2></h2><p>Il nous reste à comprendre comment constuire la jacobienne en un point d’une fonction de $\mathbb{R}^n$ dans $\mathbb{R}$. Soit $g:U\subset\mathbb{R}^n\longmapsto \mathbb{R}$ une fonction différentiable en $a\in U$. Pour h assez proche de $\underline{0}$, $g(a+h)+g(a)+\mathcal{\mathcal J}_{g}h+\Vert h\Vert \varepsilon(h)$</p><p>Soit $v\in \mathbb{R}^n$ et $t\in \mathbb{R}$, pour t assez proche de 0, \(g(a+tv)=g(a)+\mathcal{J}_g(a)tv+\Vert tv\Vert \varepsilon(tv)\) \(g(a+tv)=g(a)+t\mathcal{J}_g(a)v+\Vert tv\Vert \varepsilon(tv)\) Si $t \neq 0$: \(\frac{g(a+tv)-g(a)}{t}=\mathcal{J}_g(a)v+\varepsilon(tv)\) Quand $t\longmapsto 0$ on a \(\mathcal{J}_g(a)v=\displaystyle\lim_{t\rightarrow0}\frac{g(a+tv)-g(a)}{t}\) La valeur de la $\mathcal{J}_g(a)$ en un vecteur v est décrite par la dérivée de la direction de g à la droite a+tv. $g_v:t \rightarrow g(a +tv)$ $\mathbb R : \frac{g(a + tv)-g(a)}{t} = \frac{g_v(t) - g_v(0)}{t}$</p><div class="alert alert-success" role="alert"><p><strong>Définition:</strong> On appelle dérivée directionelle de g en a le long de v, la limite, quand elle existe, \(\partial_v(g(a))=\lim_{t\rightarrow0}\frac{g(a+tv)-g(a)}{t}\) Notation: Dans le cas v, c’est un vecteur de la base canonique, $v=ej$ on note \(\partial_{e_j}g(a)=\frac{\partial g}{\partial x_j}(a)\)</p></div><div class="alert alert-info" role="alert"><p><strong>Remarque:</strong> On vient de voir que si $g$ est différentiable en $a$ alors $g$ admet des dérivées directionnelles en $a$ le long de tout vecteur.</p><div class="alert alert-danger" role="alert"><p>La réciproque est fausse.On peut avoir des directionnelles en tout point mais ne pas être différentiable.</p>\[\begin{cases} \frac{x^2 y}{x^4 + x^2} &amp; \text{si} (x, y) \neq (0, 0) \\ 0 \qquad &amp; \text{sinon} \end{cases}\]</div><div class="alert alert-info" role="alert"><p><strong>Propriété:</strong> Si les dérivées partielles de g sont des fonctions continues alors g est différentiable en tout point de fonction différentielle $x \longmapsto Df(x)$ continue.</p></div><p>Désormais si g est différentiable en un point a alors \(\mathcal Jg(a) = (\underbrace{\frac{\delta g(a)}{\delta x_1}}_{\mathcal Jg(a)e_1} \dots \underbrace{\frac{\delta g(a)}{\delta x_n}}_{\mathcal Jg(a)e_n})\)</p><p>Pour h assez proche de 0 \(g(a+h) = g(a) + \underbrace{\mathcal J_g(a)h}_{\mathcal M_{1,n}(\mathbb R)} + \Vert h\Vert \varepsilon(h)\)</p><div class="alert alert-success" role="alert"><p><strong>Définition:</strong> [gradient] On appelle gradient de g en a \(\nabla g(a)=\mathcal{J}_g(a)^T\) <em>On lit $\nabla$ “nabla”</em></p></div><blockquote><p>Ex: Calculer: 1) $\nabla g(x,y)$ pour $\mathbb R^2 \rightarrow \mathbb R$ $(x,y) \rightarrow xy^2 + y$ 2) $\mathcal J_f(x,y)$ pour $\mathbb R^2 \rightarrow \mathbb R^3$ $(x,y) \rightarrow (xy, y^2, \sin(xy))$</p></blockquote><blockquote><p>1)</p><ul><li>$\frac{\partial g (x,y)}{\partial x} = y^2$<li>$\frac{\partial g (x,y)}{\partial y} = 2yx + 1$ $\Rightarrow \nabla g(x,y) = \begin{pmatrix}y^2 \ 2yx + 1\end{pmatrix}$</ul><p>2) $\mathcal Jf(x, y)= \begin{pmatrix}y &amp; x \ ycos(xy) &amp; xcos(xy)\end{pmatrix}$</p></blockquote></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s8/'>Image S8</a>, <a href='/cours/categories/ocvx/'>OCVX</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/scia/" class="post-tag no-text-decoration" >SCIA</a> <a href="/cours/tags/ocvx/" class="post-tag no-text-decoration" >OCVX</a> <a href="/cours/tags/s8/" class="post-tag no-text-decoration" >S8</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=OCVX: Optimisation Convexe 1 - Cours&url=https://lemasyma.github.io/cours/posts/ocvx_verjus_1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=OCVX: Optimisation Convexe 1 - Cours&u=https://lemasyma.github.io/cours/posts/ocvx_verjus_1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=OCVX: Optimisation Convexe 1 - Cours&url=https://lemasyma.github.io/cours/posts/ocvx_verjus_1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/ocvx_hyperplan/"><div class="card-body"> <em class="timeago small" date="2021-05-07 10:00:00 +0200" >May 7, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Hyperplan d'appui</h3><div class="text-muted small"><p> Lien de la note Hackmd Rappels Hyperplan d’appui a une partie $A$ de $\mathbb R^n$ en un point $p\in A$, est un hyperplan affine de $\mathbb R^n$ qui laisse $A$ dans un des deux demi-espaces de...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_espaces-tangents/"><div class="card-body"> <em class="timeago small" date="2021-05-11 09:00:00 +0200" >May 11, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Espaces tangents</h3><div class="text-muted small"><p> Lien de la note Hackmd Dedramatiser les espaces tangents Pour une fonction \(\begin{aligned}f:\mathbb R&amp;amp;\to\mathbb R \\ x&amp;amp;\mapsto f(x)\end{aligned}\) \[Gr(f) = \{(x,f(x)), x\in\mathbb ...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_introduction/"><div class="card-body"> <em class="timeago small" date="2021-03-11 13:00:00 +0100" >Mar 11, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Introduction</h3><div class="text-muted small"><p> Lien de la note Hackmd Introduction L’optimisation fait partie des missions historiques de l’ingénierie. Elle naît avec l’ère industrielle: une fois un concept élaboré il s’agit de réduire les cou...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/prst_feuille_4/" class="btn btn-outline-primary" prompt="Older"><p>PRST: Feuille 4 - Exercices</p></a> <a href="/cours/posts/ocvx_kariulele_1/" class="btn btn-outline-primary" prompt="Newer"><p>OCVX: Optimisation Convexe 1, suite</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
