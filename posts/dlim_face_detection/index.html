<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="DLIM: Face detection" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Lien de la note Hackmd" /><meta property="og:description" content="Lien de la note Hackmd" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/dlim_face_detection/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/dlim_face_detection/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-11-22T14:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="DLIM: Face detection" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-11-22T15:25:44+01:00","datePublished":"2021-11-22T14:00:00+01:00","description":"Lien de la note Hackmd","headline":"DLIM: Face detection","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/dlim_face_detection/"},"url":"https://lemasyma.github.io/cours/posts/dlim_face_detection/"}</script><title>DLIM: Face detection | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>DLIM: Face detection</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>DLIM: Face detection</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-11-22 14:00:00 +0100" data-toggle="tooltip" data-placement="bottom" title="Mon, Nov 22, 2021, 2:00 PM +0100" >Nov 22, 2021</em> </span> <span> Updated <em class="timeago" date="2021-11-22 15:25:44 +0100 " data-toggle="tooltip" data-placement="bottom" title="Mon, Nov 22, 2021, 3:25 PM +0100" >Nov 22, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1581 words"> <em>8 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Lien de la <a href="https://hackmd.io/@lemasymasa/rksnLMt_F">note Hackmd</a></p><h1 id="face-detection-in-general">Face detection in general</h1><p><em>Why is facre detection so difficult ?</em></p><blockquote><p>Pose (<em>Out-of-Plane Rotation</em>) and orientation (<em>In-Plane Rotation</em>) Presence or absence of structural components Occlusions Imaging conditions Faces are highly non-rigid object (<em>deformations</em>)</p></blockquote><h2 id="related-problems">Related problems<a href="#related-problems"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Face localization<li>Facial feature extraction (landmarks such as eyes, mouth, …)<li>Face recognition<li>Verification<li>Facial expression</ul><h1 id="overview-of-different-approaches">Overview of different approaches:</h1><ol><li>Knowledge top-down <em>base method</em><li>Feature invariant methods (localization)<li>Template-matching methods (localization)<li>Appearance-based methods (detection)</ol><h2 id="apparence-based-methods-in-details">Apparence-based methods <strong>in details</strong><a href="#apparence-based-methods-in-details"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Eigenfaces<li>Distribution-based methods<li>Support Vector Machines (SVM)<li>Sparse Network of Winnows<li>Naive Bayes Classifier<li>Hidden Markov models</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/vy81DNW.png" alt="" /></p><ul><li>Information Theoretic Approaches (ITA)<li>Inductive Learning (C4.5 and Find-S algorithms)<li>Artficial Neural Networks (ANN) techniques<ul><li>Shallow networks (inverse de Deep)<li>Deep learning</ul></ul><p><img data-proofer-ignore data-src="https://i.imgur.com/Kzwwlwo.png" alt="" /></p><p>Les connections residuelles permettent de faire une retropropagation beaucoup plus loin que le reste du reseau.</p><div class="alert alert-warning" role="alert"><p>Le gros defaut des VGG: ils ont <strong>enormement</strong> de poids</p></div><blockquote><p>poids $=$ parametres $\neq$ hyperparametres</p></blockquote><p><img data-proofer-ignore data-src="https://i.imgur.com/0AF2tEV.png" alt="" /></p><p>Il y a de moins en moins de neurones en \%.</p><div class="alert alert-danger" role="alert"><p>Plus on a de poids, plus on a de chance que notre reseau soit puissant mais plus on a de chance qu’une partie serve a rien.</p></div><h1 id="the-beginning-in-1994">The beginning in 1994</h1><div class="alert alert-info" role="alert"><p>Burel et Carel proposent une methodologie pour les ANN’s:</p><ol><li>La phase d’entrainement ou un systeme <em>tunes</em> les parametres internes<li>La phase d’entrainement local ou le systeme adapte les poids specifiques a l’environnement d’un site local<li>La phase de detection durant laquelle <strong>les poids ne bougent pas</strong></ol></div><p>Vaillant, Montcoq et Le Cun: first translation invariant ANN, decides if each pixel belong or not to a given object</p><p>Yang et Huang: first fully automatic human face recoginition system</p><h2 id="1997">1997<a href="#1997"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>Rowleu, Baluja, Kanade propose the first rotation-invariant method:</p><ul><li>Uses template-based approach<li>Methodology:<ul><li>Regions are proposed<li>A router network estimates the orientation of this region<li>This network prepares the windows using this angle<li>A detector network decides if the window contains a face</ul></ul><p><img data-proofer-ignore data-src="https://i.imgur.com/rWhIs2L.png" alt="" /></p></div><h2 id="2004">2004<a href="#2004"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>First real-time face detection algo by Viola &amp; Jones</p></div><ul><li>Tells if a given image of arbitrary size contains a human face, and if so, where it is<li>Minimizes false positive and false negative rates<li>Usually 5 types of Haar-like features</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/DuYfZWB.png" alt="" /></p><ul><li>$24\times 24$ image contains <strong>a huge number of features</strong> ($162886$)<li>Integral image for feature computation</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/vRsIdvf.png" alt="" /></p><ul><li>$A=1$, $B=2$, $C=3$, etc.</ul><div class="alert alert-success" role="alert"><p>Allows a low computational cost of features</p></div><div class="alert alert-info" role="alert"><p><strong>Principle</strong></p><blockquote><p>The algorithm should deploy more resources to work on those windows more likely to contain a face while spending as little effort as possible on the rest</p></blockquote></div><ul><li>We can use <em>weak classifiers</em><li>Then we can mae a strong one with a sequence of weak ones<li>Viola &amp; Jones: use AdaBoost</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/g4sPVPF.png" alt="" /></p><p>The more layers, the less false positive:</p><p><img data-proofer-ignore data-src="https://i.imgur.com/kqlK5x2.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/OPqvcQy.png" alt="" /></p><h1 id="overfeat-2014">Overfeat (2014)</h1><div class="alert alert-info" role="alert"><ul><li>Winner of the ImageNet Large Scale Visual Recognition Challenger of 2013<li>Makes at the sae time classification (blocks), localization (grouping blocks) and detection (merge windows)<li>This multitask approach boosts the performance of the network<li>Trained on ImageNEt 2012</ul></div><ul><li>Inspired for multi-viewing voting<li>Uses multiscale factor of 1.4<li>Using a dense sliding windows thanks to convolution</ul><blockquote><p>The better aligned the network window and the object, the strongest the confidence of the network response.</p></blockquote><ul><li>Efficiency: convolution computations in overlapping regions are <em>shared</em><li>Bounding boxes are accumulated instead of suppressed<li>Only <em>one shared network for 3 functionalities</em><li>Uses a <em>feature extractor</em> for classification purpose<li>Use <em>offset to refine the resolution of the proposed windows</em><li>Detection fine-tuning: negative training on the fly</ul><h2 id="methodology">Methodology<a href="#methodology"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>decomposition into blocks with 3 offsets<li>for each block, estimation of the most probable corresponding class<li>(overlapping) region proposals for each class (see below)<li>bounding box deduction for each class (see below)</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/g6BS6RD.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/q9SZ6Rv.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/2Bdxt6e.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/LbgeFtC.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/3qVcyge.png" alt="" /></p><h1 id="the-mtcnn-face-detection-algorithm-2016">The MTCNN face detection algorithm (2016)</h1><h2 id="zhang-zhang--li">Zhang, Zhang &amp; Li<a href="#zhang-zhang--li"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Real-time deep-learning-based face detection algorithm<li>The MTCNN is a cascade of 3 similar networks (P/R/O-nets)<li>The four steps:<ol><li>Computation of the (multiscale) image pyramid<li>P-nEt: propositional network<li>R-net: refinement net (filters and refines the results of the P-Net)<li>O-net: output network (still refines, and propose landmarks)</ol></ul><p><img data-proofer-ignore data-src="https://i.imgur.com/gbmBdst.png" alt="" /></p><ul><li>Use <strong>hard sample mining</strong> (the $30\%$ easier cases fo not intervene in the retropropagation) to improve the detection results<li>Originality: uses <em>multi-task</em> learning, that is, every network<ul><li>predict bounding boxes<li>use regression to refine/calibrate the position of the edges of the bounding box<li>applies Non-Maxmal Suppression (NMS) to keep only relevant candidate windows (merge of highly overlapped candidates)<li>(can) propose 5 facial landmarks</ul><li>This multi-task seems to improve face detection compared to usual mono-task learning<li>How does it work in pratice? It minimizes:</ul>\[Loss=$\alpha_1\times L_{detection} + \alpha_2\times L_{regression} + \alpha_3\times L_{landmarks}$\]<p>where the first is based on cross-entropy, and the others are based on Euclidian loss</p><p><img data-proofer-ignore data-src="https://i.imgur.com/ntCzVfd.png" alt="" /></p><h1 id="fast-r-cnn-and-its-predecessors-2014-2015">Fast R-CNN and its predecessors (2014-2015)</h1><h2 id="spatial-pyramid-pool-network-2014">Spatial-Pyramid Pool network (2014)<a href="#spatial-pyramid-pool-network-2014"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Have been proposed to speed up R-CNN by sharing computation,<li>The SPPnet computes a shared feature map using convolutions over the entire image, and only then extract features corresponding to each proposal to make the prediction,<li>Then it concatenates the features of the proposal coming from each scale thanks to MaxPooling to a $6 \times 6 \times$ scales map (spatial pyramid).<li>SPP-nets accelerates R-CNN by 10 to 100 times at test times and by 3 at training time.<li>Drawback 1: Like the R-CNN, it is a multi-stage approach:<ul><li>First, feature extraction using convolution,<li>Second, fine-tuning of a network using log loss,<li>Third, SVM training,<li>Fourth, fitting bounding-box regressors.</ul><li>Drawback 2: Features are written to disk,<li>The fine-tuning cannot update the convolutional layers that precede the spatial pyramid pooling (limited accuracy).</ul><h2 id="fast-r-cnn0-2015">Fast R-CNN0 (2015)<a href="#fast-r-cnn0-2015"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>a Fast Region-based Convolutional Network method,</p></div><ul><li>Mainly made of several innovation to make is faster<li>Uses Singular Value Decomposition (SVD) truncation to fasten the computations,<li>Uses a multi-task loss to train all the network in one single stage (it jointly learns to classify objects proposals (windows) and refine their spatial locations),<li>Trains the VGG16 9 times faster than the RCNN and 3 times faster than the SPP-nets,<li>Is able to retropropagate the error in the convolutional layers (contrary to SPPnets and RCNN) and then increases the accuracy,<li>No disk storage is required for feature caching.</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/GSOTYDB.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/d7Vkumr.png" alt="" /></p><h1 id="faster-r-cnn-2016">Faster R-CNN (2016)</h1><div class="alert alert-info" role="alert"><ul><li>Usual object detection methods depended on (slow) region proposal algorithms,<li>They got the original idea to use ANN’s to do these predictions on GPU (much faster),<li>They called this technology Region Proposal Networks (RPNs).</ul></div><h2 id="properties">Properties<a href="#properties"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>is just made of several convolutional layers applied on the feature maps,<li>It is then a fully convolutional layer (weights are shared in space),<li>It is then translation-invariant in space (contrary to MultiBox method),<li>it can be seen as a mini-network with a sliding-window applied on the feature map to predict proposals,<li>predicts at the same time proposals using regression and objectness scores,<li>is able to predict proposals with a wide range of scales and aspect ratios (bye default, 3 and 3 respectively).</ul><div class="alert alert-success" role="alert"><p>Since the Fast R-CNN does not have region proposal, they added their RPN before the Fast-RCNN to obtain the Faster R-CNN,</p></div><ul><li>The RPN is then an <strong>attention network</strong> since it tells to the Fast R-CNN where to look<li>Since the efficiency of the Fast R-CNN depends on the region proposals, better proposal thanks to the RPN implies a better accuracy of the Faster R-CNN,<li>To ensure that features used between the RPN and the Fast R-CNN are the same, they shared the weights of the Feature Extractor between them (faster, more accurate).<li>It took then 10 milliseconds to compute the predictions of the RPN.</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/zoZ9vtf.png" alt="" /></p><h1 id="mask-r-cnn-2018">Mask R-CNN (2018)</h1><div class="alert alert-info" role="alert"><p>Extension of Faster R-CNN</p></div><div class="alert alert-danger" role="alert"><p>aim is <strong>instance segmentation</strong></p></div><p>Has 3 outputs/prediction</p><ol><li>the usual bounding box predictions (from Faster R-CNN),<li>the usual classification predictions (still from Faster R-CNN),<li>the mask predictions (A small FCN applied to each RoI – NEW !!),</ol><p><em>No competition</em> is done among classes prediction</p><ul><li>Mask prediction is done <em>in parallel</em><li>The training is done with a multi-task loss:</ul>\[Loss = \alpha_1L_{class} + \alpha_2L_{reg}+\alpha_3L_{mask}\]<ul><li>We can easily change the backbone (feature extractor)<li>It runs a 5 fps</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/BRmLTQK.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/1xomgWc.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/VXdyxTt.png" alt="" /></p><h1 id="r-fcn-architectures-2016">R-FCN Architectures (2016)</h1><div class="alert alert-info" role="alert"><p>Region-based Fully Convolutional Networks</p></div><ul><li>2-stage object detection strategy<li>Every layer is convolutional, whatever its role<li>Almost all the computations are shared on the entire image<li>Rols (candidate regions) are extracted to a Region Proposal Network (RPN)<li>Uses position-sensitive score maps</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/MkSJ4sw.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/EoDV021.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/GwQyIok.png" alt="" /></p><p>On decale la fenetre sur la droite:</p><p><img data-proofer-ignore data-src="https://i.imgur.com/zfNAKy8.png" alt="" /></p><blockquote><p>At top-middle probability map, the white pixels correspond to the probability that a head</p></blockquote><h1 id="retinanet-2018">RetinaNet (2018)</h1><ul><li>One-stage detector<li>Uses an <em>innovative focal loss</em><li>Naturally handles <em>class imbalance</em><li>Uses a <strong>Feature Pyramid Network</strong> (FPN) backbone of ResNet architecture<li>Then it provides a <em>rich</em> multi-scale feature pyramid (efficiency)<li>At each scale, they attach <em>subnetworks</em> to classify and make regressions</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/Urr3Vbx.png" alt="" /></p><h1 id="detectrons-2018-2019">Detectrons (2018-2019)</h1><h2 id="detectron-v1-2018-facebook">Detectron V1 2018 (Facebook)<a href="#detectron-v1-2018-facebook"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/Hwu5RhV.png" alt="" /></p><h2 id="detectron--v2-facebook">Detectron V2 (Facebook)<a href="#detectron--v2-facebook"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/GqBOC9W.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/EEB6f2K.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/cblq1WV.png" alt="" /></p><h1 id="real-time-detection-algorithms">Real-time detection algorithms</h1><h2 id="yolo-you-only-look-once-2016">YOLO (You Only Look Once) (2016)<a href="#yolo-you-only-look-once-2016"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>single-shot detection architecture<ul><li>Designed for real-time applications<li>It does NOT predict regions of interests<li>It predicts a fixed amount of detections on the image directly,<li>They are then filtered to contain only the actual detections.</ul><li>faster than region-based architectures<li>lower detection accuracy<li>performs a multi-box bounding box regression on the input image directly<li>Method: the image is overlayed by a grid, and for each grid cell, a fixed amount of detections are predicted.</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/Np8qiEY.png" alt="" /></p><h2 id="ssd-single-shot-multibox-detector-2016">SSD (Single Shot Multibox Detector) (2016)<a href="#ssd-single-shot-multibox-detector-2016"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Is a single-shot detection architecture<li>Instead of performing bounding box regression on the final layer like YOLO, SSDs append additional convolutional layers that gradually decrease in size.<li>For each additional layer, a fixed amount of predictions with diverse aspect ratios are computed,<li>It results in a large number of predictions that differ heavily across size and aspect ratio.</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/ZQIztkw.png" alt="" /></p><h2 id="yolov2-yolo-9000-2016">YOLOv2 (YOLO 9000) (2016)<a href="#yolov2-yolo-9000-2016"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Extension of YOLOv1<li>Ability to predict objects at different resolutions,<li>Computes the first bounding box predictions using clustering,<li>Better performance than SSD.</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s9/'>Image S9</a>, <a href='/cours/categories/dlim/'>DLIM</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/s9/" class="post-tag no-text-decoration" >S9</a> <a href="/cours/tags/dlim/" class="post-tag no-text-decoration" >DLIM</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DLIM: Face detection - Cours&url=https://lemasyma.github.io/cours/posts/dlim_face_detection/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DLIM: Face detection - Cours&u=https://lemasyma.github.io/cours/posts/dlim_face_detection/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=DLIM: Face detection - Cours&url=https://lemasyma.github.io/cours/posts/dlim_face_detection/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/dlim_deepfake/"><div class="card-body"> <em class="timeago small" date="2021-11-29 14:00:00 +0100" >Nov 29, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DLIM: Deepfake</h3><div class="text-muted small"><p> Lien de la note Hackmd Deepfake Les GAN permettent de creer de faux surprenat avec des reseaux profonds (d’ou Deepfakes) Creation de visage (StyleGAN) Fausse video comme celle de Poutine so...</p></div></div></a></div><div class="card"> <a href="/cours/posts/dlim_res_neurones_convolutif/"><div class="card-body"> <em class="timeago small" date="2021-11-08 14:00:00 +0100" >Nov 8, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DLIM: Reseaux neuronaux convolutifs</h3><div class="text-muted small"><p> Lien de la note Hackmd Un reseau de neurones convolutif Le but est d’extraire les caracteristiques Les formules de convolution Continue 1D: [(f * g)(x) = \int_{-\infty}^{+\infty}f(x-t)g(t)...</p></div></div></a></div><div class="card"> <a href="/cours/posts/imed2_tp3/"><div class="card-body"> <em class="timeago small" date="2021-11-25 09:00:00 +0100" >Nov 25, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IMED2: TP3</h3><div class="text-muted small"><p> Reconstruction tomographique (2/3) Vous avez tous les outils pour comprendre la reconstruction tomographique 2D en géométrie parallèle dans le cadre idéal : à partir des lignes intégrales (transfo...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/tvid_image_ecran/" class="btn btn-outline-primary" prompt="Older"><p>TVID: De l'image a l'ecran</p></a> <a href="/cours/posts/tvid_codec/" class="btn btn-outline-primary" prompt="Newer"><p>TVID: Codec Video</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
