<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="IML: Dimensionality reduction" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Dimensionality reduction" /><meta property="og:description" content="Dimensionality reduction" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-03-19T13:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="IML: Dimensionality reduction" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-03T16:45:54+02:00","datePublished":"2021-03-19T13:00:00+01:00","description":"Dimensionality reduction","headline":"IML: Dimensionality reduction","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/"},"url":"https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/"}</script><title>IML: Dimensionality reduction | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>IML: Dimensionality reduction</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>IML: Dimensionality reduction</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-03-19 13:00:00 +0100" data-toggle="tooltip" data-placement="bottom" title="Fri, Mar 19, 2021, 1:00 PM +0100" >Mar 19, 2021</em> </span> <span> Updated <em class="timeago" date="2021-10-03 16:45:54 +0200 " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 3, 2021, 4:45 PM +0200" >Oct 3, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1417 words"> <em>7 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Lien de la <a href="https://hackmd.io/@lemasymasa/H1ZhGffNO">note Hackmd</a></p><h1 id="why-do-we-care-">Why do we care ?</h1><p>We have at hand $n$ points $x1,…, xn$ lying in some N-dimensional space, $x_i \in\mathbb R^n , \forall i = 1, . . . , n,$ compactly written as a $n × N$ matrix $X$</p><ul><li>One row of $X$ = one sample<li>One column of $X$ = a given feature value for all samples</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/CmbXcfy.png" alt="" /></p><h2 id="example-of-real-high-dimensional-data">Example of real high-dimensional data<a href="#example-of-real-high-dimensional-data"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-warning" role="alert"><p>Real world data is very often high-dimensional</p></div><h3 id="mnist-image-classification">MNIST image classification:<a href="#mnist-image-classification"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/psXUjPa.png" alt="" /></p><ul><li>Sample $x$:image with 28x28 pixels<li>Data set: 60000 samples<li>Dimensionality: $x \in\mathbb R^{28×28=784}$</ul><h3 id="muse-hyperspectral-image-analysis">MUSE hyperspectral image analysis:<a href="#muse-hyperspectral-image-analysis"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/afKNwIT.png" alt="" /></p><ul><li>Sample $x$: pixel with 3600 spectral bands<li>Data set: image with 300x300 pixels<li>Dimensionality: $x \in \mathbb R^{3600}$</ul><blockquote><p>Pour discriminer les galaxies <del>c’est raciste ca monsieur</del></p></blockquote><h2 id="the-curse-of-dimensionality">The curse of dimensionality<a href="#the-curse-of-dimensionality"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-danger" role="alert"><p>High-dimensional spaces <del>suck donkey ballz</del> suffer from the <em>curse of dimensionality</em> (also called Hughes’ phenomenon)</p></div><h3 id="sur-mathbb-r">Sur $\mathbb R$<a href="#sur-mathbb-r"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/vesSPAR.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/D1xI2ST.png" alt="" /></p><h3 id="sur-mathbb-r2">Sur $\mathbb R^2$<a href="#sur-mathbb-r2"><i class="fas fa-hashtag"></i></a></h3></h3><p>Revenir a la meme densite d’echantillonage: <img data-proofer-ignore data-src="https://i.imgur.com/jRNkOX5.png" alt="" /></p><h3 id="sur-mathbb-r3">Sur $\mathbb R^3$<a href="#sur-mathbb-r3"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/7rK9gAn.png" alt="" /></p><p>Revenir a la meme densite d’echantillonage: <img data-proofer-ignore data-src="https://i.imgur.com/PUKy7sm.png" alt="" /></p>\[\frac{\nu(\mathbb S^n)}{\nu([-1;1]^n)}=\frac{\pi^{\frac{n}{2}}}{2\Gamma(\frac{n}{2}+1)}\to_{n\to+\infty}0\]<div class="alert alert-warning" role="alert"><p>Points uniformly distributed in a $n$−cube of side 2 mostly fall outside of the unit sphere!</p><p><img data-proofer-ignore data-src="https://i.imgur.com/rh4aYb4.png" alt="" /></p></div><h1 id="why-is-it-tricky">Why is it tricky?</h1><ul><li>We naturally cannot picture anything that is more than 3D in our mind</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/JLWfSrR.png" alt="" /></p><ul><li>Picturing something 3D in a 2D flat screen can already be misleading<li>Real data naturally lives in (complex) high-dimensional space<li>Real data is often strongly correlated</ul><div class="alert alert-warning" role="alert"><p>And somehow, we want to have a good look to our data before feeding it to some machine learning algorithm <em>(can I use the inherent structure of my data to pimp my machine learning performances?)</em></p></div><h1 id="how-">How ?</h1><div class="alert alert-info" role="alert"><p>Dimensionality reduction: transform data set $X$ with dimensionality $N$ into a new data set $Y$ ($n \times M$ matrix) with dimensionality $M \lt N$ (hopefully $M \lt\le N$) such that as <strong>few information as possible is lost</strong> in the process. $y_i$ ($i$th row of $Y$) is the low-dimensional counterpart <em>(the projection)</em> of $x_i$.</p></div><p><strong>INFORMATION ???</strong></p><p><img data-proofer-ignore data-src="https://i.imgur.com/OYfsA0d.png" alt="" /></p><h1 id="linear-approaches">Linear approaches</h1><p>Somehow trying to find a low-dimensional subspace in which the projected data would not be too much distorted after projection.</p><ul><li>Johnson-Lindenstrauss lemma<li>Classical scaling<li><em>(The one and only)</em> Principal Component Analysis<li>And much more…</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/dugYCAS.png" alt="" /></p><h2 id="johnson-lindenstrauss-lemma">Johnson-Lindenstrauss lemma<a href="#johnson-lindenstrauss-lemma"><i class="fas fa-hashtag"></i></a></h2></h2><p>It’s not because you <em>can</em> that you <em>will</em></p><div class="alert alert-danger" role="alert"><p>Let $0\lt\varepsilon\lt1$ and let $x_1,…,x_n$ be $n$ points in $\mathbb R^N$. Then there exists a linear map $f:\mathbb R^N\to\mathbb R^M$ such that for every points $x_i$ and $x_j$</p>\[(1-\varepsilon)\Vert x_i-x_j \Vert^2\le \Vert f(x_i)-f(x_j) \Vert^2\le(1+\varepsilon)\Vert x_i-x_j \Vert^2\]<p>With $M=\frac{4\log(n)}{(\frac{\varepsilon^2}{2} − \frac{\varepsilon^3}{3}).}$</p><blockquote><p>Johnson, W. B., &amp; Lindenstrauss, J. (1984). Extensions of Lipschitz mappings into a Hilbert space. Contemporary mathematics.</p></blockquote></div><p><img data-proofer-ignore data-src="https://i.imgur.com/w1g2pBu.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>La douille: il faut trouver la matrice $M$.</p></div><h2 id="classical-scaling">Classical scaling<a href="#classical-scaling"><i class="fas fa-hashtag"></i></a></h2></h2><p>Also called Principal Coordinates Analysis (PCoA)</p><blockquote><p>Lots of formula here, but you just need to retain the overall idea</p></blockquote><div class="alert alert-danger" role="alert"><p>PCoA: project data points $X$ onto $Y$ with a linear mapping $M$ such that $Y = XM$ such that all pairwise distances between points do not change too much before/after projection</p></div><p>If $D$ is the $n \times n$ Euclidean distance matrix with entries $d_{ij} = \Vert x_i − xj\Vert_2$ and $D^{(2)} = [d_{ij}^2]$, PCoA seeks the linear mapping $M$ that minimizes</p>\[\phi(Y)=\sum_{i,j}(d_{ij}^2-\Vert y_i-y_j\Vert^2)\]<p>with $y_i = x_iM$ and $\Vert m_i\Vert^2=1\forall i$</p><div class="alert alert-success" role="alert"><p>Solution: eigendecomposition (=diagonalisation) of the Gram matrix $K = XX^T = E\Delta E$</p></div><p>$K$ can be obtained by double centering $D^{(2)}:K=-\frac{1}{2}C_nD^{(2)}C_n$ with centering matrix $C_n=I_n-\frac{1}{n}ones(n,n)$</p><p>Optimal projection onto the first $M$ dimensions $Y=\Delta_M^{\frac{1}{2}}E_M^T$ with $E_M$ matrix of the $M$ largest eigenvectors of $E$.</p><h2 id="principal-component-analysis">Principal component analysis<a href="#principal-component-analysis"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Also known as the Karhunen-Loeve transform</p></blockquote><p>Closely related to PCoA, but operates on the covariance matrix $X_c^T X_c$ PCA seeks the linear mapping $M$ that maximizes the projection variance $tr(M^T cov(X)M)$ with $\Vert mi\Vert^2 = 1 \forall i$.</p>\[X=\begin{bmatrix} \overbrace{x_{11}}^{u_1=\text{moyenne}} &amp; \overbrace{x_{12}}^{u_2}\\ \vdots &amp; \vdots\\ x_{n1}&amp;x_{n2} \end{bmatrix} \Rightarrow \text{centrage des donnees}\] \[X_c=\begin{bmatrix} x_{11}-u_1 &amp; x_{12}-u_2\\ \vdots &amp; \vdots\\ x_{n1}1-u_1&amp;x_{n2}-u_2 \end{bmatrix}\]<ol><li>Center the data $X_c = C_nX$ 1.b (opt) Reduce the data<li>Compute covariance matrix $\sum=\frac{1}{n-1}X_c^TX_c$<li>Perform eigendecomposition $(E,\Delta)$ of $\sum$<li>Project on the first $M$ principal axes $Y=XE_M$</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/WJcHD4e.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/Kw4ZzPC.png" alt="" /></p><p>Data after projection is uncorrelated, but has lost some interpretability</p><p><img data-proofer-ignore data-src="https://i.imgur.com/93SANtL.png" alt="" /></p><h2 id="major-challenges-related-to-pca">Major challenges related to PCA<a href="#major-challenges-related-to-pca"><i class="fas fa-hashtag"></i></a></h2></h2><p>PCA is probably the most popular and used unsupervised linear dimensionality reduction technique, but it comes with a bunch of operability questions, the 2 principles being:</p><ol><li>How to automatically select the right number of dimensions to project?<ul><li><img data-proofer-ignore data-src="https://i.imgur.com/EmehUrr.png" alt="" /> <img data-proofer-ignore data-src="https://i.imgur.com/2Kgc8Gj.png" alt="" /></ul><li>How to project a new data point on a learned projection subspace?<ul><li>See you in lab session for the answer</ul></ol><h1 id="non-linear-approaches">Non-linear approaches</h1><p>When it is assumed that the data does not live in an Euclidean subspace (why would it anyway?), some more advanced techniques must be relied on.</p><ul><li>Isomap<li>Locally linear embedding<li>Kernel Principal Component Analysis (aka PCA on steroids)<li>Multilayer autoencoders<li>And much more…</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/EEegGRS.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/r8hJK6Z.png" alt="" /></p><h2 id="isomap">Isomap<a href="#isomap"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Geodesic distance rocks</p></blockquote><p>Isometric feature mapping: same idea as classical scaling, but using geodesic distance instead of Euclidean distance.</p><p><img data-proofer-ignore data-src="https://i.imgur.com/OlSpeIx.png" alt="" /> <img data-proofer-ignore data-src="https://i.imgur.com/fBSAufM.png" alt="" /></p><ol><li>Compute k-nearest neighbor graph of data $x_1,…,x_n$<li>Compute all pairwise geodesic distances<li>Apply classical scaling</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/qc77r5y.png" alt="" /></p><h3 id="exemple">Exemple<a href="#exemple"><i class="fas fa-hashtag"></i></a></h3></h3><p>Isomap applied to some images of the digit 2 in MNIST data</p><p><img data-proofer-ignore data-src="https://i.imgur.com/4wnzodY.png" alt="" /></p><h2 id="locally-linear-embedding">Locally linear embedding<a href="#locally-linear-embedding"><i class="fas fa-hashtag"></i></a></h2></h2><p>Locally linear embedding: the manifold can be locally considered Euclidean</p><p><img data-proofer-ignore data-src="https://i.imgur.com/cz5QFIk.png" alt="" /></p><p>For each point $x_i$:</p><ol><li>get its k-nearest neighbors $x_j$, $j=1,…,k$<li>Get weights $w_{ij}$ that best linearly reconstruct $x_i$ with $x_j$: minimize $\sum_{i=1}^n\Vert x_i-\sum w_{ij}x_j\Vert$ <img data-proofer-ignore data-src="https://i.imgur.com/9r9RLAB.png" alt="" /><ul><li>with constraints $\sum w_{ij}=1$ (closed-form solution)</ul><li>Low-dimensional embedding $\to$ reconstruct $y_i$ with $y_j$ and same weights $w_{ij}$:</ol><p>minimize \(\sum_{i=1}^n\Vert y_i-\sum w_{ij}y_j\Vert\)</p><p>with constraints $\frac{1}{n}\sum_iy_iy_i^T$ and $\sum_iy_i=0$ (eigendecomposition of a Gram matrix)</p><h2 id="the-kernel-trick">The kernel trick<a href="#the-kernel-trick"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>When one actually wants to increase the dimension</p></blockquote><p>Base idea: map $n$ non linearly separable points to a (possibly infinite) space where they would be with a function $\phi$</p><ul><li>How should we define $\phi$ ?<li>Do we really want to compute stuff in a (possibly infinite) feature space?</ul><div class="alert alert-info" role="alert"><p>Mercer theorem: we do not need to know the mapping $\phi$ explicitly as long as we have a positive semi-definite kernel/Gram matrix $K=[\mathcal k(x_i,x_j)]=[&lt;\phi(x_i),\phi(x_j)&gt;]$</p></div><p>Widely used kernel functions:</p><ul><li>Polynomial kernel: $\mathcal k(x_i,x_j)=(x_i^Tx_j+1)^d$<li>Gaussian RBF kernel: $\mathcal k(x_i,x_j)=e^{-\gamma\Vert x_i-x_j\Vert^2}$<li>Sigmoid kernel: $\mathcal k(x_i,x_j)=\tanh(bx_i^Tx_j+c)$</ul><h2 id="kernel-pca">Kernel PCA<a href="#kernel-pca"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>PCA on steroids</p></blockquote><p>The maths behind are quite hard, but the following scikit-learn recipe works fine:</p><ol><li>Compute kernel matrix $k=[\mathcal k(x_i,x_j)]=[&lt;\phi(x_i),\phi(x_j)&gt;]$ and double-center it $K_c=C_nKC_n$<li>Eigendecomposition of $K_c$ is strongly related to this of the (intractable) covariance matrix in the feature space $\to$ get eigenvectors $V$ and corresponding eigenvalues $\Delta$ of $K_c$. <img data-proofer-ignore data-src="https://i.imgur.com/Mcio2jt.png" alt="" /><li>Keep the first $M$ columns of $\sqrt{\Delta V}$ to get the coordinates of projected data points in the low $M$-dimensional space. <img data-proofer-ignore data-src="https://i.imgur.com/OkbLVXa.png" alt="" /></ol><p>But things get nasty when one wants to project a new data point $x$ that was not known when constructing the kernel…</p><h2 id="non-linear-pca">Non-linear PCA<a href="#non-linear-pca"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p>Also known as autoencoder</p></blockquote><p>Overall idea:</p><ol><li>train an autoencoder (neural network with an autoassociative architecture) to perform an identity mapping.<li>use the output of the bottleneck layer as low-dimensional code.</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/s1fuRi5.png" alt="" /></p><p>Bottleneck code is a non-linear combination of entries (thanks to activation functions on the encoder layers) $\to$ learned mapping is a non-linear PCA. <img data-proofer-ignore data-src="https://i.imgur.com/rlaEDkG.png" alt="" /></p><p>Principal components are generalized from straight lines to curves: the projection subspace which is described by all nonlinear components is also curved.</p><p><img data-proofer-ignore data-src="https://i.imgur.com/CtdC82G.png" alt="" /></p><h1 id="lets-recap">Let’s recap</h1><p>High-dimensional data set $X$ is a $n \times N$ matrix, with $n =$ number of samples and $N =$ dimensionality of underlying space.</p><p><img data-proofer-ignore data-src="https://i.imgur.com/tSi5kMZ.png" alt="" /></p><ul><li>Parametric $\equiv$ explicit embedding from high-dimensional space to low-dimensional one<li>For LLE: $p$ is the ratio of non-zero elements in a sparse matrix to the total number of elements<li>For NL-PCA: $i$ is the number of iterations and w is the number of weights in the neural network</ul><h2 id="t-distributed-stochastic-neighbor-embedding">t-Distributed Stochastic Neighbor Embedding<a href="#t-distributed-stochastic-neighbor-embedding"><i class="fas fa-hashtag"></i></a></h2></h2><p>t-SNE is a popular method to see in 2D or 3D wtf is going on in a high-dimensional spaces.</p><ol><li>Construct a probability distribution $p$ over pairs of points in the high-dim space: the more similar (the closer) the two points, the higher the probability<li>Define a second probability distribution $q$ over the points in the low-dim space, and dispatch the points such that the distance between p and q in minimized (for the KullbackLeibler divergence)</ol><p><img data-proofer-ignore data-src="https://i.imgur.com/4645qRQ.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/b4E3cZB.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/PYnRtgM.png" alt="" /></p><ul><li>t-SNE is excellent in visualizing the well-separated clusters, but fails to preserve the global geometry of the data.<li>t-SNE depends on a perplexity parameter, which reflects the scale of search for close points.</ul><h2 id="independant-component-analysis">Independant component analysis<a href="#independant-component-analysis"><i class="fas fa-hashtag"></i></a></h2></h2><p>ICA aims to provide a solution to the so-called <em>cocktail party</em>: retrieving independent sources that got mixed-up together with unknown scaling coefficients.</p><p><img data-proofer-ignore data-src="https://i.imgur.com/2QEhLKX.png" alt="" /></p><p>Goal: estimate source $s$ <strong>and</strong> mixing matrix $A$ from observation $x = As$.</p><ul><li>Ill-posed $\Rightarrow$ enforce independence on source components<li>Work on higher order statistics (PCA limits to order-2 statistics)<li>Unkown source must <strong>not</strong> be Gaussian-distributed</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/KyAc8m2.png" alt="" /></p><p>Contrarily to PCA vectors, ICA vectors are not orthogonal and not ranked by importance, but they are mutually independents.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s8/'>Image S8</a>, <a href='/cours/categories/iml/'>IML</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/scia/" class="post-tag no-text-decoration" >SCIA</a> <a href="/cours/tags/iml/" class="post-tag no-text-decoration" >IML</a> <a href="/cours/tags/s8/" class="post-tag no-text-decoration" >S8</a> <a href="/cours/tags/principal-component-analysis/" class="post-tag no-text-decoration" >principal component analysis</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=IML: Dimensionality reduction - Cours&url=https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=IML: Dimensionality reduction - Cours&u=https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=IML: Dimensionality reduction - Cours&url=https://lemasyma.github.io/cours/posts/iml_dimensionality_reduction/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/iml_introduction/"><div class="card-body"> <em class="timeago small" date="2021-03-17 11:00:00 +0100" >Mar 17, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IML: Introduction</h3><div class="text-muted small"><p> Lien de la note Hackmd Motivation What is learning ? It’s all about evolving Definition Learning: Improver over experience to perform better in new situations. Quoting S. Bengio Learning ...</p></div></div></a></div><div class="card"> <a href="/cours/posts/iml_dimensionality_clustering/"><div class="card-body"> <em class="timeago small" date="2021-04-02 13:00:00 +0200" >Apr 2, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IML: Unsupervised clustering</h3><div class="text-muted small"><p> Lien de la note Hackmd Why do we care Group the input data into clusters that share some characteristics Find pattern in the data (data mining problem) Visualize the data in a simpler w...</p></div></div></a></div><div class="card"> <a href="/cours/posts/iml_supervised_learning/"><div class="card-body"> <em class="timeago small" date="2021-04-16 14:00:00 +0200" >Apr 16, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IML: Supervised learning</h3><div class="text-muted small"><p> Lien de la note Hackmd Supervised learning Supervised learning: process of teaching a model by feeding it input data as well as correct output data. The model will (hopefully) deduce a correct...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/ocvx_partie_r_convexite_/" class="btn btn-outline-primary" prompt="Older"><p>OCVX: Parties de R et convexite</p></a> <a href="/cours/posts/prst_seance_4/" class="btn btn-outline-primary" prompt="Newer"><p>PRST: Seance 4 - Intervalle de confiance</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
