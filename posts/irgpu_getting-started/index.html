<!DOCTYPE html><html lang="fr-FR" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="pv-proxy-endpoint" content="https://cours-v2.ew.r.appspot.com/query?id=agplfmNvdXJzLXYychULEghBcGlRdWVyeRiAgIDo14eBCgw"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="IRGPU: Getting started with CUDA" /><meta property="og:locale" content="fr_FR" /><meta name="description" content="Getting started with CUDA" /><meta property="og:description" content="Getting started with CUDA" /><link rel="canonical" href="https://lemasyma.github.io/cours/posts/irgpu_getting-started/" /><meta property="og:url" content="https://lemasyma.github.io/cours/posts/irgpu_getting-started/" /><meta property="og:site_name" content="Cours" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-05-03T14:00:00+02:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="IRGPU: Getting started with CUDA" /><meta name="google-site-verification" content="mErcVOJcxNzULHvQ99qSclI_DTX0zANqgpsd3jGhkfs" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-10-03T16:45:54+02:00","datePublished":"2021-05-03T14:00:00+02:00","description":"Getting started with CUDA","headline":"IRGPU: Getting started with CUDA","mainEntityOfPage":{"@type":"WebPage","@id":"https://lemasyma.github.io/cours/posts/irgpu_getting-started/"},"url":"https://lemasyma.github.io/cours/posts/irgpu_getting-started/"}</script><title>IRGPU: Getting started with CUDA | Cours</title><link rel="apple-touch-icon" sizes="180x180" href="/cours/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/cours/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/cours/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/cours/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/cours/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cours"><meta name="application-name" content="Cours"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/cours/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cours-v2.ew.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://cours-v2.ew.r.appspot.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/cours/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/cours/" alt="avatar" class="mx-auto"> <img src="/cours/assets/img/favicons/logo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/cours/">Cours</a></div><div class="site-subtitle font-italic">Certains cours sont en francais, certains sont en anglais, d'autres sont un mix des deux</div></div><ul class="w-100"><li class="nav-item"> <a href="/cours/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/cours/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/cours/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/cours/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/cours/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/lemasyma" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="" aria-label="" target="_blank" rel="noopener"> <i class=""></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/cours/"> Home </a> </span> <span>IRGPU: Getting started with CUDA</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>IRGPU: Getting started with CUDA</h1><div class="post-meta text-muted"><div> By <em> <a href="https://github.com/lemasyma">lemasyma</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2021-05-03 14:00:00 +0200" data-toggle="tooltip" data-placement="bottom" title="Mon, May 3, 2021, 2:00 PM +0200" >May 3, 2021</em> </span> <span> Updated <em class="timeago" date="2021-10-03 16:45:54 +0200 " data-toggle="tooltip" data-placement="bottom" title="Sun, Oct 3, 2021, 4:45 PM +0200" >Oct 3, 2021</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1647 words"> <em>9 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p>Lien de la <a href="https://hackmd.io/@lemasymasa/rkfdcPavd">note Hackmd</a></p><h1 id="cuda-overview">CUDA overview</h1><h2 id="what-is-cuda-">What is CUDA ?<a href="#what-is-cuda-"><i class="fas fa-hashtag"></i></a></h2></h2><p>A product</p><ul><li>It enables to use NVidia GPUs for computation</ul><p>A C/C++ variant</p><ul><li>Mostly C++ 15 compatible, with extensions<li>and also some restrictions !</ul><p>A SDK</p><ul><li>A set of compilers and toolchains for various architectures<li>Performance analysis tools</ul><p>A runtime</p><ul><li>An assembly specification<li>Computation libraries (linear algebra, etc.)</ul><p>A new industry standard</p><ul><li>Used by every major deep learning framework<li>Replacing OpenCL as Vulka is replacing OpenGL</ul><h2 id="the-cuda-ecosystem-2021">The CUDA ecosystem (2021)<a href="#the-cuda-ecosystem-2021"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/jyZ9il1.png" alt="" /></p><h2 id="libraries-or-compiler-directives-or-programming-language-">Libraries <em>or</em> Compiler Directives <em>or</em> Programming Language ?<a href="#libraries-or-compiler-directives-or-programming-language-"><i class="fas fa-hashtag"></i></a></h2></h2><p>CUDA is mostly based on a “new” <strong>programming language</strong>: CUDA C (or C++, or Fortran)</p><blockquote><p>This grants much flexibility and performance</p></blockquote><p>But is also exposes much of GPU goodness through <strong>libraries</strong></p><p>An it supports a few <strong>compiler directives</strong> to facilitate some constructs</p><h2 id="the-big-idea-kernels-instead-of-loops">The big idea: Kernels instead of loops<a href="#the-big-idea-kernels-instead-of-loops"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/MomimT7.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/RR1Uc8A.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/2zpoK0Z.png" alt="" /></p><div class="alert alert-success" role="alert"><p>No more <code class="language-plaintext highlighter-rouge">for</code> loop !</p></div><h2 id="arrays-of-parallel-threads">Arrays of parallel threads<a href="#arrays-of-parallel-threads"><i class="fas fa-hashtag"></i></a></h2></h2><p>A CUDA kernel is executed by a <strong>grid</strong> (array) of threads</p><ul><li>All threads in grid run the same kernel code (Single Program Mutliple Data)<li>Each thread has indexes that is used to compute memory addresses and compute decisions</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/piRxw0Q.png" alt="" /></p><h2 id="threads-blocks">Threads blocks<a href="#threads-blocks"><i class="fas fa-hashtag"></i></a></h2></h2><p>Threads are grouped into thread blocks</p><ul><li>Threads witihin a bloc cooperate via<ul><li><em>shared memory</em><li><em>atomic operations</em><li><em>barrier synchronization</em></ul><li>Threads in different blocks do not interact</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/tQmgcy8.png" alt="" /></p><h3 id="a-multidimensional-grid-of-computation-threads">A multidimensional grid of computation threads<a href="#a-multidimensional-grid-of-computation-threads"><i class="fas fa-hashtag"></i></a></h3></h3><p>Each thread uses indices to decide what data to work on: <img data-proofer-ignore data-src="https://i.imgur.com/IYWVCfx.png" alt="" /></p><p>Each index has $x$, $y$ and $z$ attributes</p><p>Grid and blocks can have different dimensions, but they usually are 2 levels of the same work decomposition</p><p><img data-proofer-ignore data-src="https://i.imgur.com/UzThWvv.png" alt="" /></p><h3 id="examples">Examples<a href="#examples"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/jI0Buab.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/j5pp4Yl.png" alt="" /></p><h2 id="block-decomposition-enable-automatic-scalability">Block decomposition enable automatic scalability<a href="#block-decomposition-enable-automatic-scalability"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/J2dsHmg.png" alt="" /></p><h1 id="architecture">Architecture</h1><h2 id="programming-modeling">Programming modeling<a href="#programming-modeling"><i class="fas fa-hashtag"></i></a></h2></h2><p><strong>Block</strong> A set of <em>threads</em> that cooperate:</p><ul><li>Synchronisation<li>Shared memory<li>Block ID = ID in a <strong>grid</strong></ul><p><strong>Grid</strong> Array of blocks executing same <em>kernel</em></p><ul><li>Access to global GPU memory<li>Sync. by stop and start a new kernel</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/w77cxAe.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/Ol0ccWU.png" alt="" /></p><h2 id="mapping-programming-model-to-hardware">Mapping Programming model to hardware<a href="#mapping-programming-model-to-hardware"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/PpSdlGS.png" alt="" /></p><h3 id="the-sms">the SMs<a href="#the-sms"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/ECGCbjm.png" alt="" /></p><h2 id="zoom-on-the-sm">Zoom on the SM<a href="#zoom-on-the-sm"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/fIOI7dS.png" alt="" /></p><div class="alert alert-info" role="alert"><p><em>warp</em>: 32 unites de calcul</p></div><ul><li>SM organize <strong>blocks</strong> into <strong>warps</strong><li>1 warp = group of 32 threads</ul><p>GTX 920:</p><ul><li>128 cores = 4 x 32 cores<li>Quad warp scheduler selects 4 warps (TLP)<li>And 2 independant instructions per warp can be dispatched each cycle (ILP)</ul><blockquote><p>Ex: 1 (logical) <em>block</em> of 96 threads maps to: 3 (physical) <em>warps</em> of 32 threads</p></blockquote><h2 id="zoom-on-the-cuda-cores">Zoom on the CUDA cores<a href="#zoom-on-the-cuda-cores"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/aNJ0IX9.png" alt="" /></p><ul><li>A <em>warp</em> executes 32 threads on the 32 CUDA cores<li>The threads executes <em>the same</em> instructions (<em>DLP</em>)<li>All instructions are SIMD (width = 32) instructions</ul><p>Each core:</p><ul><li>FLoating point &amp; integer unit<li>Fused multiply-add (FMA) instruction<li>Logic unit<li>Move, compare unit<li>Branch unit<li>The first IF/ID of the pipeline is done by the SM</ul><div class="alert alert-warning" role="alert"><p>SIMT allows to specify the execution</p></div><h2 id="the-simt-execution-model-on-cuda-cores">The SIMT Execution Model on CUDA cores<a href="#the-simt-execution-model-on-cuda-cores"><i class="fas fa-hashtag"></i></a></h2></h2><p>SIMT: on programme comme si on avait un thread qui execute une donnees mais ca se cache derriere des instructions SIMD (a + b devient la somme du vecteur a avec le vecteur b)</p><div class="alert alert-danger" role="alert"><p>Chaque thread va executer le meme kernel et instructions</p></div><ul><li>Divergent code paths (branching) pile up!</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/HhwTHnQ.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/V8hUjR1.png" alt="" /></p><p>If/else: tous les threads vont effectuer en meme temps le if et else</p><h3 id="ifelse">If/else<a href="#ifelse"><i class="fas fa-hashtag"></i></a></h3></h3><p>What is the latency of this code in the best and worst case ?</p><p><img data-proofer-ignore data-src="https://i.imgur.com/1P8XxsN.png" alt="" /></p><ul><li>Best case: $a\gt0$ is false for every thread. For all threads: <code class="language-plaintext highlighter-rouge">inst-d</code><li>Worst case: $a\gt0$ and $b\gt0$ is true for some but not all threads. For all threads: <code class="language-plaintext highlighter-rouge">inst-a</code>, <code class="language-plaintext highlighter-rouge">inst-b</code>, <code class="language-plaintext highlighter-rouge">inst-c</code>, <code class="language-plaintext highlighter-rouge">inst-d</code></ul><h3 id="loops">Loops<a href="#loops"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/W47FpFq.png" alt="" /></p><h2 id="final-note-about-terminology">Final note about terminology<a href="#final-note-about-terminology"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/HR8PZ9G.png" alt="" /></p><h1 id="gpu-memory-model">GPU memory model</h1><h2 id="computation-cost-vs-memory-cost">Computation cost vs. memory cost<a href="#computation-cost-vs-memory-cost"><i class="fas fa-hashtag"></i></a></h2></h2><p>Power measurements on NVIDIA GT200</p><p><img data-proofer-ignore data-src="https://i.imgur.com/8rcKY55.png" alt="" /></p><p>With the same amount of energy:</p><ul><li>Load 1 word from external memory (DRAM)<li>Compute 44 flops</ul><div class="alert alert-success" role="alert"><p>Must optimize memory first</p></div><h2 id="external-memory-discrete-gpu">External memory: discrete GPU<a href="#external-memory-discrete-gpu"><i class="fas fa-hashtag"></i></a></h2></h2><p>Classical CPU-GPU model</p><ul><li>Split memory space<li>Highest bandwith from GPU memory<li>Transfers to main memory are slower</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/v7rzntd.png" alt="" /></p><blockquote><p>Intel i7 4770 / GTX 780</p></blockquote><h2 id="external-memory-embedded-gpu">External memory: embedded GPU<a href="#external-memory-embedded-gpu"><i class="fas fa-hashtag"></i></a></h2></h2><p>Most GPUs today:</p><ul><li>Same memory<li>May support memory coherence (GPU can read directly from CPU caches)<li>More contention on external memory</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/fmaa5LI.png" alt="" /></p><h2 id="gpu-on-chip-memory">GPU: on-chip memory<a href="#gpu-on-chip-memory"><i class="fas fa-hashtag"></i></a></h2></h2><p>Cache area in CPU vs GPU: <img data-proofer-ignore data-src="https://i.imgur.com/WUxFyh8.png" alt="" /></p><p>But if we include registers: <img data-proofer-ignore data-src="https://i.imgur.com/IKccvrI.png" alt="" /></p><div class="alert alert-success" role="alert"><p>GPU has many more registers but made of simpler memory</p></div><h2 id="memory-model-hierarchy">Memory model hierarchy<a href="#memory-model-hierarchy"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="hardware">Hardware<a href="#hardware"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/JdjOci5.png" alt="" /></p><p>Cache hierarchy:</p><ul><li>Keep frequently-accessed data Core<li>Reduce throughtput demand on main memoru L1<li>Managed by hardware (L1, L2) or software (shared memory)</ul><p>On CPU, caches are designed to avoid memory latency On GPU, multi-threading deals with memory latency</p><h3 id="software">Software<a href="#software"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/K1c7Aqe.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/LvmwjeC.png" alt="" /></p><h2 id="building-and-running-a-simple-program">Building and running a simple program<a href="#building-and-running-a-simple-program"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/OU0vg4l.png" alt="" /></p><h2 id="what-you-need-to-get-started">What you need to get started<a href="#what-you-need-to-get-started"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>NVidia GPU hardware<li>NVidia GPU drivers, properly loaded<li>CUDA runtime libraries<li>CUDA SDK (NVCC compiler in particular)</ul><h1 id="summary">Summary</h1><ul><li>Host vs Device $\leftrightarrow$ Separate memory<ul><li>GPU are computation units which require explicit usage, as opposed to a CPU<li>Need to load data and fetch result from device</ul><li>Replace loops with kernels<ul><li>Kernel = Function computed in relative isolation on small chunks of data</ul><li>Divide the work<li>Compile and run using CUDA SDK</ul><h1 id="host-view-of-gpu-computation">Host view of GPU computation</h1><h2 id="sequential-and-parallel-sections">Sequential and parallel sections<a href="#sequential-and-parallel-sections"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>We use the GPU(s) as co-processor(s)</ul><p><img data-proofer-ignore data-src="https://i.imgur.com/kO7Anul.png" alt="" /></p><h2 id="cuda-memory-primitives">CUDA memory primitives<a href="#cuda-memory-primitives"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/NE4pw2T.png" alt="" /></p><p><strong>Why 2D and 3D variants ?</strong></p><ul><li>Strong alignment requirements in device memory<ul><li>Enables correct loading of memory chunks to SM caches (correct bank alignment)</ul><li>Proper striding management in automated fashion</ul><h2 id="host-leftrightarrow-device-memory-transfer">Host $\leftrightarrow$ Device memory transfer<a href="#host-leftrightarrow-device-memory-transfer"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/y922J5t.png" alt="" /></p><h2 id="almost-complete-code">Almost complete code<a href="#almost-complete-code"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/wWOFzSV.png" alt="" /></p><h2 id="checking-errors">Checking errors<a href="#checking-errors"><i class="fas fa-hashtag"></i></a></h2></h2><p>In practice check for API errors</p><p><img data-proofer-ignore data-src="https://i.imgur.com/0i5xiHN.png" alt="" /></p><h1 id="intermission-can-i-use-memory-management-functions-inside-kernels-">Intermission: <em>Can I use memory management functions inside kernels ?</em></h1><p><strong>No</strong>: <code class="language-plaintext highlighter-rouge">cudaMalloc()</code>, <code class="language-plaintext highlighter-rouge">cudaMemcpy()</code> and <code class="language-plaintext highlighter-rouge">cudaFree()</code> shall be called from host only</p><p>However, kernels may allocate, use and reclaim memory dynamically using regular <code class="language-plaintext highlighter-rouge">malloc()</code></p><h1 id="fix-the-kernel-invocation-line">Fix the kernel invocation line</h1><p>We want to fix this line: <img data-proofer-ignore data-src="https://i.imgur.com/6GMimst.png" alt="" /></p><p>Kernel invocation syntax: <img data-proofer-ignore data-src="https://i.imgur.com/w89y6Lw.png" alt="" /></p><h2 id="how-to-set-griddim-and-blockdim-properly-">How to set <code class="language-plaintext highlighter-rouge">gridDim</code> and <code class="language-plaintext highlighter-rouge">blockDim</code> properly ?<a href="#how-to-set-griddim-and-blockdim-properly-"><i class="fas fa-hashtag"></i></a></h2></h2><p>Lvl 0: Naive trial with as many threads as possible <img data-proofer-ignore data-src="https://i.imgur.com/xtNi10X.png" alt="" /></p><ul><li>Will fail with large vectors<ul><li>Hardware limitation on the maximum number of thread per block (1024 for compute capability 3.0-7.5)</ul><li>Will fail with vectors of size which is not a multiple of warp size</ul><p>Lvl 1: It works with just enough blocks <img data-proofer-ignore data-src="https://i.imgur.com/9dpZy3p.png" alt="" /></p><p>Lvl 2: Tune block size given the kernel requirements and hardware constraints <img data-proofer-ignore data-src="https://i.imgur.com/VvcBIdo.png" alt="" /></p><h2 id="but-wait">But wait…<a href="#but-wait"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/hGbEKSS.png" alt="" /></p><div class="alert alert-warning" role="alert"><p>This code prints nothing !</p></div><div class="alert alert-danger" role="alert"><p>Kernel invocation is asynchronous</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/r2N2gWq.png" alt="" /></p><div class="alert alert-success" role="alert"><p>Host code synchronization requires <code class="language-plaintext highlighter-rouge">cudaDeviceSynchronize()</code> because <strong>kernel invocation is asynchronous</strong> from host perspective.</p></div><p>On the device, kernel invocations are striclty sequential (unless you schedule them on different <em>streams</em>)</p><h1 id="intermission-can-i-make-kernels-inside-kernels-">Intermission: <em>Can I make kernels inside kernels ?</em></h1><p><strong>Yes</strong>. This is the basic of <em>dynamic parallelism</em></p><p>Some restrictions over the stack size apply. Remember that the device runtime is a functional subset of the host runtime, ie you can perform device management, kernel launching, device <code class="language-plaintext highlighter-rouge">memcpy</code>, etc. but with some restrictions</p><p>The compiler may inline some of those calls.</p><h1 id="conclusion-about-the-host-only-view">Conclusion about the host-only view</h1><p>A host-only view of the computation is sufficient for most of the cases:</p><ol><li>upload data to the device<li>fire a kernel<li>download output data from the device</ol><p>Advanced CUDA requires to make sure we saturate the SMs, and may imply some kernel study to determine the best:</p><ul><li>amount of threads per blocks<li>amount of blocks per grid<li>work per thread (if applicable)<li>…</ul><p>This depends on:</p><ul><li>hardware specifications: maximum <code class="language-plaintext highlighter-rouge">gridDim</code> and <code class="language-plaintext highlighter-rouge">blockDim</code>, etc.<li>kernel code: amount of register and shared memory used by each thread</ul><h1 id="kernel-programming">Kernel programming</h1><h2 id="several-api-levels">Several API levels<a href="#several-api-levels"><i class="fas fa-hashtag"></i></a></h2></h2><p>We now want to program kernels There are several APIs available:</p><ul><li>PTX assembly<li>Driver API (C)<li>Runtime C++ API $\leftarrow$ <strong>let’s use this one</strong></ul><h2 id="function-execution-space-specifiers">Function Execution Space Specifiers<a href="#function-execution-space-specifiers"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/iGYXzbR.png" alt="" /></p><ul><li><code class="language-plaintext highlighter-rouge">__global__</code> defines a kernel function<ul><li>Each <code class="language-plaintext highlighter-rouge">__</code> consists of 2 underscore characters<li>A kernel function must return void<li>It may be called from another kernel for devices of compute capability 3.2 or higher (Dynamic Parallelism support)</ul><li><code class="language-plaintext highlighter-rouge">__device__</code> and <code class="language-plaintext highlighter-rouge">__host__</code> can be used together<li><code class="language-plaintext highlighter-rouge">__host__</code> is optional if used alone</ul><h2 id="built-in-vector-types">Built-in Vector Types<a href="#built-in-vector-types"><i class="fas fa-hashtag"></i></a></h2></h2><p>They make it easy to work with data like images <strong>Alignement mus be respected</strong> in all operations</p><p><img data-proofer-ignore data-src="https://i.imgur.com/KoGHgLn.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/Lvk3u6h.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/BzDXwjl.png" alt="" /></p><div class="alert alert-info" role="alert"><p>They all are structures</p></div><p>They all come with a constructor function of the form <code class="language-plaintext highlighter-rouge">make_&lt;type name&gt;</code></p><p><img data-proofer-ignore data-src="https://i.imgur.com/w2vZgUb.png" alt="" /></p><p>The 1st, 2nd, 3rd and 4th components are accessible through the fields $x$, $y$, $z$ respectively</p><h2 id="built-in-variable">Built-in variable<a href="#built-in-variable"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/ZuyLHCx.png" alt="" /></p><h3 id="example">Example<a href="#example"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/BKYIIF9.png" alt="" /></p><h2 id="memory-hierarchy">Memory hierarchy<a href="#memory-hierarchy"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/5W9tTTJ.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/pMWK6zQ.png" alt="" /></p><h2 id="types-of-memory">Types of Memory<a href="#types-of-memory"><i class="fas fa-hashtag"></i></a></h2></h2><ul><li>Registers<ul><li>Used to store parameters, local variables, etc.<li>Very fast<li>Private to each thread<li>Lots of thread $\Rightarrow$ little memory per threads</ul><li>Shared<ul><li>Used to store temp data<li>Very fast<li><em>Shared</em> among all threads in a block</ul><li>Constant<ul><li>A special cach for read-only values</ul><li>Global<ul><li>Large and slow</ul><li>Caches<ul><li>Transparent uses</ul><li>Local</ul><h2 id="salient-features-of-device-memory">Salient features of Device Memory<a href="#salient-features-of-device-memory"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/GaNsqf7.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/MgD8NoP.png" alt="" /></p><h2 id="cost-to-access-memory">Cost to access memory<a href="#cost-to-access-memory"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/bjn3Pvg.png" alt="" /></p><h2 id="variable-memory-space-specifiers">Variable Memory Space Specifiers<a href="#variable-memory-space-specifiers"><i class="fas fa-hashtag"></i></a></h2></h2><p><strong>How to declaring CUDA variables</strong></p><p><img data-proofer-ignore data-src="https://i.imgur.com/KnC7phe.png" alt="" /></p><p><em>Remarks:</em></p><ul><li><code class="language-plaintext highlighter-rouge">__device__</code> is optional when used with <code class="language-plaintext highlighter-rouge">__shared__</code> or <code class="language-plaintext highlighter-rouge">__constant__</code><li>Automatic variables reside in a register</ul><p><em>Where to declare variables ?</em> Can host access it ?</p><div class="table-wrapper"><table><thead><tr><th>Yes<th>No<tbody><tr><td><strong>global</strong> and <strong>constant</strong>, declare outside of any function<td><strong>register</strong> and <strong>shared</strong>, use of declare in the kernel</table></div><h2 id="who-can-be-shared-by-who-">Who can be shared by who ?<a href="#who-can-be-shared-by-who-"><i class="fas fa-hashtag"></i></a></h2></h2><p>Possible memory access:</p><ul><li>Among threads in the same grid (a kernel invocation)<ul><li>Global memory</ul><li>Among threads in the same block<ul><li>Global memory<li>Shared memory</ul></ul><h2 id="relaxed-consistency-memory-model">Relaxed consistency memory model<a href="#relaxed-consistency-memory-model"><i class="fas fa-hashtag"></i></a></h2></h2><p>The CUDA programming model assumes a device with a <strong>weakly-ordered memory model</strong>, that is the order in which a CUDA thread writes data to shared memory or global memory, is not necessarily the order in which the data is observed being written by another CUDA or host thread</p><h3 id="example-1">Example<a href="#example-1"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/cS4cxhv.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/m5MoQtN.png" alt="" /></p><p><em>Possible outcomes for thread 2 ?</em> <img data-proofer-ignore data-src="https://i.imgur.com/G48Oday.png" alt="" /></p><p><img data-proofer-ignore data-src="https://i.imgur.com/jInFIjp.png" alt="" /></p><h2 id="memory-fence-functions">Memory Fence Functions<a href="#memory-fence-functions"><i class="fas fa-hashtag"></i></a></h2></h2><p>Memory fence functions can be used to enforce some ordering on memory accesses</p><p><img data-proofer-ignore data-src="https://i.imgur.com/GoKrGxt.png" alt="" /> Ensures that:</p><ul><li>All writes to all memory made by the calling thread before the call to <code class="language-plaintext highlighter-rouge">__threadfence_block()</code><li>All reads from all memory</ul><h2 id="synch-functions">Synch functions<a href="#synch-functions"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/e5hbmpM.png" alt="" /></p><div class="alert alert-danger" role="alert"><p>Stronger than <code class="language-plaintext highlighter-rouge">__threadfence()</code> because it also synchronizes the execution</p></div><p><img data-proofer-ignore data-src="https://i.imgur.com/mVylLzi.png" alt="" /></p><h2 id="atomic-functions">Atomic functions<a href="#atomic-functions"><i class="fas fa-hashtag"></i></a></h2></h2><div class="alert alert-info" role="alert"><p>Atomic functions perform a read-modify-write atomic operation on one 32-bit or 64-bit word residing in global or shared memory</p></div><p>Most of the atomic functions are available for all the numerical type</p><h3 id="arithmetic-functions">Arithmetic functions<a href="#arithmetic-functions"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/wmwCTvH.png" alt="" /></p><h1 id="debugging-performance-analysis-and-profiling">Debugging, performance analysis and profiling</h1><h2 id="printf"><code class="language-plaintext highlighter-rouge">printf</code><a href="#printf"><i class="fas fa-hashtag"></i></a></h2></h2><p>Possible since Fermi devices (Compute Capability 2.x and higher)</p><p>Limited amount of lines:</p><ul><li>circular buffer flushed at particular times</ul><h2 id="global-memory-write">Global memory write<a href="#global-memory-write"><i class="fas fa-hashtag"></i></a></h2></h2><p>To dump then inspect a larger amount of intermediate data Analysis code should be removed for production</p><h3 id="example-2">Example<a href="#example-2"><i class="fas fa-hashtag"></i></a></h3></h3><p><img data-proofer-ignore data-src="https://i.imgur.com/tRumZ46.png" alt="" /></p><h2 id="cuda-tools">CUDA tools<a href="#cuda-tools"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/dg1HAod.png" alt="" /></p><h2 id="the-complete-compilation-trajectory">The complete compilation trajectory<a href="#the-complete-compilation-trajectory"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-proofer-ignore data-src="https://i.imgur.com/bgsNioU.png" alt="" /></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/cours/categories/image-s8/'>Image S8</a>, <a href='/cours/categories/irgpu/'>IRGPU</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/cours/tags/image/" class="post-tag no-text-decoration" >Image</a> <a href="/cours/tags/s8/" class="post-tag no-text-decoration" >S8</a> <a href="/cours/tags/irgpu/" class="post-tag no-text-decoration" >IRGPU</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=IRGPU: Getting started with CUDA - Cours&url=https://lemasyma.github.io/cours/posts/irgpu_getting-started/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=IRGPU: Getting started with CUDA - Cours&u=https://lemasyma.github.io/cours/posts/irgpu_getting-started/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=IRGPU: Getting started with CUDA - Cours&url=https://lemasyma.github.io/cours/posts/irgpu_getting-started/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recent Update</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/cours/posts/ocvx_norme/">OCVX: Norme</a><li><a href="/cours/posts/imed2_tp3/">IMED2: TP3</a><li><a href="/cours/posts/prsta_revisions_1/">PRSTA: Revisions 1</a><li><a href="/cours/posts/prsta_td5/">PRSTA: TD 5</a><li><a href="/cours/posts/prsta_td4/">PRSTA: TD 4</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/cours/posts/irgpu_introduction/"><div class="card-body"> <em class="timeago small" date="2021-05-03 10:00:00 +0200" >May 3, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IRGPU: Introduction</h3><div class="text-muted small"><p> Lien de la note Hackmd Agenda GPU and architectures (2h) Programming GPUs with CUDA (2h) TP 00 CUDA (3h) Efficient programming with GPU (2h) TP 01 CUDA GPU and architectures Why using...</p></div></div></a></div><div class="card"> <a href="/cours/posts/irgpu_patterns/"><div class="card-body"> <em class="timeago small" date="2021-05-05 14:00:00 +0200" >May 5, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>IRGPU: Patterns for massively parallel programming</h3><div class="text-muted small"><p> Lien de la note Hackmd IRGPU: Patterns for massively parallel programming Programming patterns &amp;amp; memory optimizations The programming patterns Map Map + Local reduction Reduction Sc...</p></div></div></a></div><div class="card"> <a href="/cours/posts/ocvx_hyperplan/"><div class="card-body"> <em class="timeago small" date="2021-05-07 10:00:00 +0200" >May 7, 2021</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>OCVX: Hyperplan d'appui</h3><div class="text-muted small"><p> Lien de la note Hackmd Rappels Hyperplan d’appui a une partie $A$ de $\mathbb R^n$ en un point $p\in A$, est un hyperplan affine de $\mathbb R^n$ qui laisse $A$ dans un des deux demi-espaces de...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/cours/posts/irgpu_introduction/" class="btn btn-outline-primary" prompt="Older"><p>IRGPU: Introduction</p></a> <a href="/cours/posts/rse_1/" class="btn btn-outline-primary" prompt="Newer"><p>RSE: Premier cours</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/lemasyma">lemasyma</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/cours/tags/image/">Image</a> <a class="post-tag" href="/cours/tags/s8/">S8</a> <a class="post-tag" href="/cours/tags/s9/">S9</a> <a class="post-tag" href="/cours/tags/tronc-commun/">tronc commun</a> <a class="post-tag" href="/cours/tags/scia/">SCIA</a> <a class="post-tag" href="/cours/tags/s6/">S6</a> <a class="post-tag" href="/cours/tags/shannon/">Shannon</a> <a class="post-tag" href="/cours/tags/cama/">CAMA</a> <a class="post-tag" href="/cours/tags/loi/">loi</a> <a class="post-tag" href="/cours/tags/rvau/">RVAU</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/cours/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/cours/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/cours/assets/js/dist/post.min.js"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" }, tagSide: "right" }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, CommonHTML: { linebreaks: { automatic: true } }, "HTML-CSS": { linebreaks: { automatic: true } }, SVG: { linebreaks: { automatic: true } } }); MathJax.Hub.Register.StartupHook("TeX AMSmath Ready", function () { MathJax.InputJax.TeX.Stack.Item.AMSarray.Augment({ clearTag() { if (!this.global.notags) { this.super(arguments).clearTag.call(this); } } }); }); </script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script type="text/javascript" charset="utf-8" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS_CHTML"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/cours/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-PVWXSNG5J8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-PVWXSNG5J8'); }); </script>
